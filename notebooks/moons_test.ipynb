{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "sys.path.append('../src/')\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from src.utils import load_moons, load_monk1, load_mnist\n",
    "from src.network import Network\n",
    "from src.activations import ReLU, Tanh, Sigmoid\n",
    "from src.losses import MeanSquaredError\n",
    "from src.metrics import BinaryAccuracy, MulticlassAccuracy\n",
    "\n",
    "from sklearn.metrics import accuracy_score"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Moons Dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "x_train, x_val, x_test, y_train, y_val, y_test = load_moons(validation=True, noise=0.2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training:   0%|          | 0/500[]\u001B[A\n",
      "Training:   0%|          | 0/500[, loss=0.242, val_loss=0.244, val_acc=0.48]\u001B[A\n",
      "Training:   0%|          | 1/500[, loss=0.242, val_loss=0.244, val_acc=0.48]\u001B[A\n",
      "Training:   0%|          | 1/500[, loss=0.164, val_loss=0.166, val_acc=0.86]\u001B[A\n",
      "Training:   0%|          | 2/500[, loss=0.164, val_loss=0.166, val_acc=0.86]\u001B[A\n",
      "Training:   0%|          | 2/500[, loss=0.109, val_loss=0.112, val_acc=0.88]\u001B[A\n",
      "Training:   1%|          | 3/500[, loss=0.109, val_loss=0.112, val_acc=0.88]\u001B[A\n",
      "Training:   1%|          | 3/500[, loss=0.101, val_loss=0.107, val_acc=0.88]\u001B[A\n",
      "Training:   1%|          | 4/500[, loss=0.101, val_loss=0.107, val_acc=0.88]\u001B[A\n",
      "Training:   1%|          | 4/500[, loss=0.0982, val_loss=0.105, val_acc=0.86]\u001B[A\n",
      "Training:   1%|          | 5/500[, loss=0.0982, val_loss=0.105, val_acc=0.86]\u001B[A\n",
      "Training:   1%|          | 5/500[, loss=0.097, val_loss=0.105, val_acc=0.86] \u001B[A\n",
      "Training:   1%|          | 6/500[, loss=0.097, val_loss=0.105, val_acc=0.86]\u001B[A\n",
      "Training:   1%|          | 6/500[, loss=0.0964, val_loss=0.105, val_acc=0.84]\u001B[A\n",
      "Training:   1%|▏         | 7/500[, loss=0.0964, val_loss=0.105, val_acc=0.84]\u001B[A\n",
      "Training:   1%|▏         | 7/500[, loss=0.096, val_loss=0.105, val_acc=0.84] \u001B[A\n",
      "Training:   2%|▏         | 8/500[, loss=0.096, val_loss=0.105, val_acc=0.84]\u001B[A\n",
      "Training:   2%|▏         | 8/500[, loss=0.0956, val_loss=0.104, val_acc=0.84]\u001B[A\n",
      "Training:   2%|▏         | 9/500[, loss=0.0956, val_loss=0.104, val_acc=0.84]\u001B[A\n",
      "Training:   2%|▏         | 9/500[, loss=0.0954, val_loss=0.104, val_acc=0.84]\u001B[A\n",
      "Training:   2%|▏         | 10/500[, loss=0.0954, val_loss=0.104, val_acc=0.84]\u001B[A\n",
      "Training:   2%|▏         | 10/500[, loss=0.0952, val_loss=0.104, val_acc=0.84]\u001B[A\n",
      "Training:   2%|▏         | 11/500[, loss=0.0952, val_loss=0.104, val_acc=0.84]\u001B[A\n",
      "Training:   2%|▏         | 11/500[, loss=0.095, val_loss=0.104, val_acc=0.84] \u001B[A\n",
      "Training:   2%|▏         | 12/500[, loss=0.095, val_loss=0.104, val_acc=0.84]\u001B[A\n",
      "Training:   2%|▏         | 12/500[, loss=0.0948, val_loss=0.104, val_acc=0.84]\u001B[A\n",
      "Training:   3%|▎         | 13/500[, loss=0.0948, val_loss=0.104, val_acc=0.84]\u001B[A\n",
      "Training:   3%|▎         | 13/500[, loss=0.0946, val_loss=0.104, val_acc=0.84]\u001B[A\n",
      "Training:   3%|▎         | 14/500[, loss=0.0946, val_loss=0.104, val_acc=0.84]\u001B[A\n",
      "Training:   3%|▎         | 14/500[, loss=0.0944, val_loss=0.104, val_acc=0.84]\u001B[A\n",
      "Training:   3%|▎         | 15/500[, loss=0.0944, val_loss=0.104, val_acc=0.84]\u001B[A\n",
      "Training:   3%|▎         | 15/500[, loss=0.0942, val_loss=0.103, val_acc=0.84]\u001B[A\n",
      "Training:   3%|▎         | 16/500[, loss=0.0942, val_loss=0.103, val_acc=0.84]\u001B[A\n",
      "Training:   3%|▎         | 16/500[, loss=0.094, val_loss=0.103, val_acc=0.84] \u001B[A\n",
      "Training:   3%|▎         | 17/500[, loss=0.094, val_loss=0.103, val_acc=0.84]\u001B[A\n",
      "Training:   3%|▎         | 17/500[, loss=0.0937, val_loss=0.103, val_acc=0.84]\u001B[A\n",
      "Training:   4%|▎         | 18/500[, loss=0.0937, val_loss=0.103, val_acc=0.84]\u001B[A\n",
      "Training:   4%|▎         | 18/500[, loss=0.0935, val_loss=0.102, val_acc=0.84]\u001B[A\n",
      "Training:   4%|▍         | 19/500[, loss=0.0935, val_loss=0.102, val_acc=0.84]\u001B[A\n",
      "Training:   4%|▍         | 19/500[, loss=0.0933, val_loss=0.102, val_acc=0.84]\u001B[A\n",
      "Training:   4%|▍         | 20/500[, loss=0.0933, val_loss=0.102, val_acc=0.84]\u001B[A\n",
      "Training:   4%|▍         | 20/500[, loss=0.093, val_loss=0.102, val_acc=0.84] \u001B[A\n",
      "Training:   4%|▍         | 21/500[, loss=0.093, val_loss=0.102, val_acc=0.84]\u001B[A\n",
      "Training:   4%|▍         | 21/500[, loss=0.0926, val_loss=0.101, val_acc=0.84]\u001B[A\n",
      "Training:   4%|▍         | 22/500[, loss=0.0926, val_loss=0.101, val_acc=0.84]\u001B[A\n",
      "Training:   4%|▍         | 22/500[, loss=0.0923, val_loss=0.101, val_acc=0.84]\u001B[A\n",
      "Training:   5%|▍         | 23/500[, loss=0.0923, val_loss=0.101, val_acc=0.84]\u001B[A\n",
      "Training:   5%|▍         | 23/500[, loss=0.0919, val_loss=0.1, val_acc=0.84]  \u001B[A\n",
      "Training:   5%|▍         | 24/500[, loss=0.0919, val_loss=0.1, val_acc=0.84]\u001B[A\n",
      "Training:   5%|▍         | 24/500[, loss=0.0915, val_loss=0.0994, val_acc=0.86]\u001B[A\n",
      "Training:   5%|▌         | 25/500[, loss=0.0915, val_loss=0.0994, val_acc=0.86]\u001B[A\n",
      "Training:   5%|▌         | 25/500[, loss=0.091, val_loss=0.0987, val_acc=0.86] \u001B[A\n",
      "Training:   5%|▌         | 26/500[, loss=0.091, val_loss=0.0987, val_acc=0.86]\u001B[A\n",
      "Training:   5%|▌         | 26/500[, loss=0.0906, val_loss=0.0978, val_acc=0.86]\u001B[A\n",
      "Training:   5%|▌         | 27/500[, loss=0.0906, val_loss=0.0978, val_acc=0.86]\u001B[A\n",
      "Training:   5%|▌         | 27/500[, loss=0.0901, val_loss=0.097, val_acc=0.86] \u001B[A\n",
      "Training:   6%|▌         | 28/500[, loss=0.0901, val_loss=0.097, val_acc=0.86]\u001B[A\n",
      "Training:   6%|▌         | 28/500[, loss=0.0896, val_loss=0.0963, val_acc=0.86]\u001B[A\n",
      "Training:   6%|▌         | 29/500[, loss=0.0896, val_loss=0.0963, val_acc=0.86]\u001B[A\n",
      "Training:   6%|▌         | 29/500[, loss=0.0892, val_loss=0.0955, val_acc=0.86]\u001B[A\n",
      "Training:   6%|▌         | 30/500[, loss=0.0892, val_loss=0.0955, val_acc=0.86]\u001B[A\n",
      "Training:   6%|▌         | 30/500[, loss=0.0887, val_loss=0.0948, val_acc=0.86]\u001B[A\n",
      "Training:   6%|▌         | 31/500[, loss=0.0887, val_loss=0.0948, val_acc=0.86]\u001B[A\n",
      "Training:   6%|▌         | 31/500[, loss=0.0883, val_loss=0.0941, val_acc=0.86]\u001B[A\n",
      "Training:   6%|▋         | 32/500[, loss=0.0883, val_loss=0.0941, val_acc=0.86]\u001B[A\n",
      "Training:   6%|▋         | 32/500[, loss=0.0879, val_loss=0.0935, val_acc=0.86]\u001B[A\n",
      "Training:   7%|▋         | 33/500[, loss=0.0879, val_loss=0.0935, val_acc=0.86]\u001B[A\n",
      "Training:   7%|▋         | 33/500[, loss=0.0876, val_loss=0.0929, val_acc=0.86]\u001B[A\n",
      "Training:   7%|▋         | 34/500[, loss=0.0876, val_loss=0.0929, val_acc=0.86]\u001B[A\n",
      "Training:   7%|▋         | 34/500[, loss=0.0872, val_loss=0.0923, val_acc=0.86]\u001B[A\n",
      "Training:   7%|▋         | 35/500[, loss=0.0872, val_loss=0.0923, val_acc=0.86]\u001B[A\n",
      "Training:   7%|▋         | 35/500[, loss=0.087, val_loss=0.0917, val_acc=0.86] \u001B[A\n",
      "Training:   7%|▋         | 36/500[, loss=0.087, val_loss=0.0917, val_acc=0.86]\u001B[A\n",
      "Training:   7%|▋         | 36/500[, loss=0.0867, val_loss=0.0914, val_acc=0.88]\u001B[A\n",
      "Training:   7%|▋         | 37/500[, loss=0.0867, val_loss=0.0914, val_acc=0.88]\u001B[A\n",
      "Training:   7%|▋         | 37/500[, loss=0.0865, val_loss=0.0908, val_acc=0.88]\u001B[A\n",
      "Training:   8%|▊         | 38/500[, loss=0.0865, val_loss=0.0908, val_acc=0.88]\u001B[A\n",
      "Training:   8%|▊         | 38/500[, loss=0.0863, val_loss=0.0903, val_acc=0.88]\u001B[A\n",
      "Training:   8%|▊         | 39/500[, loss=0.0863, val_loss=0.0903, val_acc=0.88]\u001B[A\n",
      "Training:   8%|▊         | 39/500[, loss=0.0862, val_loss=0.0899, val_acc=0.88]\u001B[A\n",
      "Training:   8%|▊         | 40/500[, loss=0.0862, val_loss=0.0899, val_acc=0.88]\u001B[A\n",
      "Training:   8%|▊         | 40/500[, loss=0.086, val_loss=0.0895, val_acc=0.88] \u001B[A\n",
      "Training:   8%|▊         | 41/500[, loss=0.086, val_loss=0.0895, val_acc=0.88]\u001B[A\n",
      "Training:   8%|▊         | 41/500[, loss=0.0859, val_loss=0.0892, val_acc=0.88]\u001B[A\n",
      "Training:   8%|▊         | 42/500[, loss=0.0859, val_loss=0.0892, val_acc=0.88]\u001B[A\n",
      "Training:   8%|▊         | 42/500[, loss=0.0858, val_loss=0.0888, val_acc=0.88]\u001B[A\n",
      "Training:   9%|▊         | 43/500[, loss=0.0858, val_loss=0.0888, val_acc=0.88]\u001B[A\n",
      "Training:   9%|▊         | 43/500[, loss=0.0857, val_loss=0.0886, val_acc=0.88]\u001B[A\n",
      "Training:   9%|▉         | 44/500[, loss=0.0857, val_loss=0.0886, val_acc=0.88]\u001B[A\n",
      "Training:   9%|▉         | 44/500[, loss=0.0857, val_loss=0.0883, val_acc=0.88]\u001B[A\n",
      "Training:   9%|▉         | 45/500[, loss=0.0857, val_loss=0.0883, val_acc=0.88]\u001B[A\n",
      "Training:   9%|▉         | 45/500[, loss=0.0856, val_loss=0.0881, val_acc=0.88]\u001B[A\n",
      "Training:   9%|▉         | 46/500[, loss=0.0856, val_loss=0.0881, val_acc=0.88]\u001B[A\n",
      "Training:   9%|▉         | 46/500[, loss=0.0856, val_loss=0.0879, val_acc=0.88]\u001B[A\n",
      "Training:   9%|▉         | 47/500[, loss=0.0856, val_loss=0.0879, val_acc=0.88]\u001B[A\n",
      "Training:   9%|▉         | 47/500[, loss=0.0855, val_loss=0.0878, val_acc=0.88]\u001B[A\n",
      "Training:  10%|▉         | 48/500[, loss=0.0855, val_loss=0.0878, val_acc=0.88]\u001B[A\n",
      "Training:  10%|▉         | 48/500[, loss=0.0855, val_loss=0.0876, val_acc=0.88]\u001B[A\n",
      "Training:  10%|▉         | 49/500[, loss=0.0855, val_loss=0.0876, val_acc=0.88]\u001B[A\n",
      "Training:  10%|▉         | 49/500[, loss=0.0854, val_loss=0.0875, val_acc=0.88]\u001B[A\n",
      "Training:  10%|█         | 50/500[, loss=0.0854, val_loss=0.0875, val_acc=0.88]\u001B[A\n",
      "Training:  10%|█         | 50/500[, loss=0.0854, val_loss=0.0873, val_acc=0.88]\u001B[A\n",
      "Training:  10%|█         | 51/500[, loss=0.0854, val_loss=0.0873, val_acc=0.88]\u001B[A\n",
      "Training:  10%|█         | 51/500[, loss=0.0852, val_loss=0.0872, val_acc=0.88]\u001B[A\n",
      "Training:  10%|█         | 52/500[, loss=0.0852, val_loss=0.0872, val_acc=0.88]\u001B[A\n",
      "Training:  10%|█         | 52/500[, loss=0.0851, val_loss=0.087, val_acc=0.88] \u001B[A\n",
      "Training:  11%|█         | 53/500[, loss=0.0851, val_loss=0.087, val_acc=0.88]\u001B[A\n",
      "Training:  11%|█         | 53/500[, loss=0.085, val_loss=0.0868, val_acc=0.88]\u001B[A\n",
      "Training:  11%|█         | 54/500[, loss=0.085, val_loss=0.0868, val_acc=0.88]\u001B[A\n",
      "Training:  11%|█         | 54/500[, loss=0.0849, val_loss=0.0867, val_acc=0.88]\u001B[A\n",
      "Training:  11%|█         | 55/500[, loss=0.0849, val_loss=0.0867, val_acc=0.88]\u001B[A\n",
      "Training:  11%|█         | 55/500[, loss=0.0848, val_loss=0.0865, val_acc=0.88]\u001B[A\n",
      "Training:  11%|█         | 56/500[, loss=0.0848, val_loss=0.0865, val_acc=0.88]\u001B[A\n",
      "Training:  11%|█         | 56/500[, loss=0.0847, val_loss=0.0863, val_acc=0.88]\u001B[A\n",
      "Training:  11%|█▏        | 57/500[, loss=0.0847, val_loss=0.0863, val_acc=0.88]\u001B[A\n",
      "Training:  11%|█▏        | 57/500[, loss=0.0846, val_loss=0.0861, val_acc=0.88]\u001B[A\n",
      "Training:  12%|█▏        | 58/500[, loss=0.0846, val_loss=0.0861, val_acc=0.88]\u001B[A\n",
      "Training:  12%|█▏        | 58/500[, loss=0.0844, val_loss=0.086, val_acc=0.88] \u001B[A\n",
      "Training:  12%|█▏        | 59/500[, loss=0.0844, val_loss=0.086, val_acc=0.88]\u001B[A\n",
      "Training:  12%|█▏        | 59/500[, loss=0.0843, val_loss=0.0858, val_acc=0.88]\u001B[A\n",
      "Training:  12%|█▏        | 60/500[, loss=0.0843, val_loss=0.0858, val_acc=0.88]\u001B[A\n",
      "Training:  12%|█▏        | 60/500[, loss=0.0841, val_loss=0.0856, val_acc=0.88]\u001B[A\n",
      "Training:  12%|█▏        | 61/500[, loss=0.0841, val_loss=0.0856, val_acc=0.88]\u001B[A\n",
      "Training:  12%|█▏        | 61/500[, loss=0.0839, val_loss=0.0854, val_acc=0.88]\u001B[A\n",
      "Training:  12%|█▏        | 62/500[, loss=0.0839, val_loss=0.0854, val_acc=0.88]\u001B[A\n",
      "Training:  12%|█▏        | 62/500[, loss=0.0837, val_loss=0.0852, val_acc=0.88]\u001B[A\n",
      "Training:  13%|█▎        | 63/500[, loss=0.0837, val_loss=0.0852, val_acc=0.88]\u001B[A\n",
      "Training:  13%|█▎        | 63/500[, loss=0.0835, val_loss=0.085, val_acc=0.88] \u001B[A\n",
      "Training:  13%|█▎        | 64/500[, loss=0.0835, val_loss=0.085, val_acc=0.88]\u001B[A\n",
      "Training:  13%|█▎        | 64/500[, loss=0.0832, val_loss=0.0847, val_acc=0.9]\u001B[A\n",
      "Training:  13%|█▎        | 65/500[, loss=0.0832, val_loss=0.0847, val_acc=0.9]\u001B[A\n",
      "Training:  13%|█▎        | 65/500[, loss=0.083, val_loss=0.0843, val_acc=0.9] \u001B[A\n",
      "Training:  13%|█▎        | 66/500[, loss=0.083, val_loss=0.0843, val_acc=0.9]\u001B[A\n",
      "Training:  13%|█▎        | 66/500[, loss=0.0826, val_loss=0.0839, val_acc=0.9]\u001B[A\n",
      "Training:  13%|█▎        | 67/500[, loss=0.0826, val_loss=0.0839, val_acc=0.9]\u001B[A\n",
      "Training:  13%|█▎        | 67/500[, loss=0.0823, val_loss=0.0836, val_acc=0.9]\u001B[A\n",
      "Training:  14%|█▎        | 68/500[, loss=0.0823, val_loss=0.0836, val_acc=0.9]\u001B[A\n",
      "Training:  14%|█▎        | 68/500[, loss=0.0819, val_loss=0.0832, val_acc=0.9]\u001B[A\n",
      "Training:  14%|█▍        | 69/500[, loss=0.0819, val_loss=0.0832, val_acc=0.9]\u001B[A\n",
      "Training:  14%|█▍        | 69/500[, loss=0.0815, val_loss=0.0827, val_acc=0.9]\u001B[A\n",
      "Training:  14%|█▍        | 70/500[, loss=0.0815, val_loss=0.0827, val_acc=0.9]\u001B[A\n",
      "Training:  14%|█▍        | 70/500[, loss=0.0811, val_loss=0.0822, val_acc=0.9]\u001B[A\n",
      "Training:  14%|█▍        | 71/500[, loss=0.0811, val_loss=0.0822, val_acc=0.9]\u001B[A\n",
      "Training:  14%|█▍        | 71/500[, loss=0.0807, val_loss=0.0817, val_acc=0.9]\u001B[A\n",
      "Training:  14%|█▍        | 72/500[, loss=0.0807, val_loss=0.0817, val_acc=0.9]\u001B[A\n",
      "Training:  14%|█▍        | 72/500[, loss=0.0801, val_loss=0.0812, val_acc=0.9]\u001B[A\n",
      "Training:  15%|█▍        | 73/500[, loss=0.0801, val_loss=0.0812, val_acc=0.9]\u001B[A\n",
      "Training:  15%|█▍        | 73/500[, loss=0.0796, val_loss=0.0805, val_acc=0.9]\u001B[A\n",
      "Training:  15%|█▍        | 74/500[, loss=0.0796, val_loss=0.0805, val_acc=0.9]\u001B[A\n",
      "Training:  15%|█▍        | 74/500[, loss=0.079, val_loss=0.0799, val_acc=0.9] \u001B[A\n",
      "Training:  15%|█▌        | 75/500[, loss=0.079, val_loss=0.0799, val_acc=0.9]\u001B[A\n",
      "Training:  15%|█▌        | 75/500[, loss=0.0784, val_loss=0.0794, val_acc=0.9]\u001B[A\n",
      "Training:  15%|█▌        | 76/500[, loss=0.0784, val_loss=0.0794, val_acc=0.9]\u001B[A\n",
      "Training:  15%|█▌        | 76/500[, loss=0.0778, val_loss=0.0788, val_acc=0.9]\u001B[A\n",
      "Training:  15%|█▌        | 77/500[, loss=0.0778, val_loss=0.0788, val_acc=0.9]\u001B[A\n",
      "Training:  15%|█▌        | 77/500[, loss=0.0772, val_loss=0.0781, val_acc=0.9]\u001B[A\n",
      "Training:  16%|█▌        | 78/500[, loss=0.0772, val_loss=0.0781, val_acc=0.9]\u001B[A\n",
      "Training:  16%|█▌        | 78/500[, loss=0.0766, val_loss=0.0773, val_acc=0.9]\u001B[A\n",
      "Training:  16%|█▌        | 79/500[, loss=0.0766, val_loss=0.0773, val_acc=0.9]\u001B[A\n",
      "Training:  16%|█▌        | 79/500[, loss=0.076, val_loss=0.0766, val_acc=0.9] \u001B[A\n",
      "Training:  16%|█▌        | 80/500[, loss=0.076, val_loss=0.0766, val_acc=0.9]\u001B[A\n",
      "Training:  16%|█▌        | 80/500[, loss=0.0753, val_loss=0.0759, val_acc=0.9]\u001B[A\n",
      "Training:  16%|█▌        | 81/500[, loss=0.0753, val_loss=0.0759, val_acc=0.9]\u001B[A\n",
      "Training:  16%|█▌        | 81/500[, loss=0.0747, val_loss=0.0752, val_acc=0.9]\u001B[A\n",
      "Training:  16%|█▋        | 82/500[, loss=0.0747, val_loss=0.0752, val_acc=0.9]\u001B[A\n",
      "Training:  16%|█▋        | 82/500[, loss=0.0741, val_loss=0.0746, val_acc=0.9]\u001B[A\n",
      "Training:  17%|█▋        | 83/500[, loss=0.0741, val_loss=0.0746, val_acc=0.9]\u001B[A\n",
      "Training:  17%|█▋        | 83/500[, loss=0.0735, val_loss=0.074, val_acc=0.9] \u001B[A\n",
      "Training:  17%|█▋        | 84/500[, loss=0.0735, val_loss=0.074, val_acc=0.9]\u001B[A\n",
      "Training:  17%|█▋        | 84/500[, loss=0.073, val_loss=0.0735, val_acc=0.9]\u001B[A\n",
      "Training:  17%|█▋        | 85/500[, loss=0.073, val_loss=0.0735, val_acc=0.9]\u001B[A\n",
      "Training:  17%|█▋        | 85/500[, loss=0.0724, val_loss=0.0731, val_acc=0.9]\u001B[A\n",
      "Training:  17%|█▋        | 86/500[, loss=0.0724, val_loss=0.0731, val_acc=0.9]\u001B[A\n",
      "Training:  17%|█▋        | 86/500[, loss=0.0717, val_loss=0.0725, val_acc=0.9]\u001B[A\n",
      "Training:  17%|█▋        | 87/500[, loss=0.0717, val_loss=0.0725, val_acc=0.9]\u001B[A\n",
      "Training:  17%|█▋        | 87/500[, loss=0.0711, val_loss=0.0719, val_acc=0.92]\u001B[A\n",
      "Training:  18%|█▊        | 88/500[, loss=0.0711, val_loss=0.0719, val_acc=0.92]\u001B[A\n",
      "Training:  18%|█▊        | 88/500[, loss=0.0704, val_loss=0.0714, val_acc=0.92]\u001B[A\n",
      "Training:  18%|█▊        | 89/500[, loss=0.0704, val_loss=0.0714, val_acc=0.92]\u001B[A\n",
      "Training:  18%|█▊        | 89/500[, loss=0.0697, val_loss=0.0709, val_acc=0.92]\u001B[A\n",
      "Training:  18%|█▊        | 90/500[, loss=0.0697, val_loss=0.0709, val_acc=0.92]\u001B[A\n",
      "Training:  18%|█▊        | 90/500[, loss=0.0689, val_loss=0.0703, val_acc=0.94]\u001B[A\n",
      "Training:  18%|█▊        | 91/500[, loss=0.0689, val_loss=0.0703, val_acc=0.94]\u001B[A\n",
      "Training:  18%|█▊        | 91/500[, loss=0.068, val_loss=0.0695, val_acc=0.94] \u001B[A\n",
      "Training:  18%|█▊        | 92/500[, loss=0.068, val_loss=0.0695, val_acc=0.94]\u001B[A\n",
      "Training:  18%|█▊        | 92/500[, loss=0.0671, val_loss=0.0687, val_acc=0.94]\u001B[A\n",
      "Training:  19%|█▊        | 93/500[, loss=0.0671, val_loss=0.0687, val_acc=0.94]\u001B[A\n",
      "Training:  19%|█▊        | 93/500[, loss=0.0663, val_loss=0.0679, val_acc=0.94]\u001B[A\n",
      "Training:  19%|█▉        | 94/500[, loss=0.0663, val_loss=0.0679, val_acc=0.94]\u001B[A\n",
      "Training:  19%|█▉        | 94/500[, loss=0.0655, val_loss=0.0672, val_acc=0.96]\u001B[A\n",
      "Training:  19%|█▉        | 95/500[, loss=0.0655, val_loss=0.0672, val_acc=0.96]\u001B[A\n",
      "Training:  19%|█▉        | 95/500[, loss=0.0647, val_loss=0.0665, val_acc=0.96]\u001B[A\n",
      "Training:  19%|█▉        | 96/500[, loss=0.0647, val_loss=0.0665, val_acc=0.96]\u001B[A\n",
      "Training:  19%|█▉        | 96/500[, loss=0.0639, val_loss=0.0657, val_acc=0.96]\u001B[A\n",
      "Training:  19%|█▉        | 97/500[, loss=0.0639, val_loss=0.0657, val_acc=0.96]\u001B[A\n",
      "Training:  19%|█▉        | 97/500[, loss=0.0631, val_loss=0.0649, val_acc=0.96]\u001B[A\n",
      "Training:  20%|█▉        | 98/500[, loss=0.0631, val_loss=0.0649, val_acc=0.96]\u001B[A\n",
      "Training:  20%|█▉        | 98/500[, loss=0.0623, val_loss=0.0642, val_acc=0.96]\u001B[A\n",
      "Training:  20%|█▉        | 99/500[, loss=0.0623, val_loss=0.0642, val_acc=0.96]\u001B[A\n",
      "Training:  20%|█▉        | 99/500[, loss=0.0616, val_loss=0.0634, val_acc=0.96]\u001B[A\n",
      "Training:  20%|██        | 100/500[, loss=0.0616, val_loss=0.0634, val_acc=0.96]\u001B[A\n",
      "Training:  20%|██        | 100/500[, loss=0.0609, val_loss=0.0626, val_acc=0.96]\u001B[A\n",
      "Training:  20%|██        | 101/500[, loss=0.0609, val_loss=0.0626, val_acc=0.96]\u001B[A\n",
      "Training:  20%|██        | 101/500[, loss=0.0603, val_loss=0.0619, val_acc=0.96]\u001B[A\n",
      "Training:  20%|██        | 102/500[, loss=0.0603, val_loss=0.0619, val_acc=0.96]\u001B[A\n",
      "Training:  20%|██        | 102/500[, loss=0.0597, val_loss=0.0613, val_acc=0.96]\u001B[A\n",
      "Training:  21%|██        | 103/500[, loss=0.0597, val_loss=0.0613, val_acc=0.96]\u001B[A\n",
      "Training:  21%|██        | 103/500[, loss=0.0591, val_loss=0.0606, val_acc=0.96]\u001B[A\n",
      "Training:  21%|██        | 104/500[, loss=0.0591, val_loss=0.0606, val_acc=0.96]\u001B[A\n",
      "Training:  21%|██        | 104/500[, loss=0.0586, val_loss=0.06, val_acc=0.96]  \u001B[A\n",
      "Training:  21%|██        | 105/500[, loss=0.0586, val_loss=0.06, val_acc=0.96]\u001B[A\n",
      "Training:  21%|██        | 105/500[, loss=0.0581, val_loss=0.0594, val_acc=0.96]\u001B[A\n",
      "Training:  21%|██        | 106/500[, loss=0.0581, val_loss=0.0594, val_acc=0.96]\u001B[A\n",
      "Training:  21%|██        | 106/500[, loss=0.0576, val_loss=0.0589, val_acc=0.96]\u001B[A\n",
      "Training:  21%|██▏       | 107/500[, loss=0.0576, val_loss=0.0589, val_acc=0.96]\u001B[A\n",
      "Training:  21%|██▏       | 107/500[, loss=0.0572, val_loss=0.0583, val_acc=0.96]\u001B[A\n",
      "Training:  22%|██▏       | 108/500[, loss=0.0572, val_loss=0.0583, val_acc=0.96]\u001B[A\n",
      "Training:  22%|██▏       | 108/500[, loss=0.0568, val_loss=0.0578, val_acc=0.96]\u001B[A\n",
      "Training:  22%|██▏       | 109/500[, loss=0.0568, val_loss=0.0578, val_acc=0.96]\u001B[A\n",
      "Training:  22%|██▏       | 109/500[, loss=0.0563, val_loss=0.0572, val_acc=0.96]\u001B[A\n",
      "Training:  22%|██▏       | 110/500[, loss=0.0563, val_loss=0.0572, val_acc=0.96]\u001B[A\n",
      "Training:  22%|██▏       | 110/500[, loss=0.0559, val_loss=0.0567, val_acc=0.96]\u001B[A\n",
      "Training:  22%|██▏       | 111/500[, loss=0.0559, val_loss=0.0567, val_acc=0.96]\u001B[A\n",
      "Training:  22%|██▏       | 111/500[, loss=0.0556, val_loss=0.0564, val_acc=0.96]\u001B[A\n",
      "Training:  22%|██▏       | 112/500[, loss=0.0556, val_loss=0.0564, val_acc=0.96]\u001B[A\n",
      "Training:  22%|██▏       | 112/500[, loss=0.0552, val_loss=0.0561, val_acc=0.96]\u001B[A\n",
      "Training:  23%|██▎       | 113/500[, loss=0.0552, val_loss=0.0561, val_acc=0.96]\u001B[A\n",
      "Training:  23%|██▎       | 113/500[, loss=0.0549, val_loss=0.0558, val_acc=0.96]\u001B[A\n",
      "Training:  23%|██▎       | 114/500[, loss=0.0549, val_loss=0.0558, val_acc=0.96]\u001B[A\n",
      "Training:  23%|██▎       | 114/500[, loss=0.0546, val_loss=0.0554, val_acc=0.96]\u001B[A\n",
      "Training:  23%|██▎       | 115/500[, loss=0.0546, val_loss=0.0554, val_acc=0.96]\u001B[A\n",
      "Training:  23%|██▎       | 115/500[, loss=0.0543, val_loss=0.0551, val_acc=0.96]\u001B[A\n",
      "Training:  23%|██▎       | 116/500[, loss=0.0543, val_loss=0.0551, val_acc=0.96]\u001B[A\n",
      "Training:  23%|██▎       | 116/500[, loss=0.054, val_loss=0.0547, val_acc=0.96] \u001B[A\n",
      "Training:  23%|██▎       | 117/500[, loss=0.054, val_loss=0.0547, val_acc=0.96]\u001B[A\n",
      "Training:  23%|██▎       | 117/500[, loss=0.0537, val_loss=0.0544, val_acc=0.96]\u001B[A\n",
      "Training:  24%|██▎       | 118/500[, loss=0.0537, val_loss=0.0544, val_acc=0.96]\u001B[A\n",
      "Training:  24%|██▎       | 118/500[, loss=0.0534, val_loss=0.054, val_acc=0.96] \u001B[A\n",
      "Training:  24%|██▍       | 119/500[, loss=0.0534, val_loss=0.054, val_acc=0.96]\u001B[A\n",
      "Training:  24%|██▍       | 119/500[, loss=0.0532, val_loss=0.0533, val_acc=0.96]\u001B[A\n",
      "Training:  24%|██▍       | 120/500[, loss=0.0532, val_loss=0.0533, val_acc=0.96]\u001B[A\n",
      "Training:  24%|██▍       | 120/500[, loss=0.0528, val_loss=0.0527, val_acc=0.96]\u001B[A\n",
      "Training:  24%|██▍       | 121/500[, loss=0.0528, val_loss=0.0527, val_acc=0.96]\u001B[A\n",
      "Training:  24%|██▍       | 121/500[, loss=0.0526, val_loss=0.0522, val_acc=0.96]\u001B[A\n",
      "Training:  24%|██▍       | 122/500[, loss=0.0526, val_loss=0.0522, val_acc=0.96]\u001B[A\n",
      "Training:  24%|██▍       | 122/500[, loss=0.0523, val_loss=0.0516, val_acc=0.96]\u001B[A\n",
      "Training:  25%|██▍       | 123/500[, loss=0.0523, val_loss=0.0516, val_acc=0.96]\u001B[A\n",
      "Training:  25%|██▍       | 123/500[, loss=0.052, val_loss=0.051, val_acc=0.96]  \u001B[A\n",
      "Training:  25%|██▍       | 124/500[, loss=0.052, val_loss=0.051, val_acc=0.96]\u001B[A\n",
      "Training:  25%|██▍       | 124/500[, loss=0.0519, val_loss=0.0507, val_acc=0.96]\u001B[A\n",
      "Training:  25%|██▌       | 125/500[, loss=0.0519, val_loss=0.0507, val_acc=0.96]\u001B[A\n",
      "Training:  25%|██▌       | 125/500[, loss=0.0517, val_loss=0.0505, val_acc=0.96]\u001B[A\n",
      "Training:  25%|██▌       | 126/500[, loss=0.0517, val_loss=0.0505, val_acc=0.96]\u001B[A\n",
      "Training:  25%|██▌       | 126/500[, loss=0.0515, val_loss=0.0502, val_acc=0.96]\u001B[A\n",
      "Training:  25%|██▌       | 127/500[, loss=0.0515, val_loss=0.0502, val_acc=0.96]\u001B[A\n",
      "Training:  25%|██▌       | 127/500[, loss=0.0512, val_loss=0.0499, val_acc=0.96]\u001B[A\n",
      "Training:  26%|██▌       | 128/500[, loss=0.0512, val_loss=0.0499, val_acc=0.96]\u001B[A\n",
      "Training:  26%|██▌       | 128/500[, loss=0.051, val_loss=0.0498, val_acc=0.96] \u001B[A\n",
      "Training:  26%|██▌       | 129/500[, loss=0.051, val_loss=0.0498, val_acc=0.96]\u001B[A\n",
      "Training:  26%|██▌       | 129/500[, loss=0.0508, val_loss=0.0496, val_acc=0.96]\u001B[A\n",
      "Training:  26%|██▌       | 130/500[, loss=0.0508, val_loss=0.0496, val_acc=0.96]\u001B[A\n",
      "Training:  26%|██▌       | 130/500[, loss=0.0506, val_loss=0.0496, val_acc=0.98]\u001B[A\n",
      "Training:  26%|██▌       | 131/500[, loss=0.0506, val_loss=0.0496, val_acc=0.98]\u001B[A\n",
      "Training:  26%|██▌       | 131/500[, loss=0.0505, val_loss=0.0495, val_acc=0.98]\u001B[A\n",
      "Training:  26%|██▋       | 132/500[, loss=0.0505, val_loss=0.0495, val_acc=0.98]\u001B[A\n",
      "Training:  26%|██▋       | 132/500[, loss=0.0503, val_loss=0.0495, val_acc=0.98]\u001B[A\n",
      "Training:  27%|██▋       | 133/500[, loss=0.0503, val_loss=0.0495, val_acc=0.98]\u001B[A\n",
      "Training:  27%|██▋       | 133/500[, loss=0.0501, val_loss=0.0495, val_acc=0.98]\u001B[A\n",
      "Training:  27%|██▋       | 134/500[, loss=0.0501, val_loss=0.0495, val_acc=0.98]\u001B[A\n",
      "Training:  27%|██▋       | 134/500[, loss=0.05, val_loss=0.0495, val_acc=0.98]  \u001B[A\n",
      "Training:  27%|██▋       | 135/500[, loss=0.05, val_loss=0.0495, val_acc=0.98]\u001B[A\n",
      "Training:  27%|██▋       | 135/500[, loss=0.0498, val_loss=0.0494, val_acc=0.98]\u001B[A\n",
      "Training:  27%|██▋       | 136/500[, loss=0.0498, val_loss=0.0494, val_acc=0.98]\u001B[A\n",
      "Training:  27%|██▋       | 136/500[, loss=0.0497, val_loss=0.0494, val_acc=1]   \u001B[A\n",
      "Training:  27%|██▋       | 137/500[, loss=0.0497, val_loss=0.0494, val_acc=1]\u001B[A\n",
      "Training:  27%|██▋       | 137/500[, loss=0.0495, val_loss=0.0494, val_acc=1]\u001B[A\n",
      "Training:  28%|██▊       | 138/500[, loss=0.0495, val_loss=0.0494, val_acc=1]\u001B[A\n",
      "Training:  28%|██▊       | 138/500[, loss=0.0494, val_loss=0.0493, val_acc=1]\u001B[A\n",
      "Training:  28%|██▊       | 139/500[, loss=0.0494, val_loss=0.0493, val_acc=1]\u001B[A\n",
      "Training:  28%|██▊       | 139/500[, loss=0.0493, val_loss=0.0492, val_acc=1]\u001B[A\n",
      "Training:  28%|██▊       | 140/500[, loss=0.0493, val_loss=0.0492, val_acc=1]\u001B[A\n",
      "Training:  28%|██▊       | 140/500[, loss=0.0492, val_loss=0.0491, val_acc=1]\u001B[A\n",
      "Training:  28%|██▊       | 141/500[, loss=0.0492, val_loss=0.0491, val_acc=1]\u001B[A\n",
      "Training:  28%|██▊       | 141/500[, loss=0.049, val_loss=0.049, val_acc=1]  \u001B[A\n",
      "Training:  28%|██▊       | 142/500[, loss=0.049, val_loss=0.049, val_acc=1]\u001B[A\n",
      "Training:  28%|██▊       | 142/500[, loss=0.0489, val_loss=0.0489, val_acc=1]\u001B[A\n",
      "Training:  29%|██▊       | 143/500[, loss=0.0489, val_loss=0.0489, val_acc=1]\u001B[A\n",
      "Training:  29%|██▊       | 143/500[, loss=0.0488, val_loss=0.0488, val_acc=1]\u001B[A\n",
      "Training:  29%|██▉       | 144/500[, loss=0.0488, val_loss=0.0488, val_acc=1]\u001B[A\n",
      "Training:  29%|██▉       | 144/500[, loss=0.0487, val_loss=0.0487, val_acc=1]\u001B[A\n",
      "Training:  29%|██▉       | 145/500[, loss=0.0487, val_loss=0.0487, val_acc=1]\u001B[A\n",
      "Training:  29%|██▉       | 145/500[, loss=0.0486, val_loss=0.0487, val_acc=1]\u001B[A\n",
      "Training:  29%|██▉       | 146/500[, loss=0.0486, val_loss=0.0487, val_acc=1]\u001B[A\n",
      "Training:  29%|██▉       | 146/500[, loss=0.0485, val_loss=0.0485, val_acc=1]\u001B[A\n",
      "Training:  29%|██▉       | 147/500[, loss=0.0485, val_loss=0.0485, val_acc=1]\u001B[A\n",
      "Training:  29%|██▉       | 147/500[, loss=0.0484, val_loss=0.0485, val_acc=1]\u001B[A\n",
      "Training:  30%|██▉       | 148/500[, loss=0.0484, val_loss=0.0485, val_acc=1]\u001B[A\n",
      "Training:  30%|██▉       | 148/500[, loss=0.0484, val_loss=0.0484, val_acc=1]\u001B[A\n",
      "Training:  30%|██▉       | 149/500[, loss=0.0484, val_loss=0.0484, val_acc=1]\u001B[A\n",
      "Training:  30%|██▉       | 149/500[, loss=0.0483, val_loss=0.0484, val_acc=1]\u001B[A\n",
      "Training:  30%|███       | 150/500[, loss=0.0483, val_loss=0.0484, val_acc=1]\u001B[A\n",
      "Training:  30%|███       | 150/500[, loss=0.0482, val_loss=0.0483, val_acc=1]\u001B[A\n",
      "Training:  30%|███       | 151/500[, loss=0.0482, val_loss=0.0483, val_acc=1]\u001B[A\n",
      "Training:  30%|███       | 151/500[, loss=0.0481, val_loss=0.0483, val_acc=1]\u001B[A\n",
      "Training:  30%|███       | 152/500[, loss=0.0481, val_loss=0.0483, val_acc=1]\u001B[A\n",
      "Training:  30%|███       | 152/500[, loss=0.048, val_loss=0.0482, val_acc=1] \u001B[A\n",
      "Training:  31%|███       | 153/500[, loss=0.048, val_loss=0.0482, val_acc=1]\u001B[A\n",
      "Training:  31%|███       | 153/500[, loss=0.0479, val_loss=0.0481, val_acc=1]\u001B[A\n",
      "Training:  31%|███       | 154/500[, loss=0.0479, val_loss=0.0481, val_acc=1]\u001B[A\n",
      "Training:  31%|███       | 154/500[, loss=0.0478, val_loss=0.048, val_acc=1] \u001B[A\n",
      "Training:  31%|███       | 155/500[, loss=0.0478, val_loss=0.048, val_acc=1]\u001B[A\n",
      "Training:  31%|███       | 155/500[, loss=0.0477, val_loss=0.048, val_acc=1]\u001B[A\n",
      "Training:  31%|███       | 156/500[, loss=0.0477, val_loss=0.048, val_acc=1]\u001B[A\n",
      "Training:  31%|███       | 156/500[, loss=0.0476, val_loss=0.0479, val_acc=1]\u001B[A\n",
      "Training:  31%|███▏      | 157/500[, loss=0.0476, val_loss=0.0479, val_acc=1]\u001B[A\n",
      "Training:  31%|███▏      | 157/500[, loss=0.0475, val_loss=0.0479, val_acc=1]\u001B[A\n",
      "Training:  32%|███▏      | 158/500[, loss=0.0475, val_loss=0.0479, val_acc=1]\u001B[A\n",
      "Training:  32%|███▏      | 158/500[, loss=0.0475, val_loss=0.0478, val_acc=1]\u001B[A\n",
      "Training:  32%|███▏      | 159/500[, loss=0.0475, val_loss=0.0478, val_acc=1]\u001B[A\n",
      "Training:  32%|███▏      | 159/500[, loss=0.0474, val_loss=0.0477, val_acc=0.98]\u001B[A\n",
      "Training:  32%|███▏      | 160/500[, loss=0.0474, val_loss=0.0477, val_acc=0.98]\u001B[A\n",
      "Training:  32%|███▏      | 160/500[, loss=0.0473, val_loss=0.0477, val_acc=0.98]\u001B[A\n",
      "Training:  32%|███▏      | 161/500[, loss=0.0473, val_loss=0.0477, val_acc=0.98]\u001B[A\n",
      "Training:  32%|███▏      | 161/500[, loss=0.0472, val_loss=0.0476, val_acc=0.98]\u001B[A\n",
      "Training:  32%|███▏      | 162/500[, loss=0.0472, val_loss=0.0476, val_acc=0.98]\u001B[A\n",
      "Training:  32%|███▏      | 162/500[, loss=0.0472, val_loss=0.0476, val_acc=0.98]\u001B[A\n",
      "Training:  33%|███▎      | 163/500[, loss=0.0472, val_loss=0.0476, val_acc=0.98]\u001B[A\n",
      "Training:  33%|███▎      | 163/500[, loss=0.0471, val_loss=0.0476, val_acc=1]   \u001B[A\n",
      "Training:  33%|███▎      | 164/500[, loss=0.0471, val_loss=0.0476, val_acc=1]\u001B[A\n",
      "Training:  33%|███▎      | 164/500[, loss=0.0471, val_loss=0.0476, val_acc=1]\u001B[A\n",
      "Training:  33%|███▎      | 165/500[, loss=0.0471, val_loss=0.0476, val_acc=1]\u001B[A\n",
      "Training:  33%|███▎      | 165/500[, loss=0.047, val_loss=0.0476, val_acc=0.98]\u001B[A\n",
      "Training:  33%|███▎      | 166/500[, loss=0.047, val_loss=0.0476, val_acc=0.98]\u001B[A\n",
      "Training:  33%|███▎      | 166/500[, loss=0.0469, val_loss=0.0475, val_acc=0.98]\u001B[A\n",
      "Training:  33%|███▎      | 167/500[, loss=0.0469, val_loss=0.0475, val_acc=0.98]\u001B[A\n",
      "Training:  33%|███▎      | 167/500[, loss=0.0469, val_loss=0.0475, val_acc=0.98]\u001B[A\n",
      "Training:  34%|███▎      | 168/500[, loss=0.0469, val_loss=0.0475, val_acc=0.98]\u001B[A\n",
      "Training:  34%|███▎      | 168/500[, loss=0.0468, val_loss=0.0475, val_acc=0.98]\u001B[A\n",
      "Training:  34%|███▍      | 169/500[, loss=0.0468, val_loss=0.0475, val_acc=0.98]\u001B[A\n",
      "Training:  34%|███▍      | 169/500[, loss=0.0468, val_loss=0.0474, val_acc=0.98]\u001B[A\n",
      "Training:  34%|███▍      | 170/500[, loss=0.0468, val_loss=0.0474, val_acc=0.98]\u001B[A\n",
      "Training:  34%|███▍      | 170/500[, loss=0.0467, val_loss=0.0474, val_acc=0.98]\u001B[A\n",
      "Training:  34%|███▍      | 171/500[, loss=0.0467, val_loss=0.0474, val_acc=0.98]\u001B[A\n",
      "Training:  34%|███▍      | 171/500[, loss=0.0467, val_loss=0.0474, val_acc=0.98]\u001B[A\n",
      "Training:  34%|███▍      | 172/500[, loss=0.0467, val_loss=0.0474, val_acc=0.98]\u001B[A\n",
      "Training:  34%|███▍      | 172/500[, loss=0.0466, val_loss=0.0475, val_acc=1]   \u001B[A\n",
      "Training:  35%|███▍      | 173/500[, loss=0.0466, val_loss=0.0475, val_acc=1]\u001B[A\n",
      "Training:  35%|███▍      | 173/500[, loss=0.0466, val_loss=0.0475, val_acc=1]\u001B[A\n",
      "Training:  35%|███▍      | 174/500[, loss=0.0466, val_loss=0.0475, val_acc=1]\u001B[A\n",
      "Training:  35%|███▍      | 174/500[, loss=0.0465, val_loss=0.0474, val_acc=0.98]\u001B[A\n",
      "Training:  35%|███▌      | 175/500[, loss=0.0465, val_loss=0.0474, val_acc=0.98]\u001B[A\n",
      "Training:  35%|███▌      | 175/500[, loss=0.0465, val_loss=0.0474, val_acc=0.98]\u001B[A\n",
      "Training:  35%|███▌      | 176/500[, loss=0.0465, val_loss=0.0474, val_acc=0.98]\u001B[A\n",
      "Training:  35%|███▌      | 176/500[, loss=0.0464, val_loss=0.0474, val_acc=0.98]\u001B[A\n",
      "Training:  35%|███▌      | 177/500[, loss=0.0464, val_loss=0.0474, val_acc=0.98]\u001B[A\n",
      "Training:  35%|███▌      | 177/500[, loss=0.0464, val_loss=0.0474, val_acc=0.98]\u001B[A\n",
      "Training:  36%|███▌      | 178/500[, loss=0.0464, val_loss=0.0474, val_acc=0.98]\u001B[A\n",
      "Training:  36%|███▌      | 178/500[, loss=0.0464, val_loss=0.0475, val_acc=0.98]\u001B[A\n",
      "Training:  36%|███▌      | 179/500[, loss=0.0464, val_loss=0.0475, val_acc=0.98]\u001B[A\n",
      "Training:  36%|███▌      | 179/500[, loss=0.0463, val_loss=0.0475, val_acc=0.98]\u001B[A\n",
      "Training:  36%|███▌      | 180/500[, loss=0.0463, val_loss=0.0475, val_acc=0.98]\u001B[A\n",
      "Training:  36%|███▌      | 180/500[, loss=0.0463, val_loss=0.0476, val_acc=0.98]\u001B[A\n",
      "Training:  36%|███▌      | 181/500[, loss=0.0463, val_loss=0.0476, val_acc=0.98]\u001B[A\n",
      "Training:  36%|███▌      | 181/500[, loss=0.0463, val_loss=0.0475, val_acc=0.98]\u001B[A\n",
      "Training:  36%|███▋      | 182/500[, loss=0.0463, val_loss=0.0475, val_acc=0.98]\u001B[A\n",
      "Training:  36%|███▋      | 182/500[, loss=0.0462, val_loss=0.0475, val_acc=0.98]\u001B[A\n",
      "Training:  37%|███▋      | 183/500[, loss=0.0462, val_loss=0.0475, val_acc=0.98]\u001B[A\n",
      "Training:  37%|███▋      | 183/500[, loss=0.0462, val_loss=0.0476, val_acc=0.98]\u001B[A\n",
      "Training:  37%|███▋      | 184/500[, loss=0.0462, val_loss=0.0476, val_acc=0.98]\u001B[A\n",
      "Training:  37%|███▋      | 184/500[, loss=0.0462, val_loss=0.0475, val_acc=0.98]\u001B[A\n",
      "Training:  37%|███▋      | 185/500[, loss=0.0462, val_loss=0.0475, val_acc=0.98]\u001B[A\n",
      "Training:  37%|███▋      | 185/500[, loss=0.0461, val_loss=0.0475, val_acc=0.98]\u001B[A\n",
      "Training:  37%|███▋      | 186/500[, loss=0.0461, val_loss=0.0475, val_acc=0.98]\u001B[A\n",
      "Training:  37%|███▋      | 186/500[, loss=0.0461, val_loss=0.0475, val_acc=0.98]\u001B[A\n",
      "Training:  37%|███▋      | 187/500[, loss=0.0461, val_loss=0.0475, val_acc=0.98]\u001B[A\n",
      "Training:  37%|███▋      | 187/500[, loss=0.046, val_loss=0.0475, val_acc=0.98] \u001B[A\n",
      "Training:  38%|███▊      | 188/500[, loss=0.046, val_loss=0.0475, val_acc=0.98]\u001B[A\n",
      "Training:  38%|███▊      | 188/500[, loss=0.046, val_loss=0.0475, val_acc=0.98]\u001B[A\n",
      "Training:  38%|███▊      | 189/500[, loss=0.046, val_loss=0.0475, val_acc=0.98]\u001B[A\n",
      "Training:  38%|███▊      | 189/500[, loss=0.0459, val_loss=0.0475, val_acc=0.98]\u001B[A\n",
      "Training:  38%|███▊      | 190/500[, loss=0.0459, val_loss=0.0475, val_acc=0.98]\u001B[A\n",
      "Training:  38%|███▊      | 190/500[, loss=0.0459, val_loss=0.0475, val_acc=0.98]\u001B[A\n",
      "Training:  38%|███▊      | 191/500[, loss=0.0459, val_loss=0.0475, val_acc=0.98]\u001B[A\n",
      "Training:  38%|███▊      | 191/500[, loss=0.0458, val_loss=0.0475, val_acc=0.98]\u001B[A\n",
      "Training:  38%|███▊      | 192/500[, loss=0.0458, val_loss=0.0475, val_acc=0.98]\u001B[A\n",
      "Training:  38%|███▊      | 192/500[, loss=0.0458, val_loss=0.0475, val_acc=0.98]\u001B[A\n",
      "Training:  39%|███▊      | 193/500[, loss=0.0458, val_loss=0.0475, val_acc=0.98]\u001B[A\n",
      "Training:  39%|███▊      | 193/500[, loss=0.0458, val_loss=0.0474, val_acc=0.98]\u001B[A\n",
      "Training:  39%|███▉      | 194/500[, loss=0.0458, val_loss=0.0474, val_acc=0.98]\u001B[A\n",
      "Training:  39%|███▉      | 194/500[, loss=0.0457, val_loss=0.0474, val_acc=0.98]\u001B[A\n",
      "Training:  39%|███▉      | 195/500[, loss=0.0457, val_loss=0.0474, val_acc=0.98]\u001B[A\n",
      "Training:  39%|███▉      | 195/500[, loss=0.0457, val_loss=0.0474, val_acc=0.98]\u001B[A\n",
      "Training:  39%|███▉      | 196/500[, loss=0.0457, val_loss=0.0474, val_acc=0.98]\u001B[A\n",
      "Training:  39%|███▉      | 196/500[, loss=0.0456, val_loss=0.0475, val_acc=1]   \u001B[A\n",
      "Training:  39%|███▉      | 197/500[, loss=0.0456, val_loss=0.0475, val_acc=1]\u001B[A\n",
      "Training:  39%|███▉      | 197/500[, loss=0.0455, val_loss=0.0475, val_acc=1]\u001B[A\n",
      "Training:  40%|███▉      | 198/500[, loss=0.0455, val_loss=0.0475, val_acc=1]\u001B[A\n",
      "Training:  40%|███▉      | 198/500[, loss=0.0455, val_loss=0.0475, val_acc=1]\u001B[A\n",
      "Training:  40%|███▉      | 199/500[, loss=0.0455, val_loss=0.0475, val_acc=1]\u001B[A\n",
      "Training:  40%|███▉      | 199/500[, loss=0.0454, val_loss=0.0475, val_acc=1]\u001B[A\n",
      "Training:  40%|████      | 200/500[, loss=0.0454, val_loss=0.0475, val_acc=1]\u001B[A\n",
      "Training:  40%|████      | 200/500[, loss=0.0453, val_loss=0.0475, val_acc=1]\u001B[A\n",
      "Training:  40%|████      | 201/500[, loss=0.0453, val_loss=0.0475, val_acc=1]\u001B[A\n",
      "Training:  40%|████      | 201/500[, loss=0.0452, val_loss=0.0475, val_acc=1]\u001B[A\n",
      "Training:  40%|████      | 202/500[, loss=0.0452, val_loss=0.0475, val_acc=1]\u001B[A\n",
      "Training:  40%|████      | 202/500[, loss=0.0452, val_loss=0.0474, val_acc=1]\u001B[A\n",
      "Training:  41%|████      | 203/500[, loss=0.0452, val_loss=0.0474, val_acc=1]\u001B[A\n",
      "Training:  41%|████      | 203/500[, loss=0.0451, val_loss=0.0474, val_acc=1]\u001B[A\n",
      "Training:  41%|████      | 204/500[, loss=0.0451, val_loss=0.0474, val_acc=1]\u001B[A\n",
      "Training:  41%|████      | 204/500[, loss=0.0451, val_loss=0.0473, val_acc=1]\u001B[A\n",
      "Training:  41%|████      | 205/500[, loss=0.0451, val_loss=0.0473, val_acc=1]\u001B[A\n",
      "Training:  41%|████      | 205/500[, loss=0.045, val_loss=0.0472, val_acc=1] \u001B[A\n",
      "Training:  41%|████      | 206/500[, loss=0.045, val_loss=0.0472, val_acc=1]\u001B[A\n",
      "Training:  41%|████      | 206/500[, loss=0.045, val_loss=0.0471, val_acc=1]\u001B[A\n",
      "Training:  41%|████▏     | 207/500[, loss=0.045, val_loss=0.0471, val_acc=1]\u001B[A\n",
      "Training:  41%|████▏     | 207/500[, loss=0.0449, val_loss=0.047, val_acc=1]\u001B[A\n",
      "Training:  42%|████▏     | 208/500[, loss=0.0449, val_loss=0.047, val_acc=1]\u001B[A\n",
      "Training:  42%|████▏     | 208/500[, loss=0.0449, val_loss=0.0469, val_acc=1]\u001B[A\n",
      "Training:  42%|████▏     | 209/500[, loss=0.0449, val_loss=0.0469, val_acc=1]\u001B[A\n",
      "Training:  42%|████▏     | 209/500[, loss=0.0448, val_loss=0.0468, val_acc=1]\u001B[A\n",
      "Training:  42%|████▏     | 210/500[, loss=0.0448, val_loss=0.0468, val_acc=1]\u001B[A\n",
      "Training:  42%|████▏     | 210/500[, loss=0.0447, val_loss=0.0466, val_acc=1]\u001B[A\n",
      "Training:  42%|████▏     | 211/500[, loss=0.0447, val_loss=0.0466, val_acc=1]\u001B[A\n",
      "Training:  42%|████▏     | 211/500[, loss=0.0446, val_loss=0.0465, val_acc=1]\u001B[A\n",
      "Training:  42%|████▏     | 212/500[, loss=0.0446, val_loss=0.0465, val_acc=1]\u001B[A\n",
      "Training:  42%|████▏     | 212/500[, loss=0.0446, val_loss=0.0465, val_acc=1]\u001B[A\n",
      "Training:  43%|████▎     | 213/500[, loss=0.0446, val_loss=0.0465, val_acc=1]\u001B[A\n",
      "Training:  43%|████▎     | 213/500[, loss=0.0445, val_loss=0.0464, val_acc=1]\u001B[A\n",
      "Training:  43%|████▎     | 214/500[, loss=0.0445, val_loss=0.0464, val_acc=1]\u001B[A\n",
      "Training:  43%|████▎     | 214/500[, loss=0.0445, val_loss=0.0464, val_acc=1]\u001B[A\n",
      "Training:  43%|████▎     | 215/500[, loss=0.0445, val_loss=0.0464, val_acc=1]\u001B[A\n",
      "Training:  43%|████▎     | 215/500[, loss=0.0444, val_loss=0.0463, val_acc=1]\u001B[A\n",
      "Training:  43%|████▎     | 216/500[, loss=0.0444, val_loss=0.0463, val_acc=1]\u001B[A\n",
      "Training:  43%|████▎     | 216/500[, loss=0.0443, val_loss=0.0462, val_acc=1]\u001B[A\n",
      "Training:  43%|████▎     | 217/500[, loss=0.0443, val_loss=0.0462, val_acc=1]\u001B[A\n",
      "Training:  43%|████▎     | 217/500[, loss=0.0443, val_loss=0.0461, val_acc=1]\u001B[A\n",
      "Training:  44%|████▎     | 218/500[, loss=0.0443, val_loss=0.0461, val_acc=1]\u001B[A\n",
      "Training:  44%|████▎     | 218/500[, loss=0.0442, val_loss=0.046, val_acc=1] \u001B[A\n",
      "Training:  44%|████▍     | 219/500[, loss=0.0442, val_loss=0.046, val_acc=1]\u001B[A\n",
      "Training:  44%|████▍     | 219/500[, loss=0.0442, val_loss=0.046, val_acc=1]\u001B[A\n",
      "Training:  44%|████▍     | 220/500[, loss=0.0442, val_loss=0.046, val_acc=1]\u001B[A\n",
      "Training:  44%|████▍     | 220/500[, loss=0.0441, val_loss=0.0459, val_acc=1]\u001B[A\n",
      "Training:  44%|████▍     | 221/500[, loss=0.0441, val_loss=0.0459, val_acc=1]\u001B[A\n",
      "Training:  44%|████▍     | 221/500[, loss=0.0441, val_loss=0.0459, val_acc=1]\u001B[A\n",
      "Training:  44%|████▍     | 222/500[, loss=0.0441, val_loss=0.0459, val_acc=1]\u001B[A\n",
      "Training:  44%|████▍     | 222/500[, loss=0.044, val_loss=0.0458, val_acc=1] \u001B[A\n",
      "Training:  45%|████▍     | 223/500[, loss=0.044, val_loss=0.0458, val_acc=1]\u001B[A\n",
      "Training:  45%|████▍     | 223/500[, loss=0.0439, val_loss=0.0458, val_acc=1]\u001B[A\n",
      "Training:  45%|████▍     | 224/500[, loss=0.0439, val_loss=0.0458, val_acc=1]\u001B[A\n",
      "Training:  45%|████▍     | 224/500[, loss=0.0439, val_loss=0.0458, val_acc=1]\u001B[A\n",
      "Training:  45%|████▌     | 225/500[, loss=0.0439, val_loss=0.0458, val_acc=1]\u001B[A\n",
      "Training:  45%|████▌     | 225/500[, loss=0.0438, val_loss=0.0456, val_acc=1]\u001B[A\n",
      "Training:  45%|████▌     | 226/500[, loss=0.0438, val_loss=0.0456, val_acc=1]\u001B[A\n",
      "Training:  45%|████▌     | 226/500[, loss=0.0438, val_loss=0.0456, val_acc=1]\u001B[A\n",
      "Training:  45%|████▌     | 227/500[, loss=0.0438, val_loss=0.0456, val_acc=1]\u001B[A\n",
      "Training:  45%|████▌     | 227/500[, loss=0.0437, val_loss=0.0456, val_acc=1]\u001B[A\n",
      "Training:  46%|████▌     | 228/500[, loss=0.0437, val_loss=0.0456, val_acc=1]\u001B[A\n",
      "Training:  46%|████▌     | 228/500[, loss=0.0437, val_loss=0.0456, val_acc=1]\u001B[A\n",
      "Training:  46%|████▌     | 229/500[, loss=0.0437, val_loss=0.0456, val_acc=1]\u001B[A\n",
      "Training:  46%|████▌     | 229/500[, loss=0.0437, val_loss=0.0455, val_acc=1]\u001B[A\n",
      "Training:  46%|████▌     | 230/500[, loss=0.0437, val_loss=0.0455, val_acc=1]\u001B[A\n",
      "Training:  46%|████▌     | 230/500[, loss=0.0436, val_loss=0.0455, val_acc=1]\u001B[A\n",
      "Training:  46%|████▌     | 231/500[, loss=0.0436, val_loss=0.0455, val_acc=1]\u001B[A\n",
      "Training:  46%|████▌     | 231/500[, loss=0.0436, val_loss=0.0455, val_acc=1]\u001B[A\n",
      "Training:  46%|████▋     | 232/500[, loss=0.0436, val_loss=0.0455, val_acc=1]\u001B[A\n",
      "Training:  46%|████▋     | 232/500[, loss=0.0435, val_loss=0.0455, val_acc=1]\u001B[A\n",
      "Training:  47%|████▋     | 233/500[, loss=0.0435, val_loss=0.0455, val_acc=1]\u001B[A\n",
      "Training:  47%|████▋     | 233/500[, loss=0.0435, val_loss=0.0455, val_acc=1]\u001B[A\n",
      "Training:  47%|████▋     | 234/500[, loss=0.0435, val_loss=0.0455, val_acc=1]\u001B[A\n",
      "Training:  47%|████▋     | 234/500[, loss=0.0434, val_loss=0.0455, val_acc=1]\u001B[A\n",
      "Training:  47%|████▋     | 235/500[, loss=0.0434, val_loss=0.0455, val_acc=1]\u001B[A\n",
      "Training:  47%|████▋     | 235/500[, loss=0.0434, val_loss=0.0454, val_acc=1]\u001B[A\n",
      "Training:  47%|████▋     | 236/500[, loss=0.0434, val_loss=0.0454, val_acc=1]\u001B[A\n",
      "Training:  47%|████▋     | 236/500[, loss=0.0434, val_loss=0.0454, val_acc=1]\u001B[A\n",
      "Training:  47%|████▋     | 237/500[, loss=0.0434, val_loss=0.0454, val_acc=1]\u001B[A\n",
      "Training:  47%|████▋     | 237/500[, loss=0.0433, val_loss=0.0454, val_acc=1]\u001B[A\n",
      "Training:  48%|████▊     | 238/500[, loss=0.0433, val_loss=0.0454, val_acc=1]\u001B[A\n",
      "Training:  48%|████▊     | 238/500[, loss=0.0433, val_loss=0.0454, val_acc=1]\u001B[A\n",
      "Training:  48%|████▊     | 239/500[, loss=0.0433, val_loss=0.0454, val_acc=1]\u001B[A\n",
      "Training:  48%|████▊     | 239/500[, loss=0.0433, val_loss=0.0454, val_acc=1]\u001B[A\n",
      "Training:  48%|████▊     | 240/500[, loss=0.0433, val_loss=0.0454, val_acc=1]\u001B[A\n",
      "Training:  48%|████▊     | 240/500[, loss=0.0432, val_loss=0.0454, val_acc=1]\u001B[A\n",
      "Training:  48%|████▊     | 241/500[, loss=0.0432, val_loss=0.0454, val_acc=1]\u001B[A\n",
      "Training:  48%|████▊     | 241/500[, loss=0.0432, val_loss=0.0453, val_acc=1]\u001B[A\n",
      "Training:  48%|████▊     | 242/500[, loss=0.0432, val_loss=0.0453, val_acc=1]\u001B[A\n",
      "Training:  48%|████▊     | 242/500[, loss=0.0432, val_loss=0.0453, val_acc=1]\u001B[A\n",
      "Training:  49%|████▊     | 243/500[, loss=0.0432, val_loss=0.0453, val_acc=1]\u001B[A\n",
      "Training:  49%|████▊     | 243/500[, loss=0.0431, val_loss=0.0453, val_acc=1]\u001B[A\n",
      "Training:  49%|████▉     | 244/500[, loss=0.0431, val_loss=0.0453, val_acc=1]\u001B[A\n",
      "Training:  49%|████▉     | 244/500[, loss=0.0431, val_loss=0.0453, val_acc=1]\u001B[A\n",
      "Training:  49%|████▉     | 245/500[, loss=0.0431, val_loss=0.0453, val_acc=1]\u001B[A\n",
      "Training:  49%|████▉     | 245/500[, loss=0.0431, val_loss=0.0453, val_acc=1]\u001B[A\n",
      "Training:  49%|████▉     | 246/500[, loss=0.0431, val_loss=0.0453, val_acc=1]\u001B[A\n",
      "Training:  49%|████▉     | 246/500[, loss=0.043, val_loss=0.0453, val_acc=1] \u001B[A\n",
      "Training:  49%|████▉     | 247/500[, loss=0.043, val_loss=0.0453, val_acc=1]\u001B[A\n",
      "Training:  49%|████▉     | 247/500[, loss=0.043, val_loss=0.0452, val_acc=1]\u001B[A\n",
      "Training:  50%|████▉     | 248/500[, loss=0.043, val_loss=0.0452, val_acc=1]\u001B[A\n",
      "Training:  50%|████▉     | 248/500[, loss=0.043, val_loss=0.0452, val_acc=1]\u001B[A\n",
      "Training:  50%|████▉     | 249/500[, loss=0.043, val_loss=0.0452, val_acc=1]\u001B[A\n",
      "Training:  50%|████▉     | 249/500[, loss=0.043, val_loss=0.0452, val_acc=1]\u001B[A\n",
      "Training:  50%|█████     | 250/500[, loss=0.043, val_loss=0.0452, val_acc=1]\u001B[A\n",
      "Training:  50%|█████     | 250/500[, loss=0.0429, val_loss=0.0452, val_acc=1]\u001B[A\n",
      "Training:  50%|█████     | 251/500[, loss=0.0429, val_loss=0.0452, val_acc=1]\u001B[A\n",
      "Training:  50%|█████     | 251/500[, loss=0.0429, val_loss=0.0452, val_acc=1]\u001B[A\n",
      "Training:  50%|█████     | 252/500[, loss=0.0429, val_loss=0.0452, val_acc=1]\u001B[A\n",
      "Training:  50%|█████     | 252/500[, loss=0.0429, val_loss=0.0452, val_acc=1]\u001B[A\n",
      "Training:  51%|█████     | 253/500[, loss=0.0429, val_loss=0.0452, val_acc=1]\u001B[A\n",
      "Training:  51%|█████     | 253/500[, loss=0.0429, val_loss=0.0451, val_acc=1]\u001B[A\n",
      "Training:  51%|█████     | 254/500[, loss=0.0429, val_loss=0.0451, val_acc=1]\u001B[A\n",
      "Training:  51%|█████     | 254/500[, loss=0.0428, val_loss=0.0451, val_acc=1]\u001B[A\n",
      "Training:  51%|█████     | 255/500[, loss=0.0428, val_loss=0.0451, val_acc=1]\u001B[A\n",
      "Training:  51%|█████     | 255/500[, loss=0.0428, val_loss=0.0451, val_acc=1]\u001B[A\n",
      "Training:  51%|█████     | 256/500[, loss=0.0428, val_loss=0.0451, val_acc=1]\u001B[A\n",
      "Training:  51%|█████     | 256/500[, loss=0.0428, val_loss=0.0451, val_acc=1]\u001B[A\n",
      "Training:  51%|█████▏    | 257/500[, loss=0.0428, val_loss=0.0451, val_acc=1]\u001B[A\n",
      "Training:  51%|█████▏    | 257/500[, loss=0.0428, val_loss=0.0451, val_acc=0.98]\u001B[A\n",
      "Training:  52%|█████▏    | 258/500[, loss=0.0428, val_loss=0.0451, val_acc=0.98]\u001B[A\n",
      "Training:  52%|█████▏    | 258/500[, loss=0.0427, val_loss=0.0451, val_acc=0.98]\u001B[A\n",
      "Training:  52%|█████▏    | 259/500[, loss=0.0427, val_loss=0.0451, val_acc=0.98]\u001B[A\n",
      "Training:  52%|█████▏    | 259/500[, loss=0.0427, val_loss=0.045, val_acc=0.98] \u001B[A\n",
      "Training:  52%|█████▏    | 260/500[, loss=0.0427, val_loss=0.045, val_acc=0.98]\u001B[A\n",
      "Training:  52%|█████▏    | 260/500[, loss=0.0427, val_loss=0.045, val_acc=0.98]\u001B[A\n",
      "Training:  52%|█████▏    | 261/500[, loss=0.0427, val_loss=0.045, val_acc=0.98]\u001B[A\n",
      "Training:  52%|█████▏    | 261/500[, loss=0.0427, val_loss=0.045, val_acc=0.98]\u001B[A\n",
      "Training:  52%|█████▏    | 262/500[, loss=0.0427, val_loss=0.045, val_acc=0.98]\u001B[A\n",
      "Training:  52%|█████▏    | 262/500[, loss=0.0427, val_loss=0.045, val_acc=0.98]\u001B[A\n",
      "Training:  53%|█████▎    | 263/500[, loss=0.0427, val_loss=0.045, val_acc=0.98]\u001B[A\n",
      "Training:  53%|█████▎    | 263/500[, loss=0.0426, val_loss=0.045, val_acc=0.98]\u001B[A\n",
      "Training:  53%|█████▎    | 264/500[, loss=0.0426, val_loss=0.045, val_acc=0.98]\u001B[A\n",
      "Training:  53%|█████▎    | 264/500[, loss=0.0426, val_loss=0.045, val_acc=0.98]\u001B[A\n",
      "Training:  53%|█████▎    | 265/500[, loss=0.0426, val_loss=0.045, val_acc=0.98]\u001B[A\n",
      "Training:  53%|█████▎    | 265/500[, loss=0.0426, val_loss=0.0449, val_acc=0.98]\u001B[A\n",
      "Training:  53%|█████▎    | 266/500[, loss=0.0426, val_loss=0.0449, val_acc=0.98]\u001B[A\n",
      "Training:  53%|█████▎    | 266/500[, loss=0.0426, val_loss=0.0449, val_acc=0.98]\u001B[A\n",
      "Training:  53%|█████▎    | 267/500[, loss=0.0426, val_loss=0.0449, val_acc=0.98]\u001B[A\n",
      "Training:  53%|█████▎    | 267/500[, loss=0.0426, val_loss=0.0449, val_acc=0.98]\u001B[A\n",
      "Training:  54%|█████▎    | 268/500[, loss=0.0426, val_loss=0.0449, val_acc=0.98]\u001B[A\n",
      "Training:  54%|█████▎    | 268/500[, loss=0.0425, val_loss=0.0449, val_acc=0.98]\u001B[A\n",
      "Training:  54%|█████▍    | 269/500[, loss=0.0425, val_loss=0.0449, val_acc=0.98]\u001B[A\n",
      "Training:  54%|█████▍    | 269/500[, loss=0.0425, val_loss=0.0449, val_acc=0.98]\u001B[A\n",
      "Training:  54%|█████▍    | 270/500[, loss=0.0425, val_loss=0.0449, val_acc=0.98]\u001B[A\n",
      "Training:  54%|█████▍    | 270/500[, loss=0.0425, val_loss=0.0449, val_acc=0.98]\u001B[A\n",
      "Training:  54%|█████▍    | 271/500[, loss=0.0425, val_loss=0.0449, val_acc=0.98]\u001B[A\n",
      "Training:  54%|█████▍    | 271/500[, loss=0.0425, val_loss=0.0449, val_acc=0.98]\u001B[A\n",
      "Training:  54%|█████▍    | 272/500[, loss=0.0425, val_loss=0.0449, val_acc=0.98]\u001B[A\n",
      "Training:  54%|█████▍    | 272/500[, loss=0.0425, val_loss=0.0448, val_acc=0.98]\u001B[A\n",
      "Training:  55%|█████▍    | 273/500[, loss=0.0425, val_loss=0.0448, val_acc=0.98]\u001B[A\n",
      "Training:  55%|█████▍    | 273/500[, loss=0.0424, val_loss=0.0448, val_acc=0.98]\u001B[A\n",
      "Training:  55%|█████▍    | 274/500[, loss=0.0424, val_loss=0.0448, val_acc=0.98]\u001B[A\n",
      "Training:  55%|█████▍    | 274/500[, loss=0.0424, val_loss=0.0448, val_acc=0.98]\u001B[A\n",
      "Training:  55%|█████▌    | 275/500[, loss=0.0424, val_loss=0.0448, val_acc=0.98]\u001B[A\n",
      "Training:  55%|█████▌    | 275/500[, loss=0.0424, val_loss=0.0448, val_acc=0.98]\u001B[A\n",
      "Training:  55%|█████▌    | 276/500[, loss=0.0424, val_loss=0.0448, val_acc=0.98]\u001B[A\n",
      "Training:  55%|█████▌    | 276/500[, loss=0.0424, val_loss=0.0448, val_acc=0.98]\u001B[A\n",
      "Training:  55%|█████▌    | 277/500[, loss=0.0424, val_loss=0.0448, val_acc=0.98]\u001B[A\n",
      "Training:  55%|█████▌    | 277/500[, loss=0.0424, val_loss=0.0447, val_acc=0.98]\u001B[A\n",
      "Training:  56%|█████▌    | 278/500[, loss=0.0424, val_loss=0.0447, val_acc=0.98]\u001B[A\n",
      "Training:  56%|█████▌    | 278/500[, loss=0.0424, val_loss=0.0447, val_acc=0.98]\u001B[A\n",
      "Training:  56%|█████▌    | 279/500[, loss=0.0424, val_loss=0.0447, val_acc=0.98]\u001B[A\n",
      "Training:  56%|█████▌    | 279/500[, loss=0.0423, val_loss=0.0447, val_acc=0.98]\u001B[A\n",
      "Training:  56%|█████▌    | 280/500[, loss=0.0423, val_loss=0.0447, val_acc=0.98]\u001B[A\n",
      "Training:  56%|█████▌    | 280/500[, loss=0.0423, val_loss=0.0447, val_acc=0.98]\u001B[A\n",
      "Training:  56%|█████▌    | 281/500[, loss=0.0423, val_loss=0.0447, val_acc=0.98]\u001B[A\n",
      "Training:  56%|█████▌    | 281/500[, loss=0.0423, val_loss=0.0447, val_acc=0.98]\u001B[A\n",
      "Training:  56%|█████▋    | 282/500[, loss=0.0423, val_loss=0.0447, val_acc=0.98]\u001B[A\n",
      "Training:  56%|█████▋    | 282/500[, loss=0.0423, val_loss=0.0447, val_acc=0.98]\u001B[A\n",
      "Training:  57%|█████▋    | 283/500[, loss=0.0423, val_loss=0.0447, val_acc=0.98]\u001B[A\n",
      "Training:  57%|█████▋    | 283/500[, loss=0.0423, val_loss=0.0446, val_acc=0.98]\u001B[A\n",
      "Training:  57%|█████▋    | 284/500[, loss=0.0423, val_loss=0.0446, val_acc=0.98]\u001B[A\n",
      "Training:  57%|█████▋    | 284/500[, loss=0.0423, val_loss=0.0446, val_acc=0.98]\u001B[A\n",
      "Training:  57%|█████▋    | 285/500[, loss=0.0423, val_loss=0.0446, val_acc=0.98]\u001B[A\n",
      "Training:  57%|█████▋    | 285/500[, loss=0.0423, val_loss=0.0446, val_acc=0.98]\u001B[A\n",
      "Training:  57%|█████▋    | 286/500[, loss=0.0423, val_loss=0.0446, val_acc=0.98]\u001B[A\n",
      "Training:  57%|█████▋    | 286/500[, loss=0.0422, val_loss=0.0446, val_acc=0.98]\u001B[A\n",
      "Training:  57%|█████▋    | 287/500[, loss=0.0422, val_loss=0.0446, val_acc=0.98]\u001B[A\n",
      "Training:  57%|█████▋    | 287/500[, loss=0.0422, val_loss=0.0446, val_acc=0.98]\u001B[A\n",
      "Training:  58%|█████▊    | 288/500[, loss=0.0422, val_loss=0.0446, val_acc=0.98]\u001B[A\n",
      "Training:  58%|█████▊    | 288/500[, loss=0.0422, val_loss=0.0445, val_acc=0.98]\u001B[A\n",
      "Training:  58%|█████▊    | 289/500[, loss=0.0422, val_loss=0.0445, val_acc=0.98]\u001B[A\n",
      "Training:  58%|█████▊    | 289/500[, loss=0.0422, val_loss=0.0446, val_acc=0.98]\u001B[A\n",
      "Training:  58%|█████▊    | 290/500[, loss=0.0422, val_loss=0.0446, val_acc=0.98]\u001B[A\n",
      "Training:  58%|█████▊    | 290/500[, loss=0.0422, val_loss=0.0446, val_acc=0.98]\u001B[A\n",
      "Training:  58%|█████▊    | 291/500[, loss=0.0422, val_loss=0.0446, val_acc=0.98]\u001B[A\n",
      "Training:  58%|█████▊    | 291/500[, loss=0.0421, val_loss=0.0444, val_acc=0.98]\u001B[A\n",
      "Training:  58%|█████▊    | 292/500[, loss=0.0421, val_loss=0.0444, val_acc=0.98]\u001B[A\n",
      "Training:  58%|█████▊    | 292/500[, loss=0.0422, val_loss=0.0445, val_acc=0.98]\u001B[A\n",
      "Training:  59%|█████▊    | 293/500[, loss=0.0422, val_loss=0.0445, val_acc=0.98]\u001B[A\n",
      "Training:  59%|█████▊    | 293/500[, loss=0.0421, val_loss=0.0444, val_acc=0.98]\u001B[A\n",
      "Training:  59%|█████▉    | 294/500[, loss=0.0421, val_loss=0.0444, val_acc=0.98]\u001B[A\n",
      "Training:  59%|█████▉    | 294/500[, loss=0.0422, val_loss=0.0445, val_acc=0.98]\u001B[A\n",
      "Training:  59%|█████▉    | 295/500[, loss=0.0422, val_loss=0.0445, val_acc=0.98]\u001B[A\n",
      "Training:  59%|█████▉    | 295/500[, loss=0.0421, val_loss=0.0444, val_acc=0.98]\u001B[A\n",
      "Training:  59%|█████▉    | 296/500[, loss=0.0421, val_loss=0.0444, val_acc=0.98]\u001B[A\n",
      "Training:  59%|█████▉    | 296/500[, loss=0.0421, val_loss=0.0445, val_acc=0.98]\u001B[A\n",
      "Training:  59%|█████▉    | 297/500[, loss=0.0421, val_loss=0.0445, val_acc=0.98]\u001B[A\n",
      "Training:  59%|█████▉    | 297/500[, loss=0.0421, val_loss=0.0444, val_acc=0.98]\u001B[A\n",
      "Training:  60%|█████▉    | 298/500[, loss=0.0421, val_loss=0.0444, val_acc=0.98]\u001B[A\n",
      "Training:  60%|█████▉    | 298/500[, loss=0.0421, val_loss=0.0445, val_acc=0.98]\u001B[A\n",
      "Training:  60%|█████▉    | 299/500[, loss=0.0421, val_loss=0.0445, val_acc=0.98]\u001B[A\n",
      "Training:  60%|█████▉    | 299/500[, loss=0.0421, val_loss=0.0445, val_acc=0.98]\u001B[A\n",
      "Training:  60%|██████    | 300/500[, loss=0.0421, val_loss=0.0445, val_acc=0.98]\u001B[A\n",
      "Training:  60%|██████    | 300/500[, loss=0.042, val_loss=0.0444, val_acc=0.98] \u001B[A\n",
      "Training:  60%|██████    | 301/500[, loss=0.042, val_loss=0.0444, val_acc=0.98]\u001B[A\n",
      "Training:  60%|██████    | 301/500[, loss=0.0421, val_loss=0.0445, val_acc=0.98]\u001B[A\n",
      "Training:  60%|██████    | 302/500[, loss=0.0421, val_loss=0.0445, val_acc=0.98]\u001B[A\n",
      "Training:  60%|██████    | 302/500[, loss=0.0421, val_loss=0.0445, val_acc=0.98]\u001B[A\n",
      "Training:  61%|██████    | 303/500[, loss=0.0421, val_loss=0.0445, val_acc=0.98]\u001B[A\n",
      "Training:  61%|██████    | 303/500[, loss=0.042, val_loss=0.0444, val_acc=0.98] \u001B[A\n",
      "Training:  61%|██████    | 304/500[, loss=0.042, val_loss=0.0444, val_acc=0.98]\u001B[A\n",
      "Training:  61%|██████    | 304/500[, loss=0.042, val_loss=0.0445, val_acc=0.98]\u001B[A\n",
      "Training:  61%|██████    | 305/500[, loss=0.042, val_loss=0.0445, val_acc=0.98]\u001B[A\n",
      "Training:  61%|██████    | 305/500[, loss=0.042, val_loss=0.0445, val_acc=0.98]\u001B[A\n",
      "Training:  61%|██████    | 306/500[, loss=0.042, val_loss=0.0445, val_acc=0.98]\u001B[A\n",
      "Training:  61%|██████    | 306/500[, loss=0.042, val_loss=0.0445, val_acc=0.98]\u001B[A\n",
      "Training:  61%|██████▏   | 307/500[, loss=0.042, val_loss=0.0445, val_acc=0.98]\u001B[A\n",
      "Training:  61%|██████▏   | 307/500[, loss=0.0419, val_loss=0.0443, val_acc=0.98]\u001B[A\n",
      "Training:  62%|██████▏   | 308/500[, loss=0.0419, val_loss=0.0443, val_acc=0.98]\u001B[A\n",
      "Training:  62%|██████▏   | 308/500[, loss=0.042, val_loss=0.0444, val_acc=0.98] \u001B[A\n",
      "Training:  62%|██████▏   | 309/500[, loss=0.042, val_loss=0.0444, val_acc=0.98]\u001B[A\n",
      "Training:  62%|██████▏   | 309/500[, loss=0.042, val_loss=0.0445, val_acc=0.98]\u001B[A\n",
      "Training:  62%|██████▏   | 310/500[, loss=0.042, val_loss=0.0445, val_acc=0.98]\u001B[A\n",
      "Training:  62%|██████▏   | 310/500[, loss=0.042, val_loss=0.0445, val_acc=0.98]\u001B[A\n",
      "Training:  62%|██████▏   | 311/500[, loss=0.042, val_loss=0.0445, val_acc=0.98]\u001B[A\n",
      "Training:  62%|██████▏   | 311/500[, loss=0.042, val_loss=0.0445, val_acc=0.98]\u001B[A\n",
      "Training:  62%|██████▏   | 312/500[, loss=0.042, val_loss=0.0445, val_acc=0.98]\u001B[A\n",
      "Training:  62%|██████▏   | 312/500[, loss=0.042, val_loss=0.0445, val_acc=0.98]\u001B[A\n",
      "Training:  63%|██████▎   | 313/500[, loss=0.042, val_loss=0.0445, val_acc=0.98]\u001B[A\n",
      "Training:  63%|██████▎   | 313/500[, loss=0.042, val_loss=0.0445, val_acc=0.98]\u001B[A\n",
      "Training:  63%|██████▎   | 314/500[, loss=0.042, val_loss=0.0445, val_acc=0.98]\u001B[A\n",
      "Training:  63%|██████▎   | 314/500[, loss=0.042, val_loss=0.0446, val_acc=0.98]\u001B[A\n",
      "Training:  63%|██████▎   | 315/500[, loss=0.042, val_loss=0.0446, val_acc=0.98]\u001B[A\n",
      "Training:  63%|██████▎   | 315/500[, loss=0.042, val_loss=0.0446, val_acc=0.98]\u001B[A\n",
      "Training:  63%|██████▎   | 316/500[, loss=0.042, val_loss=0.0446, val_acc=0.98]\u001B[A\n",
      "Training:  63%|██████▎   | 316/500[, loss=0.042, val_loss=0.0446, val_acc=0.98]\u001B[A\n",
      "Training:  63%|██████▎   | 317/500[, loss=0.042, val_loss=0.0446, val_acc=0.98]\u001B[A\n",
      "Training:  63%|██████▎   | 317/500[, loss=0.042, val_loss=0.0445, val_acc=0.98]\u001B[A\n",
      "Training:  64%|██████▎   | 318/500[, loss=0.042, val_loss=0.0445, val_acc=0.98]\u001B[A\n",
      "Training:  64%|██████▎   | 318/500[, loss=0.042, val_loss=0.0445, val_acc=0.98]\u001B[A\n",
      "Training:  64%|██████▍   | 319/500[, loss=0.042, val_loss=0.0445, val_acc=0.98]\u001B[A\n",
      "Training:  64%|██████▍   | 319/500[, loss=0.042, val_loss=0.0445, val_acc=0.98]\u001B[A\n",
      "Training:  64%|██████▍   | 320/500[, loss=0.042, val_loss=0.0445, val_acc=0.98]\u001B[A\n",
      "Training:  64%|██████▍   | 320/500[, loss=0.042, val_loss=0.0445, val_acc=0.98]\u001B[A\n",
      "Training:  64%|██████▍   | 321/500[, loss=0.042, val_loss=0.0445, val_acc=0.98]\u001B[A\n",
      "Training:  64%|██████▍   | 321/500[, loss=0.042, val_loss=0.0445, val_acc=0.98]\u001B[A\n",
      "Training:  64%|██████▍   | 322/500[, loss=0.042, val_loss=0.0445, val_acc=0.98]\u001B[A\n",
      "Training:  64%|██████▍   | 322/500[, loss=0.042, val_loss=0.0445, val_acc=0.98]\u001B[A\n",
      "Training:  65%|██████▍   | 323/500[, loss=0.042, val_loss=0.0445, val_acc=0.98]\u001B[A\n",
      "Training:  65%|██████▍   | 323/500[, loss=0.042, val_loss=0.0445, val_acc=0.98]\u001B[A\n",
      "Training:  65%|██████▍   | 324/500[, loss=0.042, val_loss=0.0445, val_acc=0.98]\u001B[A\n",
      "Training:  65%|██████▍   | 324/500[, loss=0.0419, val_loss=0.0445, val_acc=0.98]\u001B[A\n",
      "Training:  65%|██████▌   | 325/500[, loss=0.0419, val_loss=0.0445, val_acc=0.98]\u001B[A\n",
      "Training:  65%|██████▌   | 325/500[, loss=0.0419, val_loss=0.0445, val_acc=0.98]\u001B[A\n",
      "Training:  65%|██████▌   | 326/500[, loss=0.0419, val_loss=0.0445, val_acc=0.98]\u001B[A\n",
      "Training:  65%|██████▌   | 326/500[, loss=0.0419, val_loss=0.0445, val_acc=0.98]\u001B[A\n",
      "Training:  65%|██████▌   | 327/500[, loss=0.0419, val_loss=0.0445, val_acc=0.98]\u001B[A\n",
      "Training:  65%|██████▌   | 327/500[, loss=0.0419, val_loss=0.0445, val_acc=0.98]\u001B[A\n",
      "Training:  66%|██████▌   | 328/500[, loss=0.0419, val_loss=0.0445, val_acc=0.98]\u001B[A\n",
      "Training:  66%|██████▌   | 328/500[, loss=0.0419, val_loss=0.0445, val_acc=0.98]\u001B[A\n",
      "Training:  66%|██████▌   | 329/500[, loss=0.0419, val_loss=0.0445, val_acc=0.98]\u001B[A\n",
      "Training:  66%|██████▌   | 329/500[, loss=0.0419, val_loss=0.0445, val_acc=0.98]\u001B[A\n",
      "Training:  66%|██████▌   | 330/500[, loss=0.0419, val_loss=0.0445, val_acc=0.98]\u001B[A\n",
      "Training:  66%|██████▌   | 330/500[, loss=0.0419, val_loss=0.0444, val_acc=0.98]\u001B[A\n",
      "Training:  66%|██████▌   | 331/500[, loss=0.0419, val_loss=0.0444, val_acc=0.98]\u001B[A\n",
      "Training:  66%|██████▌   | 331/500[, loss=0.0419, val_loss=0.0444, val_acc=0.98]\u001B[A\n",
      "Training:  66%|██████▋   | 332/500[, loss=0.0419, val_loss=0.0444, val_acc=0.98]\u001B[A\n",
      "Training:  66%|██████▋   | 332/500[, loss=0.0419, val_loss=0.0444, val_acc=0.98]\u001B[A\n",
      "Training:  67%|██████▋   | 333/500[, loss=0.0419, val_loss=0.0444, val_acc=0.98]\u001B[A\n",
      "Training:  67%|██████▋   | 333/500[, loss=0.0419, val_loss=0.0444, val_acc=0.98]\u001B[A\n",
      "Training:  67%|██████▋   | 334/500[, loss=0.0419, val_loss=0.0444, val_acc=0.98]\u001B[A\n",
      "Training:  67%|██████▋   | 334/500[, loss=0.0419, val_loss=0.0444, val_acc=0.98]\u001B[A\n",
      "Training:  67%|██████▋   | 335/500[, loss=0.0419, val_loss=0.0444, val_acc=0.98]\u001B[A\n",
      "Training:  67%|██████▋   | 335/500[, loss=0.0418, val_loss=0.0444, val_acc=0.98]\u001B[A\n",
      "Training:  67%|██████▋   | 336/500[, loss=0.0418, val_loss=0.0444, val_acc=0.98]\u001B[A\n",
      "Training:  67%|██████▋   | 336/500[, loss=0.0418, val_loss=0.0444, val_acc=0.98]\u001B[A\n",
      "Training:  67%|██████▋   | 337/500[, loss=0.0418, val_loss=0.0444, val_acc=0.98]\u001B[A\n",
      "Training:  67%|██████▋   | 337/500[, loss=0.0418, val_loss=0.0444, val_acc=0.98]\u001B[A\n",
      "Training:  68%|██████▊   | 338/500[, loss=0.0418, val_loss=0.0444, val_acc=0.98]\u001B[A\n",
      "Training:  68%|██████▊   | 338/500[, loss=0.0418, val_loss=0.0444, val_acc=0.96]\u001B[A\n",
      "Training:  68%|██████▊   | 339/500[, loss=0.0418, val_loss=0.0444, val_acc=0.96]\u001B[A\n",
      "Training:  68%|██████▊   | 339/500[, loss=0.0418, val_loss=0.0444, val_acc=0.98]\u001B[A\n",
      "Training:  68%|██████▊   | 340/500[, loss=0.0418, val_loss=0.0444, val_acc=0.98]\u001B[A\n",
      "Training:  68%|██████▊   | 340/500[, loss=0.0418, val_loss=0.0444, val_acc=0.98]\u001B[A\n",
      "Training:  68%|██████▊   | 341/500[, loss=0.0418, val_loss=0.0444, val_acc=0.98]\u001B[A\n",
      "Training:  68%|██████▊   | 341/500[, loss=0.0418, val_loss=0.0444, val_acc=0.96]\u001B[A\n",
      "Training:  68%|██████▊   | 342/500[, loss=0.0418, val_loss=0.0444, val_acc=0.96]\u001B[A\n",
      "Training:  68%|██████▊   | 342/500[, loss=0.0418, val_loss=0.0444, val_acc=0.96]\u001B[A\n",
      "Training:  69%|██████▊   | 343/500[, loss=0.0418, val_loss=0.0444, val_acc=0.96]\u001B[A\n",
      "Training:  69%|██████▊   | 343/500[, loss=0.0418, val_loss=0.0444, val_acc=0.96]\u001B[A\n",
      "Training:  69%|██████▉   | 344/500[, loss=0.0418, val_loss=0.0444, val_acc=0.96]\u001B[A\n",
      "Training:  69%|██████▉   | 344/500[, loss=0.0418, val_loss=0.0444, val_acc=0.96]\u001B[A\n",
      "Training:  69%|██████▉   | 345/500[, loss=0.0418, val_loss=0.0444, val_acc=0.96]\u001B[A\n",
      "Training:  69%|██████▉   | 345/500[, loss=0.0418, val_loss=0.0444, val_acc=0.96]\u001B[A\n",
      "Training:  69%|██████▉   | 346/500[, loss=0.0418, val_loss=0.0444, val_acc=0.96]\u001B[A\n",
      "Training:  69%|██████▉   | 346/500[, loss=0.0418, val_loss=0.0444, val_acc=0.96]\u001B[A\n",
      "Training:  69%|██████▉   | 347/500[, loss=0.0418, val_loss=0.0444, val_acc=0.96]\u001B[A\n",
      "Training:  69%|██████▉   | 347/500[, loss=0.0418, val_loss=0.0444, val_acc=0.96]\u001B[A\n",
      "Training:  70%|██████▉   | 348/500[, loss=0.0418, val_loss=0.0444, val_acc=0.96]\u001B[A\n",
      "Training:  70%|██████▉   | 348/500[, loss=0.0418, val_loss=0.0444, val_acc=0.96]\u001B[A\n",
      "Training:  70%|██████▉   | 349/500[, loss=0.0418, val_loss=0.0444, val_acc=0.96]\u001B[A\n",
      "Training:  70%|██████▉   | 349/500[, loss=0.0417, val_loss=0.0444, val_acc=0.96]\u001B[A\n",
      "Training:  70%|███████   | 350/500[, loss=0.0417, val_loss=0.0444, val_acc=0.96]\u001B[A\n",
      "Training:  70%|███████   | 350/500[, loss=0.0418, val_loss=0.0444, val_acc=0.96]\u001B[A\n",
      "Training:  70%|███████   | 351/500[, loss=0.0418, val_loss=0.0444, val_acc=0.96]\u001B[A\n",
      "Training:  70%|███████   | 351/500[, loss=0.0418, val_loss=0.0444, val_acc=0.96]\u001B[A\n",
      "Training:  70%|███████   | 352/500[, loss=0.0418, val_loss=0.0444, val_acc=0.96]\u001B[A\n",
      "Training:  70%|███████   | 352/500[, loss=0.0417, val_loss=0.0444, val_acc=0.96]\u001B[A\n",
      "Training:  71%|███████   | 353/500[, loss=0.0417, val_loss=0.0444, val_acc=0.96]\u001B[A\n",
      "Training:  71%|███████   | 353/500[, loss=0.0417, val_loss=0.0443, val_acc=0.96]\u001B[A\n",
      "Training:  71%|███████   | 354/500[, loss=0.0417, val_loss=0.0443, val_acc=0.96]\u001B[A\n",
      "Training:  71%|███████   | 354/500[, loss=0.0417, val_loss=0.0444, val_acc=0.96]\u001B[A\n",
      "Training:  71%|███████   | 355/500[, loss=0.0417, val_loss=0.0444, val_acc=0.96]\u001B[A\n",
      "Training:  71%|███████   | 355/500[, loss=0.0417, val_loss=0.0443, val_acc=0.96]\u001B[A\n",
      "Training:  71%|███████   | 356/500[, loss=0.0417, val_loss=0.0443, val_acc=0.96]\u001B[A\n",
      "Training:  71%|███████   | 356/500[, loss=0.0418, val_loss=0.0444, val_acc=0.96]\u001B[A\n",
      "Training:  71%|███████▏  | 357/500[, loss=0.0418, val_loss=0.0444, val_acc=0.96]\u001B[A\n",
      "Training:  71%|███████▏  | 357/500[, loss=0.0417, val_loss=0.0443, val_acc=0.96]\u001B[A\n",
      "Training:  72%|███████▏  | 358/500[, loss=0.0417, val_loss=0.0443, val_acc=0.96]\u001B[A\n",
      "Training:  72%|███████▏  | 358/500[, loss=0.0417, val_loss=0.0443, val_acc=0.96]\u001B[A\n",
      "Training:  72%|███████▏  | 359/500[, loss=0.0417, val_loss=0.0443, val_acc=0.96]\u001B[A\n",
      "Training:  72%|███████▏  | 359/500[, loss=0.0417, val_loss=0.0444, val_acc=0.96]\u001B[A\n",
      "Training:  72%|███████▏  | 360/500[, loss=0.0417, val_loss=0.0444, val_acc=0.96]\u001B[A\n",
      "Training:  72%|███████▏  | 360/500[, loss=0.0417, val_loss=0.0443, val_acc=0.96]\u001B[A\n",
      "Training:  72%|███████▏  | 361/500[, loss=0.0417, val_loss=0.0443, val_acc=0.96]\u001B[A\n",
      "Training:  72%|███████▏  | 361/500[, loss=0.0417, val_loss=0.0443, val_acc=0.96]\u001B[A\n",
      "Training:  72%|███████▏  | 362/500[, loss=0.0417, val_loss=0.0443, val_acc=0.96]\u001B[A\n",
      "Training:  72%|███████▏  | 362/500[, loss=0.0417, val_loss=0.0444, val_acc=0.96]\u001B[A\n",
      "Training:  73%|███████▎  | 363/500[, loss=0.0417, val_loss=0.0444, val_acc=0.96]\u001B[A\n",
      "Training:  73%|███████▎  | 363/500[, loss=0.0417, val_loss=0.0443, val_acc=0.96]\u001B[A\n",
      "Training:  73%|███████▎  | 364/500[, loss=0.0417, val_loss=0.0443, val_acc=0.96]\u001B[A\n",
      "Training:  73%|███████▎  | 364/500[, loss=0.0417, val_loss=0.0444, val_acc=0.96]\u001B[A\n",
      "Training:  73%|███████▎  | 365/500[, loss=0.0417, val_loss=0.0444, val_acc=0.96]\u001B[A\n",
      "Training:  73%|███████▎  | 365/500[, loss=0.0417, val_loss=0.0443, val_acc=0.96]\u001B[A\n",
      "Training:  73%|███████▎  | 366/500[, loss=0.0417, val_loss=0.0443, val_acc=0.96]\u001B[A\n",
      "Training:  73%|███████▎  | 366/500[, loss=0.0417, val_loss=0.0444, val_acc=0.96]\u001B[A\n",
      "Training:  73%|███████▎  | 367/500[, loss=0.0417, val_loss=0.0444, val_acc=0.96]\u001B[A\n",
      "Training:  73%|███████▎  | 367/500[, loss=0.0417, val_loss=0.0443, val_acc=0.96]\u001B[A\n",
      "Training:  74%|███████▎  | 368/500[, loss=0.0417, val_loss=0.0443, val_acc=0.96]\u001B[A\n",
      "Training:  74%|███████▎  | 368/500[, loss=0.0417, val_loss=0.0444, val_acc=0.96]\u001B[A\n",
      "Training:  74%|███████▍  | 369/500[, loss=0.0417, val_loss=0.0444, val_acc=0.96]\u001B[A\n",
      "Training:  74%|███████▍  | 369/500[, loss=0.0417, val_loss=0.0443, val_acc=0.96]\u001B[A\n",
      "Training:  74%|███████▍  | 370/500[, loss=0.0417, val_loss=0.0443, val_acc=0.96]\u001B[A\n",
      "Training:  74%|███████▍  | 370/500[, loss=0.0416, val_loss=0.0443, val_acc=0.96]\u001B[A\n",
      "Training:  74%|███████▍  | 371/500[, loss=0.0416, val_loss=0.0443, val_acc=0.96]\u001B[A\n",
      "Training:  74%|███████▍  | 371/500[, loss=0.0417, val_loss=0.0443, val_acc=0.96]\u001B[A\n",
      "Training:  74%|███████▍  | 372/500[, loss=0.0417, val_loss=0.0443, val_acc=0.96]\u001B[A\n",
      "Training:  74%|███████▍  | 372/500[, loss=0.0416, val_loss=0.0443, val_acc=0.96]\u001B[A\n",
      "Training:  75%|███████▍  | 373/500[, loss=0.0416, val_loss=0.0443, val_acc=0.96]\u001B[A\n",
      "Training:  75%|███████▍  | 373/500[, loss=0.0416, val_loss=0.0443, val_acc=0.96]\u001B[A\n",
      "Training:  75%|███████▍  | 374/500[, loss=0.0416, val_loss=0.0443, val_acc=0.96]\u001B[A\n",
      "Training:  75%|███████▍  | 374/500[, loss=0.0417, val_loss=0.0443, val_acc=0.96]\u001B[A\n",
      "Training:  75%|███████▌  | 375/500[, loss=0.0417, val_loss=0.0443, val_acc=0.96]\u001B[A\n",
      "Training:  75%|███████▌  | 375/500[, loss=0.0416, val_loss=0.0443, val_acc=0.96]\u001B[A\n",
      "Training:  75%|███████▌  | 376/500[, loss=0.0416, val_loss=0.0443, val_acc=0.96]\u001B[A\n",
      "Training:  75%|███████▌  | 376/500[, loss=0.0417, val_loss=0.0443, val_acc=0.96]\u001B[A\n",
      "Training:  75%|███████▌  | 377/500[, loss=0.0417, val_loss=0.0443, val_acc=0.96]\u001B[A\n",
      "Training:  75%|███████▌  | 377/500[, loss=0.0416, val_loss=0.0443, val_acc=0.96]\u001B[A\n",
      "Training:  76%|███████▌  | 378/500[, loss=0.0416, val_loss=0.0443, val_acc=0.96]\u001B[A\n",
      "Training:  76%|███████▌  | 378/500[, loss=0.0417, val_loss=0.0443, val_acc=0.96]\u001B[A\n",
      "Training:  76%|███████▌  | 379/500[, loss=0.0417, val_loss=0.0443, val_acc=0.96]\u001B[A\n",
      "Training:  76%|███████▌  | 379/500[, loss=0.0416, val_loss=0.0442, val_acc=0.96]\u001B[A\n",
      "Training:  76%|███████▌  | 380/500[, loss=0.0416, val_loss=0.0442, val_acc=0.96]\u001B[A\n",
      "Training:  76%|███████▌  | 380/500[, loss=0.0416, val_loss=0.0443, val_acc=0.96]\u001B[A\n",
      "Training:  76%|███████▌  | 381/500[, loss=0.0416, val_loss=0.0443, val_acc=0.96]\u001B[A\n",
      "Training:  76%|███████▌  | 381/500[, loss=0.0416, val_loss=0.0442, val_acc=0.96]\u001B[A\n",
      "Training:  76%|███████▋  | 382/500[, loss=0.0416, val_loss=0.0442, val_acc=0.96]\u001B[A\n",
      "Training:  76%|███████▋  | 382/500[, loss=0.0416, val_loss=0.0443, val_acc=0.96]\u001B[A\n",
      "Training:  77%|███████▋  | 383/500[, loss=0.0416, val_loss=0.0443, val_acc=0.96]\u001B[A\n",
      "Training:  77%|███████▋  | 383/500[, loss=0.0416, val_loss=0.0442, val_acc=0.96]\u001B[A\n",
      "Training:  77%|███████▋  | 384/500[, loss=0.0416, val_loss=0.0442, val_acc=0.96]\u001B[A\n",
      "Training:  77%|███████▋  | 384/500[, loss=0.0416, val_loss=0.0443, val_acc=0.96]\u001B[A\n",
      "Training:  77%|███████▋  | 385/500[, loss=0.0416, val_loss=0.0443, val_acc=0.96]\u001B[A\n",
      "Training:  77%|███████▋  | 385/500[, loss=0.0416, val_loss=0.0442, val_acc=0.96]\u001B[A\n",
      "Training:  77%|███████▋  | 386/500[, loss=0.0416, val_loss=0.0442, val_acc=0.96]\u001B[A\n",
      "Training:  77%|███████▋  | 386/500[, loss=0.0416, val_loss=0.0442, val_acc=0.96]\u001B[A\n",
      "Training:  77%|███████▋  | 387/500[, loss=0.0416, val_loss=0.0442, val_acc=0.96]\u001B[A\n",
      "Training:  77%|███████▋  | 387/500[, loss=0.0416, val_loss=0.0442, val_acc=0.96]\u001B[A\n",
      "Training:  78%|███████▊  | 388/500[, loss=0.0416, val_loss=0.0442, val_acc=0.96]\u001B[A\n",
      "Training:  78%|███████▊  | 388/500[, loss=0.0416, val_loss=0.0442, val_acc=0.96]\u001B[A\n",
      "Training:  78%|███████▊  | 389/500[, loss=0.0416, val_loss=0.0442, val_acc=0.96]\u001B[A\n",
      "Training:  78%|███████▊  | 389/500[, loss=0.0416, val_loss=0.0442, val_acc=0.96]\u001B[A\n",
      "Training:  78%|███████▊  | 390/500[, loss=0.0416, val_loss=0.0442, val_acc=0.96]\u001B[A\n",
      "Training:  78%|███████▊  | 390/500[, loss=0.0416, val_loss=0.0442, val_acc=0.96]\u001B[A\n",
      "Training:  78%|███████▊  | 391/500[, loss=0.0416, val_loss=0.0442, val_acc=0.96]\u001B[A\n",
      "Training:  78%|███████▊  | 391/500[, loss=0.0416, val_loss=0.0442, val_acc=0.96]\u001B[A\n",
      "Training:  78%|███████▊  | 392/500[, loss=0.0416, val_loss=0.0442, val_acc=0.96]\u001B[A\n",
      "Training:  78%|███████▊  | 392/500[, loss=0.0416, val_loss=0.0442, val_acc=0.96]\u001B[A\n",
      "Training:  79%|███████▊  | 393/500[, loss=0.0416, val_loss=0.0442, val_acc=0.96]\u001B[A\n",
      "Training:  79%|███████▊  | 393/500[, loss=0.0416, val_loss=0.0442, val_acc=0.96]\u001B[A\n",
      "Training:  79%|███████▉  | 394/500[, loss=0.0416, val_loss=0.0442, val_acc=0.96]\u001B[A\n",
      "Training:  79%|███████▉  | 394/500[, loss=0.0416, val_loss=0.0442, val_acc=0.96]\u001B[A\n",
      "Training:  79%|███████▉  | 395/500[, loss=0.0416, val_loss=0.0442, val_acc=0.96]\u001B[A\n",
      "Training:  79%|███████▉  | 395/500[, loss=0.0415, val_loss=0.0441, val_acc=0.96]\u001B[A\n",
      "Training:  79%|███████▉  | 396/500[, loss=0.0415, val_loss=0.0441, val_acc=0.96]\u001B[A\n",
      "Training:  79%|███████▉  | 396/500[, loss=0.0416, val_loss=0.0442, val_acc=0.96]\u001B[A\n",
      "Training:  79%|███████▉  | 397/500[, loss=0.0416, val_loss=0.0442, val_acc=0.96]\u001B[A\n",
      "Training:  79%|███████▉  | 397/500[, loss=0.0415, val_loss=0.0441, val_acc=0.96]\u001B[A\n",
      "Training:  80%|███████▉  | 398/500[, loss=0.0415, val_loss=0.0441, val_acc=0.96]\u001B[A\n",
      "Training:  80%|███████▉  | 398/500[, loss=0.0416, val_loss=0.0442, val_acc=0.96]\u001B[A\n",
      "Training:  80%|███████▉  | 399/500[, loss=0.0416, val_loss=0.0442, val_acc=0.96]\u001B[A\n",
      "Training:  80%|███████▉  | 399/500[, loss=0.0415, val_loss=0.0441, val_acc=0.96]\u001B[A\n",
      "Training:  80%|████████  | 400/500[, loss=0.0415, val_loss=0.0441, val_acc=0.96]\u001B[A\n",
      "Training:  80%|████████  | 400/500[, loss=0.0416, val_loss=0.0442, val_acc=0.96]\u001B[A\n",
      "Training:  80%|████████  | 401/500[, loss=0.0416, val_loss=0.0442, val_acc=0.96]\u001B[A\n",
      "Training:  80%|████████  | 401/500[, loss=0.0415, val_loss=0.0441, val_acc=0.96]\u001B[A\n",
      "Training:  80%|████████  | 402/500[, loss=0.0415, val_loss=0.0441, val_acc=0.96]\u001B[A\n",
      "Training:  80%|████████  | 402/500[, loss=0.0416, val_loss=0.0441, val_acc=0.96]\u001B[A\n",
      "Training:  81%|████████  | 403/500[, loss=0.0416, val_loss=0.0441, val_acc=0.96]\u001B[A\n",
      "Training:  81%|████████  | 403/500[, loss=0.0415, val_loss=0.0441, val_acc=0.96]\u001B[A\n",
      "Training:  81%|████████  | 404/500[, loss=0.0415, val_loss=0.0441, val_acc=0.96]\u001B[A\n",
      "Training:  81%|████████  | 404/500[, loss=0.0415, val_loss=0.0441, val_acc=0.96]\u001B[A\n",
      "Training:  81%|████████  | 405/500[, loss=0.0415, val_loss=0.0441, val_acc=0.96]\u001B[A\n",
      "Training:  81%|████████  | 405/500[, loss=0.0415, val_loss=0.0441, val_acc=0.96]\u001B[A\n",
      "Training:  81%|████████  | 406/500[, loss=0.0415, val_loss=0.0441, val_acc=0.96]\u001B[A\n",
      "Training:  81%|████████  | 406/500[, loss=0.0415, val_loss=0.0441, val_acc=0.96]\u001B[A\n",
      "Training:  81%|████████▏ | 407/500[, loss=0.0415, val_loss=0.0441, val_acc=0.96]\u001B[A\n",
      "Training:  81%|████████▏ | 407/500[, loss=0.0415, val_loss=0.0441, val_acc=0.96]\u001B[A\n",
      "Training:  82%|████████▏ | 408/500[, loss=0.0415, val_loss=0.0441, val_acc=0.96]\u001B[A\n",
      "Training:  82%|████████▏ | 408/500[, loss=0.0415, val_loss=0.0441, val_acc=0.96]\u001B[A\n",
      "Training:  82%|████████▏ | 409/500[, loss=0.0415, val_loss=0.0441, val_acc=0.96]\u001B[A\n",
      "Training:  82%|████████▏ | 409/500[, loss=0.0415, val_loss=0.0441, val_acc=0.96]\u001B[A\n",
      "Training:  82%|████████▏ | 410/500[, loss=0.0415, val_loss=0.0441, val_acc=0.96]\u001B[A\n",
      "Training:  82%|████████▏ | 410/500[, loss=0.0415, val_loss=0.0441, val_acc=0.96]\u001B[A\n",
      "Training:  82%|████████▏ | 411/500[, loss=0.0415, val_loss=0.0441, val_acc=0.96]\u001B[A\n",
      "Training:  82%|████████▏ | 411/500[, loss=0.0415, val_loss=0.044, val_acc=0.96] \u001B[A\n",
      "Training:  82%|████████▏ | 412/500[, loss=0.0415, val_loss=0.044, val_acc=0.96]\u001B[A\n",
      "Training:  82%|████████▏ | 412/500[, loss=0.0415, val_loss=0.0441, val_acc=0.96]\u001B[A\n",
      "Training:  83%|████████▎ | 413/500[, loss=0.0415, val_loss=0.0441, val_acc=0.96]\u001B[A\n",
      "Training:  83%|████████▎ | 413/500[, loss=0.0415, val_loss=0.044, val_acc=0.96] \u001B[A\n",
      "Training:  83%|████████▎ | 414/500[, loss=0.0415, val_loss=0.044, val_acc=0.96]\u001B[A\n",
      "Training:  83%|████████▎ | 414/500[, loss=0.0415, val_loss=0.0441, val_acc=0.96]\u001B[A\n",
      "Training:  83%|████████▎ | 415/500[, loss=0.0415, val_loss=0.0441, val_acc=0.96]\u001B[A\n",
      "Training:  83%|████████▎ | 415/500[, loss=0.0415, val_loss=0.044, val_acc=0.96] \u001B[A\n",
      "Training:  83%|████████▎ | 416/500[, loss=0.0415, val_loss=0.044, val_acc=0.96]\u001B[A\n",
      "Training:  83%|████████▎ | 416/500[, loss=0.0415, val_loss=0.0441, val_acc=0.96]\u001B[A\n",
      "Training:  83%|████████▎ | 417/500[, loss=0.0415, val_loss=0.0441, val_acc=0.96]\u001B[A\n",
      "Training:  83%|████████▎ | 417/500[, loss=0.0415, val_loss=0.044, val_acc=0.96] \u001B[A\n",
      "Training:  84%|████████▎ | 418/500[, loss=0.0415, val_loss=0.044, val_acc=0.96]\u001B[A\n",
      "Training:  84%|████████▎ | 418/500[, loss=0.0415, val_loss=0.0441, val_acc=0.96]\u001B[A\n",
      "Training:  84%|████████▍ | 419/500[, loss=0.0415, val_loss=0.0441, val_acc=0.96]\u001B[A\n",
      "Training:  84%|████████▍ | 419/500[, loss=0.0415, val_loss=0.044, val_acc=0.96] \u001B[A\n",
      "Training:  84%|████████▍ | 420/500[, loss=0.0415, val_loss=0.044, val_acc=0.96]\u001B[A\n",
      "Training:  84%|████████▍ | 420/500[, loss=0.0415, val_loss=0.0441, val_acc=0.96]\u001B[A\n",
      "Training:  84%|████████▍ | 421/500[, loss=0.0415, val_loss=0.0441, val_acc=0.96]\u001B[A\n",
      "Training:  84%|████████▍ | 421/500[, loss=0.0415, val_loss=0.044, val_acc=0.96] \u001B[A\n",
      "Training:  84%|████████▍ | 422/500[, loss=0.0415, val_loss=0.044, val_acc=0.96]\u001B[A\n",
      "Training:  84%|████████▍ | 422/500[, loss=0.0415, val_loss=0.0441, val_acc=0.96]\u001B[A\n",
      "Training:  85%|████████▍ | 423/500[, loss=0.0415, val_loss=0.0441, val_acc=0.96]\u001B[A\n",
      "Training:  85%|████████▍ | 423/500[, loss=0.0415, val_loss=0.044, val_acc=0.96] \u001B[A\n",
      "Training:  85%|████████▍ | 424/500[, loss=0.0415, val_loss=0.044, val_acc=0.96]\u001B[A\n",
      "Training:  85%|████████▍ | 424/500[, loss=0.0415, val_loss=0.044, val_acc=0.96]\u001B[A\n",
      "Training:  85%|████████▌ | 425/500[, loss=0.0415, val_loss=0.044, val_acc=0.96]\u001B[A\n",
      "Training:  85%|████████▌ | 425/500[, loss=0.0415, val_loss=0.044, val_acc=0.96]\u001B[A\n",
      "Training:  85%|████████▌ | 426/500[, loss=0.0415, val_loss=0.044, val_acc=0.96]\u001B[A\n",
      "Training:  85%|████████▌ | 426/500[, loss=0.0415, val_loss=0.044, val_acc=0.96]\u001B[A\n",
      "Training:  85%|████████▌ | 427/500[, loss=0.0415, val_loss=0.044, val_acc=0.96]\u001B[A\n",
      "Training:  85%|████████▌ | 427/500[, loss=0.0415, val_loss=0.044, val_acc=0.96]\u001B[A\n",
      "Training:  86%|████████▌ | 428/500[, loss=0.0415, val_loss=0.044, val_acc=0.96]\u001B[A\n",
      "Training:  86%|████████▌ | 428/500[, loss=0.0415, val_loss=0.044, val_acc=0.96]\u001B[A\n",
      "Training:  86%|████████▌ | 429/500[, loss=0.0415, val_loss=0.044, val_acc=0.96]\u001B[A\n",
      "Training:  86%|████████▌ | 429/500[, loss=0.0414, val_loss=0.044, val_acc=0.96]\u001B[A\n",
      "Training:  86%|████████▌ | 430/500[, loss=0.0414, val_loss=0.044, val_acc=0.96]\u001B[A\n",
      "Training:  86%|████████▌ | 430/500[, loss=0.0415, val_loss=0.044, val_acc=0.96]\u001B[A\n",
      "Training:  86%|████████▌ | 431/500[, loss=0.0415, val_loss=0.044, val_acc=0.96]\u001B[A\n",
      "Training:  86%|████████▌ | 431/500[, loss=0.0414, val_loss=0.044, val_acc=0.96]\u001B[A\n",
      "Training:  86%|████████▋ | 432/500[, loss=0.0414, val_loss=0.044, val_acc=0.96]\u001B[A\n",
      "Training:  86%|████████▋ | 432/500[, loss=0.0415, val_loss=0.044, val_acc=0.96]\u001B[A\n",
      "Training:  87%|████████▋ | 433/500[, loss=0.0415, val_loss=0.044, val_acc=0.96]\u001B[A\n",
      "Training:  87%|████████▋ | 433/500[, loss=0.0414, val_loss=0.0439, val_acc=0.96]\u001B[A\n",
      "Training:  87%|████████▋ | 434/500[, loss=0.0414, val_loss=0.0439, val_acc=0.96]\u001B[A\n",
      "Training:  87%|████████▋ | 434/500[, loss=0.0415, val_loss=0.044, val_acc=0.96] \u001B[A\n",
      "Training:  87%|████████▋ | 435/500[, loss=0.0415, val_loss=0.044, val_acc=0.96]\u001B[A\n",
      "Training:  87%|████████▋ | 435/500[, loss=0.0414, val_loss=0.0439, val_acc=0.96]\u001B[A\n",
      "Training:  87%|████████▋ | 436/500[, loss=0.0414, val_loss=0.0439, val_acc=0.96]\u001B[A\n",
      "Training:  87%|████████▋ | 436/500[, loss=0.0415, val_loss=0.044, val_acc=0.96] \u001B[A\n",
      "Training:  87%|████████▋ | 437/500[, loss=0.0415, val_loss=0.044, val_acc=0.96]\u001B[A\n",
      "Training:  87%|████████▋ | 437/500[, loss=0.0414, val_loss=0.0439, val_acc=0.96]\u001B[A\n",
      "Training:  88%|████████▊ | 438/500[, loss=0.0414, val_loss=0.0439, val_acc=0.96]\u001B[A\n",
      "Training:  88%|████████▊ | 438/500[, loss=0.0415, val_loss=0.044, val_acc=0.96] \u001B[A\n",
      "Training:  88%|████████▊ | 439/500[, loss=0.0415, val_loss=0.044, val_acc=0.96]\u001B[A\n",
      "Training:  88%|████████▊ | 439/500[, loss=0.0414, val_loss=0.0439, val_acc=0.96]\u001B[A\n",
      "Training:  88%|████████▊ | 440/500[, loss=0.0414, val_loss=0.0439, val_acc=0.96]\u001B[A\n",
      "Training:  88%|████████▊ | 440/500[, loss=0.0415, val_loss=0.044, val_acc=0.96] \u001B[A\n",
      "Training:  88%|████████▊ | 441/500[, loss=0.0415, val_loss=0.044, val_acc=0.96]\u001B[A\n",
      "Training:  88%|████████▊ | 441/500[, loss=0.0414, val_loss=0.0439, val_acc=0.96]\u001B[A\n",
      "Training:  88%|████████▊ | 442/500[, loss=0.0414, val_loss=0.0439, val_acc=0.96]\u001B[A\n",
      "Training:  88%|████████▊ | 442/500[, loss=0.0415, val_loss=0.044, val_acc=0.96] \u001B[A\n",
      "Training:  89%|████████▊ | 443/500[, loss=0.0415, val_loss=0.044, val_acc=0.96]\u001B[A\n",
      "Training:  89%|████████▊ | 443/500[, loss=0.0414, val_loss=0.0439, val_acc=0.96]\u001B[A\n",
      "Training:  89%|████████▉ | 444/500[, loss=0.0414, val_loss=0.0439, val_acc=0.96]\u001B[A\n",
      "Training:  89%|████████▉ | 444/500[, loss=0.0415, val_loss=0.0439, val_acc=0.96]\u001B[A\n",
      "Training:  89%|████████▉ | 445/500[, loss=0.0415, val_loss=0.0439, val_acc=0.96]\u001B[A\n",
      "Training:  89%|████████▉ | 445/500[, loss=0.0414, val_loss=0.0439, val_acc=0.96]\u001B[A\n",
      "Training:  89%|████████▉ | 446/500[, loss=0.0414, val_loss=0.0439, val_acc=0.96]\u001B[A\n",
      "Training:  89%|████████▉ | 446/500[, loss=0.0414, val_loss=0.0439, val_acc=0.96]\u001B[A\n",
      "Training:  89%|████████▉ | 447/500[, loss=0.0414, val_loss=0.0439, val_acc=0.96]\u001B[A\n",
      "Training:  89%|████████▉ | 447/500[, loss=0.0415, val_loss=0.044, val_acc=0.96] \u001B[A\n",
      "Training:  90%|████████▉ | 448/500[, loss=0.0415, val_loss=0.044, val_acc=0.96]\u001B[A\n",
      "Training:  90%|████████▉ | 448/500[, loss=0.0414, val_loss=0.0439, val_acc=0.96]\u001B[A\n",
      "Training:  90%|████████▉ | 449/500[, loss=0.0414, val_loss=0.0439, val_acc=0.96]\u001B[A\n",
      "Training:  90%|████████▉ | 449/500[, loss=0.0414, val_loss=0.0439, val_acc=0.96]\u001B[A\n",
      "Training:  90%|█████████ | 450/500[, loss=0.0414, val_loss=0.0439, val_acc=0.96]\u001B[A\n",
      "Training:  90%|█████████ | 450/500[, loss=0.0414, val_loss=0.0439, val_acc=0.96]\u001B[A\n",
      "Training:  90%|█████████ | 451/500[, loss=0.0414, val_loss=0.0439, val_acc=0.96]\u001B[A\n",
      "Training:  90%|█████████ | 451/500[, loss=0.0414, val_loss=0.0439, val_acc=0.96]\u001B[A\n",
      "Training:  90%|█████████ | 452/500[, loss=0.0414, val_loss=0.0439, val_acc=0.96]\u001B[A\n",
      "Training:  90%|█████████ | 452/500[, loss=0.0414, val_loss=0.0439, val_acc=0.96]\u001B[A\n",
      "Training:  91%|█████████ | 453/500[, loss=0.0414, val_loss=0.0439, val_acc=0.96]\u001B[A\n",
      "Training:  91%|█████████ | 453/500[, loss=0.0414, val_loss=0.0439, val_acc=0.96]\u001B[A\n",
      "Training:  91%|█████████ | 454/500[, loss=0.0414, val_loss=0.0439, val_acc=0.96]\u001B[A\n",
      "Training:  91%|█████████ | 454/500[, loss=0.0415, val_loss=0.0439, val_acc=0.96]\u001B[A\n",
      "Training:  91%|█████████ | 455/500[, loss=0.0415, val_loss=0.0439, val_acc=0.96]\u001B[A\n",
      "Training:  91%|█████████ | 455/500[, loss=0.0414, val_loss=0.0439, val_acc=0.96]\u001B[A\n",
      "Training:  91%|█████████ | 456/500[, loss=0.0414, val_loss=0.0439, val_acc=0.96]\u001B[A\n",
      "Training:  91%|█████████ | 456/500[, loss=0.0414, val_loss=0.0439, val_acc=0.96]\u001B[A\n",
      "Training:  91%|█████████▏| 457/500[, loss=0.0414, val_loss=0.0439, val_acc=0.96]\u001B[A\n",
      "Training:  91%|█████████▏| 457/500[, loss=0.0414, val_loss=0.0438, val_acc=0.96]\u001B[A\n",
      "Training:  92%|█████████▏| 458/500[, loss=0.0414, val_loss=0.0438, val_acc=0.96]\u001B[A\n",
      "Training:  92%|█████████▏| 458/500[, loss=0.0414, val_loss=0.0439, val_acc=0.96]\u001B[A\n",
      "Training:  92%|█████████▏| 459/500[, loss=0.0414, val_loss=0.0439, val_acc=0.96]\u001B[A\n",
      "Training:  92%|█████████▏| 459/500[, loss=0.0414, val_loss=0.0439, val_acc=0.96]\u001B[A\n",
      "Training:  92%|█████████▏| 460/500[, loss=0.0414, val_loss=0.0439, val_acc=0.96]\u001B[A\n",
      "Training:  92%|█████████▏| 460/500[, loss=0.0414, val_loss=0.0438, val_acc=0.96]\u001B[A\n",
      "Training:  92%|█████████▏| 461/500[, loss=0.0414, val_loss=0.0438, val_acc=0.96]\u001B[A\n",
      "Training:  92%|█████████▏| 461/500[, loss=0.0414, val_loss=0.0439, val_acc=0.96]\u001B[A\n",
      "Training:  92%|█████████▏| 462/500[, loss=0.0414, val_loss=0.0439, val_acc=0.96]\u001B[A\n",
      "Training:  92%|█████████▏| 462/500[, loss=0.0414, val_loss=0.0438, val_acc=0.96]\u001B[A\n",
      "Training:  93%|█████████▎| 463/500[, loss=0.0414, val_loss=0.0438, val_acc=0.96]\u001B[A\n",
      "Training:  93%|█████████▎| 463/500[, loss=0.0414, val_loss=0.0439, val_acc=0.96]\u001B[A\n",
      "Training:  93%|█████████▎| 464/500[, loss=0.0414, val_loss=0.0439, val_acc=0.96]\u001B[A\n",
      "Training:  93%|█████████▎| 464/500[, loss=0.0414, val_loss=0.0439, val_acc=0.96]\u001B[A\n",
      "Training:  93%|█████████▎| 465/500[, loss=0.0414, val_loss=0.0439, val_acc=0.96]\u001B[A\n",
      "Training:  93%|█████████▎| 465/500[, loss=0.0414, val_loss=0.0438, val_acc=0.96]\u001B[A\n",
      "Training:  93%|█████████▎| 466/500[, loss=0.0414, val_loss=0.0438, val_acc=0.96]\u001B[A\n",
      "Training:  93%|█████████▎| 466/500[, loss=0.0414, val_loss=0.0439, val_acc=0.96]\u001B[A\n",
      "Training:  93%|█████████▎| 467/500[, loss=0.0414, val_loss=0.0439, val_acc=0.96]\u001B[A\n",
      "Training:  93%|█████████▎| 467/500[, loss=0.0414, val_loss=0.0438, val_acc=0.96]\u001B[A\n",
      "Training:  94%|█████████▎| 468/500[, loss=0.0414, val_loss=0.0438, val_acc=0.96]\u001B[A\n",
      "Training:  94%|█████████▎| 468/500[, loss=0.0414, val_loss=0.0438, val_acc=0.96]\u001B[A\n",
      "Training:  94%|█████████▍| 469/500[, loss=0.0414, val_loss=0.0438, val_acc=0.96]\u001B[A\n",
      "Training:  94%|█████████▍| 469/500[, loss=0.0414, val_loss=0.0439, val_acc=0.96]\u001B[A\n",
      "Training:  94%|█████████▍| 470/500[, loss=0.0414, val_loss=0.0439, val_acc=0.96]\u001B[A\n",
      "Training:  94%|█████████▍| 470/500[, loss=0.0414, val_loss=0.0438, val_acc=0.96]\u001B[A\n",
      "Training:  94%|█████████▍| 471/500[, loss=0.0414, val_loss=0.0438, val_acc=0.96]\u001B[A\n",
      "Training:  94%|█████████▍| 471/500[, loss=0.0414, val_loss=0.0438, val_acc=0.96]\u001B[A\n",
      "Training:  94%|█████████▍| 472/500[, loss=0.0414, val_loss=0.0438, val_acc=0.96]\u001B[A\n",
      "Training:  94%|█████████▍| 472/500[, loss=0.0414, val_loss=0.0439, val_acc=0.96]\u001B[A\n",
      "Training:  95%|█████████▍| 473/500[, loss=0.0414, val_loss=0.0439, val_acc=0.96]\u001B[A\n",
      "Training:  95%|█████████▍| 473/500[, loss=0.0414, val_loss=0.0438, val_acc=0.96]\u001B[A\n",
      "Training:  95%|█████████▍| 474/500[, loss=0.0414, val_loss=0.0438, val_acc=0.96]\u001B[A\n",
      "Training:  95%|█████████▍| 474/500[, loss=0.0414, val_loss=0.0438, val_acc=0.96]\u001B[A\n",
      "Training:  95%|█████████▌| 475/500[, loss=0.0414, val_loss=0.0438, val_acc=0.96]\u001B[A\n",
      "Training:  95%|█████████▌| 475/500[, loss=0.0414, val_loss=0.0438, val_acc=0.96]\u001B[A\n",
      "Training:  95%|█████████▌| 476/500[, loss=0.0414, val_loss=0.0438, val_acc=0.96]\u001B[A\n",
      "Training:  95%|█████████▌| 476/500[, loss=0.0414, val_loss=0.0438, val_acc=0.96]\u001B[A\n",
      "Training:  95%|█████████▌| 477/500[, loss=0.0414, val_loss=0.0438, val_acc=0.96]\u001B[A\n",
      "Training:  95%|█████████▌| 477/500[, loss=0.0414, val_loss=0.0438, val_acc=0.96]\u001B[A\n",
      "Training:  96%|█████████▌| 478/500[, loss=0.0414, val_loss=0.0438, val_acc=0.96]\u001B[A\n",
      "Training:  96%|█████████▌| 478/500[, loss=0.0414, val_loss=0.0438, val_acc=0.96]\u001B[A\n",
      "Training:  96%|█████████▌| 479/500[, loss=0.0414, val_loss=0.0438, val_acc=0.96]\u001B[A\n",
      "Training:  96%|█████████▌| 479/500[, loss=0.0414, val_loss=0.0438, val_acc=0.96]\u001B[A\n",
      "Training:  96%|█████████▌| 480/500[, loss=0.0414, val_loss=0.0438, val_acc=0.96]\u001B[A\n",
      "Training:  96%|█████████▌| 480/500[, loss=0.0414, val_loss=0.0438, val_acc=0.96]\u001B[A\n",
      "Training:  96%|█████████▌| 481/500[, loss=0.0414, val_loss=0.0438, val_acc=0.96]\u001B[A\n",
      "Training:  96%|█████████▌| 481/500[, loss=0.0414, val_loss=0.0438, val_acc=0.96]\u001B[A\n",
      "Training:  96%|█████████▋| 482/500[, loss=0.0414, val_loss=0.0438, val_acc=0.96]\u001B[A\n",
      "Training:  96%|█████████▋| 482/500[, loss=0.0414, val_loss=0.0438, val_acc=0.96]\u001B[A\n",
      "Training:  97%|█████████▋| 483/500[, loss=0.0414, val_loss=0.0438, val_acc=0.96]\u001B[A\n",
      "Training:  97%|█████████▋| 483/500[, loss=0.0414, val_loss=0.0438, val_acc=0.96]\u001B[A\n",
      "Training:  97%|█████████▋| 484/500[, loss=0.0414, val_loss=0.0438, val_acc=0.96]\u001B[A\n",
      "Training:  97%|█████████▋| 484/500[, loss=0.0413, val_loss=0.0437, val_acc=0.96]\u001B[A\n",
      "Training:  97%|█████████▋| 485/500[, loss=0.0413, val_loss=0.0437, val_acc=0.96]\u001B[A\n",
      "Training:  97%|█████████▋| 485/500[, loss=0.0414, val_loss=0.0438, val_acc=0.96]\u001B[A\n",
      "Training:  97%|█████████▋| 486/500[, loss=0.0414, val_loss=0.0438, val_acc=0.96]\u001B[A\n",
      "Training:  97%|█████████▋| 486/500[, loss=0.0414, val_loss=0.0438, val_acc=0.96]\u001B[A\n",
      "Training:  97%|█████████▋| 487/500[, loss=0.0414, val_loss=0.0438, val_acc=0.96]\u001B[A\n",
      "Training:  97%|█████████▋| 487/500[, loss=0.0413, val_loss=0.0437, val_acc=0.96]\u001B[A\n",
      "Training:  98%|█████████▊| 488/500[, loss=0.0413, val_loss=0.0437, val_acc=0.96]\u001B[A\n",
      "Training:  98%|█████████▊| 488/500[, loss=0.0414, val_loss=0.0438, val_acc=0.96]\u001B[A\n",
      "Training:  98%|█████████▊| 489/500[, loss=0.0414, val_loss=0.0438, val_acc=0.96]\u001B[A\n",
      "Training:  98%|█████████▊| 489/500[, loss=0.0414, val_loss=0.0438, val_acc=0.96]\u001B[A\n",
      "Training:  98%|█████████▊| 490/500[, loss=0.0414, val_loss=0.0438, val_acc=0.96]\u001B[A\n",
      "Training:  98%|█████████▊| 490/500[, loss=0.0413, val_loss=0.0437, val_acc=0.96]\u001B[A\n",
      "Training:  98%|█████████▊| 491/500[, loss=0.0413, val_loss=0.0437, val_acc=0.96]\u001B[A\n",
      "Training:  98%|█████████▊| 491/500[, loss=0.0414, val_loss=0.0438, val_acc=0.96]\u001B[A\n",
      "Training:  98%|█████████▊| 492/500[, loss=0.0414, val_loss=0.0438, val_acc=0.96]\u001B[A\n",
      "Training:  98%|█████████▊| 492/500[, loss=0.0414, val_loss=0.0438, val_acc=0.96]\u001B[A\n",
      "Training:  99%|█████████▊| 493/500[, loss=0.0414, val_loss=0.0438, val_acc=0.96]\u001B[A\n",
      "Training:  99%|█████████▊| 493/500[, loss=0.0413, val_loss=0.0437, val_acc=0.96]\u001B[A\n",
      "Training:  99%|█████████▉| 494/500[, loss=0.0413, val_loss=0.0437, val_acc=0.96]\u001B[A\n",
      "Training:  99%|█████████▉| 494/500[, loss=0.0414, val_loss=0.0438, val_acc=0.96]\u001B[A\n",
      "Training:  99%|█████████▉| 495/500[, loss=0.0414, val_loss=0.0438, val_acc=0.96]\u001B[A\n",
      "Training:  99%|█████████▉| 495/500[, loss=0.0414, val_loss=0.0438, val_acc=0.96]\u001B[A\n",
      "Training:  99%|█████████▉| 496/500[, loss=0.0414, val_loss=0.0438, val_acc=0.96]\u001B[A\n",
      "Training:  99%|█████████▉| 496/500[, loss=0.0413, val_loss=0.0437, val_acc=0.96]\u001B[A\n",
      "Training:  99%|█████████▉| 497/500[, loss=0.0413, val_loss=0.0437, val_acc=0.96]\u001B[A\n",
      "Training:  99%|█████████▉| 497/500[, loss=0.0414, val_loss=0.0438, val_acc=0.96]\u001B[A\n",
      "Training: 100%|█████████▉| 498/500[, loss=0.0414, val_loss=0.0438, val_acc=0.96]\u001B[A\n",
      "Training: 100%|█████████▉| 498/500[, loss=0.0414, val_loss=0.0438, val_acc=0.96]\u001B[A\n",
      "Training: 100%|█████████▉| 499/500[, loss=0.0414, val_loss=0.0438, val_acc=0.96]\u001B[A\n",
      "Training: 100%|█████████▉| 499/500[, loss=0.0413, val_loss=0.0437, val_acc=0.96]\u001B[A\n",
      "Training: 100%|██████████| 500/500[, loss=0.0413, val_loss=0.0437, val_acc=0.96]\u001B[A\n"
     ]
    }
   ],
   "source": [
    "model = Network(2)\n",
    "model.add_layer(8, ReLU())\n",
    "model.add_layer(1, Tanh())\n",
    "\n",
    "stats = model.train(train=(x_train, y_train), validation=(x_val, y_val), metric=BinaryAccuracy(), loss=MeanSquaredError(), epochs=500, eta=0.01)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "<AxesSubplot: >"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 1500x500 with 2 Axes>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABMsAAAGsCAYAAADDpCDnAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8o6BhiAAAACXBIWXMAAA9hAAAPYQGoP6dpAACLxUlEQVR4nOzdeZxVdeH/8fc5d599mIFhlVUUR0QWt8SlTMLSRIyyRXGtLNE28QtoYqUI2a8yv6VS9MX0m0WppaKm5dpXLZElVJBFBWSbgdmXu53z++MuM+OwzL0zd85w7+v5eODce+45n/O5nzPgue/7WQzbtm0BAAAAAAAAkOl0BQAAAAAAAIC+grAMAAAAAAAAiCMsAwAAAAAAAOIIywAAAAAAAIA4wjIAAAAAAAAgjrAMAAAAAAAAiCMsAwAAAAAAAOIIywAAAAAAAIA4wjIAAAAAAAAgzu10BTJt374G2XbPl2sYUllZYcbKx6HR/s6i/Z1F+zuPa+CsTLZ/omwcGbjPy060v7Nof+dxDZxF+zurr9znpRyWBYNB3Xbbbfrb3/4mv9+vK6+8UldeeeUB933hhRf005/+VNu2bdPQoUP1rW99S+ecc07y9SlTpqihoaHDMW+++aby8/NTOs+h2LYy+gue6fJxaLS/s2h/Z9H+zuMaOIv2B/d52Y32dxbt7zyugbNof2c53f4ph2VLlizR+vXrtXz5cu3cuVM33XSTBg8erOnTp3fYb8OGDbruuus0d+5cnXXWWXrllVd0ww036E9/+pOOPfZY7dmzRw0NDXruuefk9/uTx+Xl5aV0HgAAAAAAAKCnpBSWNTc3a8WKFVq6dKkqKytVWVmpTZs26aGHHuoUYj3xxBM69dRTddlll0mShg8frn/84x966qmndOyxx2rLli3q37+/hg0b1q3zAAAAAAAAAD0lpQn+N2zYoEgkookTJya3TZ48WWvXrpVlWR32veiii/S9732vUxmJYZebN2/WyJEju30eAAAAAAAAoKek1LOsqqpKpaWl8nq9yW3l5eUKBoOqra1Vv379kttHjx7d4dhNmzbp1Vdf1SWXXCJJ2rJli1paWnTppZfqvffe07hx4zR//nyNHDkypfMcjmGk8g67LlFupsrHodH+zqL9nUX7O49r4KxMtj/XFAAAACmFZS0tLR0CLEnJ56FQ6KDH7d+/X3PmzNGkSZOSE/xv3bpVdXV1+s53vqOCggItXbpUl19+uZ588sm0z3MgmV7RihWznEX7O4v2dxbt7zyugbNofwAAAGRCSmGZz+frFFYlnrefpL+96upqXXHFFbJtW3fffbdMMzby8ze/+Y3C4bDy8/MlSXfddZfOOussPf/882md52BYUjw70f7Oov2dRfs7j2vgrL6ypDgAAACyU0phWUVFhWpqahSJROR2xw6tqqqS3+9XUVFRp/337NmTnOD/gQce6DB80uv1dug95vP5NHToUO3Zs0eTJk1K6TyHwpLi2Y32dxbt7yza33lcA2fR/gAAAMiElCb4HzdunNxut9asWZPctmrVKo0fPz7ZYyyhublZV199tUzT1IMPPqiKiorka7Zt65Of/KQeeeSRDvt/8MEHGjVqVErnAQAAAAAAAHpKSslTIBDQjBkztHDhQq1bt07PPfecli1bluw9VlVVpdbWVknSfffdp23btmnx4sXJ16qqqtTQ0CDDMHT22WfrF7/4hV5//XVt2rRJc+fO1cCBA3XWWWcd9jwAAAAAAABAJqQ0DFOS5s2bp4ULF2r27NkqKCjQnDlzNG3aNEnS1KlTtWjRIs2cOVPPPPOMWltbNWvWrA7HX3TRRbrzzjt14403yu1267vf/a4aGxt16qmn6v7775fL5TrseQAAAAAAAIBMMGw7u2f7qK7O3AT/5eWFGSsfh0b7O4v2dxbt7zyugbMy2f6JspG+UCikmTNn6pZbbtEpp5xywH3efvtt3XrrrXr33Xc1ZswY3XbbbTr++ONTPhf3edmJ9ncW7e88roGzaH9n9ZX7PCYAAwAAQI8IBoP6zne+o02bNh10n+bmZn31q1/VlClT9Mgjj2jixIn62te+pubm5l6sKQAAwMERlgEAAKDbNm/erM9//vPatm3bIfdbuXKlfD6f5s6dq9GjR2vBggXKz8/X008/3Us1BQAAOLSU5yyDJNuWq2azVHqC0zUBAADoE/71r3/plFNO0be//W2deOKJB91v7dq1mjx5sgzDkCQZhqFJkyZpzZo1mjlzZi/VFugC25a7+i0ZrTU9XLChcMVEyZvf4TwKNysycJJkHuAjWqRVnt2rZOVXKFo6pmerEw3Ls+dNKRqK1c6QVBeQp74l64agRYuHyyo6yulqADgCEJalwfv+cypeeYV0+rekid9zujoAAACO+9KXvtSl/aqqqjRmTMcP+2VlZYccunkw8bytxyXKzVT5OLS+0v6e9/+u4icvz0jZoaFTVT/jYUmS9/1nVfTklZKk5ik3qPnUGzvtX/D8jfK/+6gkqeaSZxUtH9djdcn71xLlvfmrTtuLe+wMfYdturV/9uuy8yucrsoh9ZW/A7mK9ndWJts/lTIJy9JRv12SFKp+z+GKAAAAHFlaWlrk9Xo7bPN6vQqFQimXVVaW2cUYMl0+Ds3x9n8nfq/vL5GKhvRMmZFWaf8WeWs3tU0y/XbbZ4q8xi3KO9Dk03Wbkw9Lw9uk8pN7pj6S1LAl9rNoSOy9Zqt9m2VEgyqz90jlPdw7L0Mc/zuQ42h/Zznd/oRladi4t1knS3p3T70GOl0ZAACAI4jP5+sUjIVCIfn9/pTL2rcvc6thlpUVZqx8HFpfaf+8/buVJ6n52C+oeeotPVKm2bBT/ZafLLt5v/ZV1UuGofz9exSIvx6ur1ZddUOn40qbquWKP26s3q3WA+yTruL6Knkk1U/9gUKjPtVn2r+nFf/pQnl2r1L9ng8VKui59suEbL0GRwra31mZbP9E2V1BWJaG5oglSQpHIg7XBAAA4MhSUVGh6urqDtuqq6s1YMCAlMuybWX0g0ymy8ehOd3+RmttrB6+kh6rR9RXEivbCssONUve/OR5YuesOeC5zPb7tBx4n3Qlzm995H063f49zUq0fWvtEfO+su0aHGlof2c53f6shpkOI9ZsBn9zAAAAUjJhwgStXr1advw+yrZtvfnmm5owYYLDNQM6SgRUVk8OTXQHZJuxYchmMFZ+h7AsWNv5mEirjEjLoffphkQ9EmFStrLj17F9ewPAwRCWpcGIh2XEzAAAAIdXVVWl1tZWSdL06dNVX1+v22+/XZs3b9btt9+ulpYWnXfeeQ7XEugo2ePKX9qDhRrJ8sz4KptmsG21TfMAQY75kXDsQPukzbZkBOsk9fD77IOS7d6uvQHgYAjL0mArvtS5LIdrAgAA0PdNnTpVK1eulCQVFBTovvvu06pVqzRz5kytXbtW999/v/Ly8hyuJdBRIqSye7jH1Ud7OHXoWRYNSu16kX30dalne5YZwXoZthWvVzauf9nGbjcMEwAOhznL0pEYhklYBgAA0MnGjRsP+fyEE07Qo48+2ptVAlKWnLOsh1eITM6dFQ+9PtpTzGytkVUQaHveqWdZz/WMStTBdudJLl+PldsXJYbTfrQ9AeBA6FmWBoM5ywAAAIDsZdvJUKqn5/JKhG9ma41k2zI+MiywU0+y1kO/3h3J95jlQzAlyY6/x4+2JwAcCGFZOozYMEzZ9CwDAAAAsk6kRYYVktTzQVKyh1NrrYxwkwwrIkmKFgyJbT/IHGUHe707jEwsYtBHJULPHp3zDUDWIixLB8MwAQAAgKyVCFRs0yN5enY+PbvdMMzkUE+XT1bBoNj2Tj3LYs+jJSPbnvfQCJdMzcvWFyXnimMYJoAuICxLh0GzAQAAANkqMVTP9pW0jSrpIe1Xw0yszGj5SjqtkpmQ2CdaNDxWNzsqI9TQI3UxcmgY5sHaFwAOhNQnHck5y+hZBgAAAGSbRI+rTAxPbL8qY/tFBA7W8yk5VLJgoGy3/4D7pMvM0CIGfVGy3SOtnVYcBYCPYjXMNCQn+BcT/AMAAABHNCuiwr9/W66azclNZrBeUtuk8D16ungw5fnwn3Lv35jcZvli5wqs/Y18m59I7u+q3xbfp1SWv1Suxl0qfmJ2MjjrDlfjLkk5MgzTWyjbcMmwoyr902dlm333o7AhSW5TJRGrT3zijPY7RtHCofJ+8HeFh3xMTaff4nSVgIzru/9C9GX0LAMAAACygrtqvfzvPnrA1yKlR/f4+aL9xkqSzHCTFG6KbSsdq2i/2LlczXvlat7b+bjSoxUtHStX4y65azb1aJ0i8TplNcNQtN/Rcu/bIPe+d5yuTZf0lQ/rnqr/dHjcPPm6jATJQF/SV/7+HVGM+LwFTPAPAAAAHNkS83ZFikeo8Ywftr1gehQeNKXHzxftN1b7L/m7zMadbecZfJJkehTpN1ZGvFdbe3Zef0XKK1U/YILcu1dJPfilve0rUqRiUo+V15fVzlgh9541TlfjsAxDKi7KU119c0+t5ZC2whfny9WwvcM2s7VGUcIyZDnCsnQkJvh3+l8uAAAAAN2SnJ+scKjCwz/eK+eMlh2jaNkxnbZHBk4+5HG2t0Dho87KVLWynu0v7bVr3B2GIam8UOHqBsc/ckYLBncKywwWSUAOYIL/dCR7lhGWAQAAAEey5IqQOTBvF5CqAy3+kFgYAshmhGXpMFyxHwzDBAAAAI5oubQiJJCqA4XIRpCeZch+hGVpMMx4zzKn+8QCAAAA6BYjMQyTOZiATuhZhlxFWJYOepYBAAAAWcGMD8O0GYYJdHKgENkgLEMOICxLQ3I1THqWAQAAAEe0xAd/i2GYQCcHCpETi2IA2YywLC2xZqNnGQAAAHBkS3zwtxmGCXTSPkS2PAWSWA0TuYGwLA2GmQjL6FkGAAAAHMlYDRM4uPYhcrRkpCR6liE3EJalg2GYAAAAQFZgNUzg4NqHyNHiWFjGnGXIBYRlaTCY4B8AAAA48tmWjGCdJFbDBA6kQ8+y4uGSWA0TucHtdAWOSGa8ZxnDMAEAAIC0GM3V8n74qvTRe2pD0m6/vA2tnV7qcZGW5D297SvO8MmAI0+HOcvyBkiSjJZq+Tb91aEa9YLe/DcInUTLjpHKpzhdDcKydBhK9Czjbw4AAACQjqJnvibvztcP/nov1sXyFkouby+eETiw6qaQ/vKfXZp16gg11LXoqbf36rPHD9SAQp8zFXIHkg+jxSMkSWa4SUV/+4Yz9elFvflvENrYLp/0Xx84XY3Uw7JgMKjbbrtNf/vb3+T3+3XllVfqyiuvPOC+L7zwgn76059q27ZtGjp0qL71rW/pnHPOkSTZtq2lS5fq4YcfVm1trcaPH69bbrlFY8aMkSS9/fbbuuiiizqUV1lZqUceeSTVKvc8I/7DZhgmAAAAkA53zVZJUnjAibI9eR1e83pcCoWjvVaX4JjP9tq5ektrOKrGYCSlY/K8buX7XB22NbRG9P9e2KLmUFTXnj5CBT6XalrC+u+X39eO2pZOZbhdhj57/EBNO6a/JMnndqnQ71ZtS1iRaOqfn/rle2XG54w+kMZgRK3x3xW/x6UCX9tHXNu2tb85LLvdXNPBqKWl//eB1u9qOGiZpmHonLHlmjlhkA5+5q5pDEV17z/fl9/j0nnHDtD/vrlDoYilcNSWx20qHLHkcZuKRC2dObpMT7+zV+9WNel3/96hAp9bexqCWrFmp2765NF6e3eDXtq8T1Yvz509w3ONBtl79avn8nW5+wuaYL3Vq+d3gmEYHX5vjmyGSvM8GlDg3BcCdS0R7WkMdqlNqwoqNc3tl9SY+YodQsph2ZIlS7R+/XotX75cO3fu1E033aTBgwdr+vTpHfbbsGGDrrvuOs2dO1dnnXWWXnnlFd1www3605/+pGOPPVYPP/ywli1bpkWLFmnEiBH69a9/rWuuuUYrV65UIBDQ5s2bNW7cOC1durStsu6+0RHOMGP/AzHpWQYAAACkzrZlxFfUq59+v6zCwcmXDEMqLy9UfXWDeuqz6u76Vq39sF7DSgM6bmBhp9fX7azXrnf2yjCkSUOLVV7gUC+eHvDW7gb939b9+t0b29USTi2c8rgMzT5pmMaP6KeGhlaFo5Z+89o27ahtlST9Y1N1l8r56Qtb9dMXtqZc9wMZ0S+gy08+Sm6zc2y1bme9/rR2p6z474lpSBdPGKwJg4tkS3pk3S6t3lGX1nl//do2/fq1bd2oeWdPvrXnkK+v+bA++bgpFFVTKBYC7m8O66a/vt2jdUnFz/Xx+KNW3aYLJV3oWF2QpmZJXfvr67j8iEurIs53TEopfWpubtaKFSu0dOlSVVZWqrKyUps2bdJDDz3UKSx74okndOqpp+qyyy6TJA0fPlz/+Mc/9NRTT+nYY4/Vo48+qiuvvFIf/3jsL97ChQt18skn680339Tpp5+uLVu2aPTo0erfv38PvdWeYxixdRGY4B8AAABIQ7hZhhWWlPmJ9Z95Z69+9Ld31Rr/8PXxo8tV7G/7GBSKWlr59t7k80KfW4vOH6dTRhw5E/5HLFt/WrNT/3xvv157vya53TTU5Z5RtqRw1I4FRAcIiXzx3k8Jk4aV6IpThsltdlwz7p09DVr22rZkr7Zou8DTlWI3LcuW3t/fooVPbzzkfolyo7a0Ys1OrViz84CvJ4wsy9e1U0eo0Hfgj8Pbapr1q39+oNrmUGoVPgi3K9ZGkagll2nIZRqKWLa8LlOtEUt5HpcagxH5PaYsW/reJ0Zrf8jSY2/u0Hc/PlqvbN2vlW/vUZHfo2unjtCwksBhzojuMAypuDhPdXXNPRbYO2l7bYvu/ef72t/UM7/P6fB7XPrKlKGaPKzksPsO7xeQ3+NyuF9ZimHZhg0bFIlENHHixOS2yZMn695775VlWTLb/UN50UUXKRwOdyqjoSHW3XXu3LkaOnRocnuim2Pi9S1btuiYY45J7d30luT7zIK/OQAAAEAvM1tjgY7t8kluf4+Wvb85pEjUVr7PpRWrd+q/X3lfklTsd6uuNaLnD9I7anR5niJRWx/UtGj+k+9o6SUTVBLwqF9e29Al27a1Ys0u/XntToXTGFKYinyvW5efMkzjBx145qRQ1NKvX9umdR/WqSVsqbrdB+HjBhbq3GP660uThxxyCGN7tm3r8bf26NmNVXK5TYVDUdmShpUEdO3UESoJeLpUzsShxfrS5LbPea+9v19PvLVHn584RCcMTm0WqNrmsH75z/eSPds+yusyddEJg3TWmDJJ0stb9unPa3cpFL82pQGPvn76CA0rTS1cmji0WBeOH5TSMT0p0bvyqilDZNvSWWPKtWDaWMfqk2sS7V9d7cmKsGzi0GJ99viBTlejy7r4T1bGpRSWVVVVqbS0VF5v2/8wysvLFQwGVVtbq379+iW3jx49usOxmzZt0quvvqpLLrlEkjRlSsfVDVasWKFIJKLJkydLioVllmXpggsuUENDg84880zNnTtXBQUFKb3BTDR0omeZKbvPXMhck2h32t8ZtL+zaH/ncQ2clcn255oCvcOMD8G0fCU98hevORTVK1v36fG39nToWZVw2UnD9I2pI7TmwzqtbTfUra41rD+s3qkhxX795osnyusydcX/rtHGvY36wv+skiSdMaqfPhmff+uVrfv17Maqbte3q/7r8Xe6vG+ex6UZJwzUWWPKNGloScrnMozYXGMXjh8YDwp6ZhjsqSP66dQR/Q6/4wGU5Hk0/9yuh0RnjC7TGaPL0joXALSXUljW0tLSISiTlHweCh28S9/+/fs1Z84cTZo0KTnBf3tr167V4sWLddVVV6l///4Kh8Pavn27hg4dqjvuuEP19fVatGiRbrzxRv3qV79KpcoqK+s8J0F37SuKTUBq2HZGykfX0f7Oov2dRfs7j2vgLNofOHIZrbWSJNtfknYZO2pb9MfVO9Ucimr1h3XaVtM22bxpKDmPVf8Cr755xgiZhqHJw0o6DQO64uSj5HEbyvfGPhp9/1Njdf0j61XXElbEsvXy1v16eev+5P4u09A3p45IuZdUqv6xqVqPrtulcPTgidXwfgF9Y+pIFfvdGt4vr8u9vwAAh5ZSWObz+TqFYonnfv+Bu09XV1friiuukG3buvvuuzsM1ZSk1atX65prrtGZZ56pG264QZLk8Xj02muvyefzyeOJ/YN/55136uKLL9aePXtUUVHR5Trv29dzE4MmNDYFJUmmrIyUj8MzjNiHJNrfGbS/s2h/53ENnJXJ9k+UDSCzEsMwrRTDsm01Lbr5yXdU3xrRvqZQch4ySSrwuXTyUaW64pRh8rhMXbI81jNs8rCSQw5FLMnrGDCNHVCgp79+qiRp455G/fZfbfNvBeLz7kwYUpxSvdMxYUixvn326MPvCADocSmFZRUVFaqpqVEkEkmuTFlVVSW/36+ios7frOzZsyc5wf8DDzzQYZimJL3++uv6+te/rtNPP10/+clPOgRpHx1umRjWmWpYZtvKwAeZxAT/dobKR1fR/s6i/Z1F+zuPa+As2h84ciVWwrR9JV3a37Ztvfp+jW57eqP2N7fNi3z8oEKdObpMAY9L048d0CH4OntMmV7esk+XTBqSdj2PqSjQnRccl/bxAIAjU0ph2bhx4+R2u7VmzZrknGOrVq3S+PHjO/UYa25u1tVXXy3TNPXAAw90WtXy3Xff1bXXXqszzjhD/+///b9k+CZJmzdv1qxZs/TXv/5Vw4YNkyS98847crvdGj58eFpvtCcZZttqmNyjAwAAAKkx48Mwu7oS5hNv7dEPnnlXUmxuriWfPU7FAbeO7l8gl3ngXmM/+sw41beG1b/A1yN1BgDkjpTCskAgoBkzZmjhwoW64447tHfvXi1btkyLFi2SFOtlVlhYKL/fr/vuu0/btm3T7373u+RrUmy4ZmFhob7//e9r0KBBmjdvnmpq2ibhLCws1KhRozR8+HDdcsstmj9/vurr63Xrrbdq1qxZKi7OfJfnwzISYRkAAACAVBmJ1TAPMwyztiWsuX99W6t31EmSBhR4NfecMTplxOFDNp/bJCgDAKQlpbBMkubNm6eFCxdq9uzZKigo0Jw5czRt2jRJ0tSpU7Vo0SLNnDlTzzzzjFpbWzVr1qwOx1900UX67ne/q9WrV0uSzj777A6vJ47/1a9+pdtvv11f/vKXZZqmLrjgAs2dOzfNt9mzDKOtZxkAAACA1HRYDfMQfvbClmRQVjmwUL/+4olyH6QnGQAAPSXlsCwQCGjx4sVavHhxp9c2btyYfPz0008fspz2+x7IoEGDdM8996RavV6RGIZpMggTAAAASFlXVsN89b39evLtvZKkm84Zo/MrKwjKAAC9IuWwDPQsAwAAAFLl3fyECl5dJFkRmS3Vkg4+Z9kH+5p0+982SZIumTREnztxcK/VEwAAwrI0JMIyepYBAAAAXRN4+/dy1X+QfG4bpqJl4zrss3FPo/75/n49+MYONbRGNLjYr2tPH9HLNQUA5DrCsnQYse7fBmEZAAAA0CWJSf0bpy5UeNBJsvIrZOUPlCRZtq3fvLZNS//vg+Qd9gmDi7To/HHK87ocqjEAIFcRlqXBNGP/wzZthmECAAAAXZGY1D9cMVGRARM6vLbwqY166p3Y/GTHDSzU9PGDNOv4AXLH5woGAKA3EZalIzlnGQAAAICuSE7q/5EVMHfWteqpd/bKNKSbp43VZ8cPVHl5oaqrG2QzkAMA4AC+qklDYjVMJvgHAAAAusCKyAzVxx5+ZFL/5zfFJvufOLRYFxw/sNerBgDARxGWpcFkgn8AAACgy4xgXfKx7Svq8FoiLPv4mPJerRMAAAdDWJaGRM8ywjIAAADg8Mz4EEzLWySZbTPBVDcGtW5nrMfZ2UcTlgEA+gbCsrTEZiszDVtMpAAAAAAcWmIlTPsjQzBf2LxPtqTjBxWqotDnQM0AAOiMCf7TYHZYlccWU/0DAAAAB5dYCdPyl0iSopatzdVNeuw/uyUxBBMA0LcQlqXDdLU9tm2yMgAAAOAQ2q+EuaO2RTf+5W1trm6SJPncps49tr+DtQMAoCPCsjQkJviXJNuKSi7XIfYGAAAAclv7nmU/+tu7yaDs6P75mvfJozWoyO9g7QAA6IiwLA1Gu2GYtiw6lgEAAACHkJizLOQp1podsZUxV1wxRSP65TlZLQAADoiwLB1GWzxmWRYdywAAAIADMEIN8m5ZKc/O1yVJO0MBRW1pcLGfoAwA0GcRlqXBNNqlY5blXEUAAACAPizw5q+Uv+ru5PNNTQFJ0knDShyqEQAAh0dYlob2wzAtyxIdywAAAIDOXPUfSJLC5cersewELdk4XpJ08vASB2sFAMChEZalocOcZVbUwZoAAAAAfVdiYv+WCVfrxk2V2t5apWMGFOgTY1n9EgDQd5mH3wUfZbYPy2zbwZoAAAAAfZfRWitJ+k+NqeferZLLkG6edrTcJktkAQD6LsKytLQPy5izDAAAADgQMx6W/XFDsyTpS5OH6tiKQgdrBADA4RGWpaFjzzLCMgAAAOBAjPgwzP/UuOVzm5p98jBnKwQAQBcQlqXBNNq6jVvMWQYAAAB0ZkVkBuskSbV2gT593AAVBzwOVwoAgMMjLEuDYZqy7HhgZjFnGQAAAPBRRrA++TjoLqRXGQDgiEFYlgbTkCzFwjKGYQIAAAAH0FojSaq383TlaaM0pDjgcIUAAOgawrI0GIbRFpZZhGUAAADBYFDz58/XlClTNHXqVC1btuyg+77yyiv67Gc/q4kTJ+ryyy/X1q1be7Gm6C3vvL9NklSnAl184iCHawMAQNcRlqXJjjedRc8yAAAALVmyROvXr9fy5ct166236p577tHTTz/dab9Nmzbpa1/7ms455xz9+c9/1nHHHafZs2erqanJgVojU8JRS0+9uVGSZARKle91O1wjAAC6jrAsTYmZyhiGCQAAcl1zc7NWrFihBQsWqLKyUueee66uvvpqPfTQQ532/f3vf6+JEyfqhhtu0KhRo3TjjTeqsLBQjz/+uAM1RyY0BiOa/8Q7aqnfJ0kq7TfA4RoBAJAawrI0WfGmYxgmAADIdRs2bFAkEtHEiROT2yZPnqy1a9fK+si90vbt23XCCScknxuGobFjx2rNmjW9VV1kQF1LWAuf2qBvP7peX35glV7YvE9lZqMkyZXfz+HaAQCQGvpDpykxZ5nsqLMVAQAAcFhVVZVKS0vl9XqT28rLyxUMBlVbW6t+/fp12L5nz54Ox+/evVvFxcUpn9cw0q9zV8rNVPl9kX/975T3f4tkWOG0ji+2bP042rZKvOGXfEZUsiXbX5JSW+Zi+/cltL/zuAbOov2dlcn2T6VMwrI02ckJ/u3D7AkAAJDdWlpaOgRlkpLPQ6FQh+3nnXeevvGNb+j888/XGWecoccff1z/+c9/dMopp6R83rKywvQr3QfK71O2/lUK1ad9uFeS96MfQuK3yYGjz1CgPPW2zKn274Nof+dxDZxF+zvL6fYnLEtT2zBMepYBAIDc5vP5OoViied+v7/D9jPPPFPf/OY3NWfOHEWjUZ1yyim68MIL1djYmPJ59+1rkJ2B7y0NI3aTnqny+6KSxn1yS2o4926FB03p8nHVjSH9+O+b9c7eRg0o8Oq+L0yQy2xLzWx3nuy8cqm6octl5mL79yW0v/O4Bs6i/Z2VyfZPlN0VhGVpSvYsE397AABAbquoqFBNTY0ikYjc7tjtZVVVlfx+v4qKijrtf+211+qqq65SQ0ODysrKdMMNN2jIkCEpn9e2ldEPMpkuvy8xWmslSZHSsYoWHtWlY3bVt+ryx1Zrf3OeCnyFmj/9OKm4VJ2+Sk6zDXOp/fsi2t95XANn0f7Ocrr9U57gPxgMav78+ZoyZYqmTp2qZcuWHXTfF154QRdeeKEmTpyoCy64QH//+987vP7EE0/ok5/8pCZMmKBvfvOb2r9/f/I127Z111136dRTT9XJJ5+sJUuWdJog1kmJOctYDRMAAOS6cePGye12d5ikf9WqVRo/frxMs+Pt5hNPPKHbb79dXq9XZWVlam1t1euvv57WMEz0ENuWGQ/LLH9plw55flO1Ln9otfY3hzWqLE+/+8oknXRU144FAKCvSzksW7JkidavX6/ly5fr1ltv1T333KOnn366034bNmzQddddp4svvliPPfaYLrnkEt1www3asGGDJGndunVasGCBrrvuOv3hD39QfX295s2blzz+t7/9rZ544gndc889uvvuu/X444/rt7/9bTfeas9qm7OMsAwAAOS2QCCgGTNmaOHChVq3bp2ee+45LVu2TJdddpmkWC+z1tZWSdKIESP08MMP629/+5vef/99ffe739WgQYN05plnOvkWclu4WYYVGzZr+UoOu/vGPY2a+9e3tb85LK/L0JLPHqehJYEMVxIAgN6TUljW3NysFStWaMGCBaqsrNS5556rq6++Wg899FCnfZ944gmdeuqpuuyyyzR8+HB9+ctf1imnnKKnnnpKkvTggw/qvPPO04wZM3TsscdqyZIlevHFF7V9+3ZJ0gMPPKDrr79eU6ZM0amnnqrvfe97BzyPU+zEMgqEZQAAAJo3b54qKys1e/Zs3XbbbZozZ46mTZsmSZo6dapWrlwpSTr++OO1cOFC3XnnnZo5c6Yk6b777uvUAw29xwzWSpJs0yN58g67/7PvViUfL7rgOA3vd/hjAAA4kqQ0Z9mGDRsUiUQ0ceLE5LbJkyfr3nvvlWVZHW5yLrroIoXDnZeebmiITe65du1aXXPNNcntgwYN0uDBg7V27Vp5vV7t2rVLJ510UofzfPjhh9q7d68GDBiQSrUzIjnBP8MwAQAAFAgEtHjxYi1evLjTaxs3buzw/OKLL9bFF1/cW1XDYRjth2AaH13Sso1t23ppy34t/1fsy+3bP3Oszhxd1htVBACgV6UUllVVVam0tLTD0uDl5eUKBoOqra1Vv379kttHjx7d4dhNmzbp1Vdf1SWXXCJJBwy9ysrKtHv3blVVxb6tav96eXm5JGn37t0phWWH+P99t7SfsyxT58DBJdqctncG7e8s2t95XANnZbL9uabIRcmeZYcYgtkajuqOZzfpqXf2SpK8LkOnj+p30P0BADiSpRSWtbS0dAjKJCWff3S58Pb279+vOXPmaNKkSTrnnHMkSa2trQcsKxQKJee0aP96V85zIF1dFjRVu+M9ywoKfCovz8w5cHiZur7oGtrfWbS/87gGzqL9gZ5htNZIkmx/SXJbcyiq1Tvq9OCqHVq9o062bcuyJZchnTG6TJ8+rkL53pQ+SgAAcMRI6f9wPp+vU1iVeO73+w94THV1ta644grZtq277747OVTzYGUFAoEOwZjP5+twnkAgtclD9+1ryMhyo4meZQ31zaqubuj5E+CQDCP2ISlT1xeHRvs7i/Z3HtfAWZls/0TZQC5pvxLmpqpGPbuxSn/5z27tb+44pUr/Aq9+9JljNWloSe9XEgCAXpRSWFZRUaGamhpFIhG53bFDq6qq5Pf7VVRU1Gn/PXv2JFdBeuCBBzoM06yoqFB1dXWH/aurq9W/f39VVFQkyx46dGjysST1798/lSrLtpWhDzKxsMyyLD4oOShz1xddQfs7i/Z3HtfAWbQ/0DOM+DDMrU1eXfbgakWs2F+sgMfUpKEl+uYZI1QS8Kg04JHbxUIMAIDsl1JYNm7cOLndbq1Zs0ZTpkyRJK1atUrjx4/vtIJRc3Ozrr76apmmqQceeKBTyDVhwgStWrUquQrSrl27tGvXLk2YMEEVFRUaPHiwVq1alQzLVq1apcGDB/eJyf2ltp5lsqPOVgQAAADoIsu2tXFvo55+Z68eWbtLwYilee7/6Ktu6eWdliKWrfGDCnX6qH760uShCnhcTlcZAIBel1JYFggENGPGDC1cuFB33HGH9u7dq2XLlmnRokWSYr2/CgsL5ff7dd9992nbtm363e9+l3xNig3XLCws1Be/+EVdeumlOvHEEzV+/HjdfvvtOvvsszVs2DBJ0he/+EXdddddGjhwoCTpJz/5ia688soee+PdZScm+Lf4ShsAACDrhFvlX/c/UrhJwWM+Jyu/oterYDbslO/dR2REU5uzt72IFQvHWsOxFdz3NLRqR22r+kv6miS5pFPNtyVJ9UaBrj9zpL4yZagMVrsAAOSwlGflnDdvnhYuXKjZs2eroKBAc+bM0bRp0yRJU6dO1aJFizRz5kw988wzam1t1axZszocf9FFF+nOO+/UxIkT9YMf/EB333236urqdPrpp+uHP/xhcr+rrrpK+/bt03XXXSeXy6XPfe5zuvzyy7v3bnuQpURPOnqWAQAAZJ23HlXBSzdLklz1O9R49qJer0L+a3fK/+4j3S7n5I9u8Bx4vys+cZKs44d1+3wAABzpUg7LAoGAFi9erMWLF3d6bePGjcnHTz/99GHLmjlzZnIY5ke5XC7NmzdP8+bNS7WKvcI2DMmmZxkAAEBWqt+RfGg2ftirpw5HLf38xa26aMM7OtWUXoqO1za7bSqSPK9Lxw8qkt9tKt/nVt5Hhko2hSL6x6ZqhSKx3mQ+t6nBxQEZkkxTGtEvT8X+jomZFegna+wFGX9vAAAcCVjvOU12vGeZbVsO1wQAAAA9rrkm+TCxWmRvuf//PtAfVu/UFd5GSdL7R18p18iz9IfVO7VuZ70UkbQltq/bNHR+ZYWmjipTaziq9/c36/8+rNHbrQ0aXZ6ny04aphNH9FNJXsdwrLFX3xEAAEcWwrI0JfuTEZYBAABkn5a2sCyxWmRvsGxbK9/eI0ka6muVwtIFU45VpP8AnXtMf+1pCOrBN3boX9tqFYpY+rCuVY/9Z7ce+8/uDuX43KZ+/NlKDSsN9FrdAQDIFoRlaUr2LLMIywAAALJOS/ueZTWH2LFnvbO7QXsbQ8rzuJRv1UuSLF+pJMkwDA0s8ut7nxgjSbJtWyvf3quXtuzTC5ur5TINnTdugLwuU2eOKSMoAwAgTYRlaUquhknPMgAAgOzTsj/50AjWxUYTGOYhDugZj8Z7iH18RJ6M7UFJku0vOeC+hmHoM5UV+kxlhbbua5LXZWpoCQEZAADdRViWJtsw42MxCcsAAACyTvthmLYlI9Qg21eckVPtqG3Rkr9vVlMoGpuTTNLnjg1I2yXbdMv25B+2jFFlh98HAAB0DWFZmqx4zzIxDBMAACD7tHQcemm01mQsLLv3n+/r1ffbzjdj/EBN6BeRJNm+UskwMnJeAABwYIRlaWpbDdM+zJ4AAAA4oth2MiyzDVOGbclsrZWVgaxsb0NQz71bLUm66ZwxGlTk18nDS2TuelWSZB1kCCYAAMgcwrLuYs4yAACArGKEGyUr1rMrWjxC7tqtKa+Iadu2IlbsS1W3acgwDFm2rajV9kXrluom/dfj7yhq2TphcJE+d+LgtjrEz3ew+coAAEDmEJalyTboWQYAAJCNjNZaSZLt8snKHyTVbj3sipj1rWG9vbtBtqRg2NLSVz/Qu1VNkqRjBhRozhkj9YNnNmpvY6jTsYOL/Vow7egO28x4HRIrYQIAgN5DWJYmOzlnWdTZigAAAKBHJcIyy1+a7NmV2Nbexj2N+semKkUsW39dv0e1LeEDlrdxb6Ou+/N/DvjaWaPL9P3pY1Xk93ykDvFhoPQsAwCg1xGWpclKzFkmepYBAABkE7NdUGX5SmLbWqqTr7+8ZZ9+89o2vbW7ocNxAwq8Ks3zSpJGluXpG1NHSJJue3qjVm2vk8dlaNklJ2hYoUuSZBiG8rwuSVEp0vELWLNlnyQlzw8AAHoPYVma7MSqRMxZBgAAkFWS84X5SmT7Y8Mg89/4ubZ7Rys65jO69amNagjG5jQ7dUSpRpXlqX+BT5+bMEh+j6tTefd87gQ9+dZujQiE9LGnz5GraXeX65I4PwAA6D2EZWlKrIYpi7AMAAAgmxiRVkmS7QkoNPR05b15jyTpzZce0/x/lEuS+uV5dOcFx+nEIUUyEl+iHoTbNHTh+EHybH8ptaDMHVBoyGlpvgsAAJAuwrI0JeYsY4J/AACALJO8vzMUHnaGNlbeqGPe+rGKjUZJsaDsl7NO0Ojy/JSKTUzaHx50surOf+Dw1XB5JZc3pXMAAIDuIyxLF8MwAQAAslRbWCZJz2w3dYyko/ytmnPqSJ133AD1L/ClXGpi0n4rUCbbW9BDdQUAAD2NsCxNyWGYhGUAAABZJh6WGYbe39esN6oleaUxBSENPHlY2qWa8bnQLFa4BACgTzOdrsCRKjEMk7AMAAAgy8SHYdoy9Nf1u1Vrx3qBecP13SrWiA/DZNJ+AAD6NnqWpck2YjmjTVgGAACQZdrmpP3HpmqZioVlZnwYZboSx1u+km6VAwAAMoueZWmiZxkAAECWivcsawxZ+rCuVS2uIkmSEWmR4itlpsOID8O0GYYJAECfRliWpkTPMsIyAACA7BL/SlQ1LWFJUuVRg5P3fmawLu1yE6thWgzDBACgTyMsS1vsNsq27cPsBwAAgCNL7P6uKRT7UnRsRaHs+NBJoxtDMRPH2gzDBACgTyMsSxOrYQIAAGSp+JehzeHYfd7QkkByBcvEipbpYDVMAACODIRlabKNRAd9epYBAABkl45h2VGlgXY9y2rTLNJmNUwAAI4QhGVpS0zwH3W2GgAAAOhZ8Z5lrZFYWDasJJCcZyzdFTGNUIOM+H2j5SvugUoCAIBMcTtdgSNVcoJ/i55lAAAA2cWO/9dQkd+t4oAnuYKlf8MKuarfTrlEI9wcK9Ptl9yBHqspAADoeYRlaWI1TAAAgCyVXMDJ0LCSWLAVLRgsSfLs+pc8u/6VdtGJcgAAQN9FWJY25iwDAADITnbyv4OK/JKklgnXyPbky4i0dKvk0Ihzu1s5AACQYYRlabKTc5bRswwAACC7tA3DLA7EbpftQD+1TL7OyUoBAIBewgT/6WIYJgAAQHayEz9ic5YBAIDcQliWpraeZQzDBAAAyC5twzALfYRlAADkGsKyNDHBPwAAQJayO66GCQAAcgthWboIywAAALKSofZhmcfh2gAAgN6WclgWDAY1f/58TZkyRVOnTtWyZcsOe8wbb7yhc845p8O2Y4455oB/HnvsMUnSs88+2+m166+/PtXqZkzb4EvCMgAAgKxitw3DpGcZAAC5J+X/+y9ZskTr16/X8uXLtXPnTt10000aPHiwpk+ffsD9N27cqBtuuEE+n6/D9ldeeaXD8//5n//RU089lQzVNm/erI9//OP64Q9/mNzno2U4KtmzjDnLAAAAskvi/s5gzjIAAHJQSv/3b25u1ooVK7R06VJVVlaqsrJSmzZt0kMPPXTAsOzhhx/W4sWLNWzYMDU2NnZ4rX///snH27dv1+9+9zvde++9KiwslCRt2bJFY8eO7bBfX2KLYZgAAADZyGbOMgAAclpK//ffsGGDIpGIJk6cmNw2efJk3XvvvbIsS6bZcVTnSy+9pMWLF6uxsVH33HPPQcu9++67ddppp+ljH/tYctuWLVs6PE+XYXS7iIMUHHuvhuzMnQMHlWhz2t4ZtL+zaH/ncQ2clcn255pCkiLR2Jehti3mLAMAIAelFJZVVVWptLRUXq83ua28vFzBYFC1tbXq169fh/1/+ctfSpIeeeSRg5a5c+dOPfHEE3r44YeT22zb1nvvvadXXnlF9913n6LRqKZPn67rr7++w7m7oqysMKX9u8rldkmS3C5D5eWZOQcOL1PXF11D+zuL9nce18BZtD8ypTUciT0wDAU8rIcFAECuSSksa2lp6RRWJZ6HQqG0KvCnP/1Jxx9/vCZMmJDctnPnzuS5fvazn2nHjh360Y9+pNbWVt18880plb9vX0NGphWLRGM/w+Gwqqsbev4EOCTDiH1IytT1xaHR/s6i/Z3HNXBWJts/UTZyWzB+o+dxmTLobggAQM5JKSzz+XydQrHEc7/fn1YFnnnmGV1yySUdtg0ZMkSvv/66iouLZRiGxo0bJ8uydOONN2revHlyuVxdLt+2MzMHv50YhmlbfFByUKauL7qG9ncW7e88roGzaH9kSvuwDAAA5J6U7gAqKipUU1OjSCSS3FZVVSW/36+ioqKUT75r1y5t3rw5uQJmeyUlJR2+yRs9erSCwaDq6upSPk8mJGvGXToAAEBWCYUJywAAyGUp3QGMGzdObrdba9asSW5btWqVxo8f32ly/65Yu3atBg0apMGDB3fY/vLLL+uUU05RS0tLcts777yjkpKSTvOiOSXRs6xtaXEAAABkg1C8Z5k7hdEMAAAge6SUcAUCAc2YMUMLFy7UunXr9Nxzz2nZsmW67LLLJMV6mbW2tna5vE2bNmn06NGdtk+cOFE+n08333yztm7dqhdffFFLlizR1VdfnUp1MysRltmWs/UAAABAj2r7KpT5ygAAyEUpdwebN2+eKisrNXv2bN12222aM2eOpk2bJkmaOnWqVq5c2eWyqqurVVxc3Gl7QUGBfvOb32j//v26+OKLtWDBAn3hC1/oU2FZ+znLAAAAkIXIygAAyEkpTfAvxXqXLV68WIsXL+702saNGw94zMyZMzVz5sxO22+77baDnufoo4/Wb3/721Sr14sSd08MwwQAAMgq8S9DbdIyAAByErOWpothmAAAANnNICwDACAXEZalLXbzxDBMAACALJNc7ZywDACAXERYlq5kzzKGYQIAAGQXwjIAAHIZYVm6EmGZ6FkGAACQTezEl6FkZQAA5CTCsjTZRmIYJj3LAAAAgsGg5s+frylTpmjq1KlatmzZQfd99tlndd5552nixIn64he/qLfeeqsXa9oF8fs7m1tlAAByEncAaaNnGQAAQMKSJUu0fv16LV++XLfeeqvuuecePf30053227Rpk7773e/qa1/7mv7yl79o3Lhx+trXvqaWlhYHan0wfBkKAEAuIyxLV3wYJhP8AwCAXNfc3KwVK1ZowYIFqqys1Lnnnqurr75aDz30UKd9//nPf2rMmDGaMWOGjjrqKH3nO99RVVWVNm/e7EDND4IJ/gEAyGlupytwpLIJywAAACRJGzZsUCQS0cSJE5PbJk+erHvvvVeWZck0276fLSkp0ebNm7Vq1SpNnDhRjzzyiAoKCnTUUUelfF4jQ1mWYSTmLDMydg4cXKLNaXtn0P7O4xo4i/Z3VibbP5UyCcvSlWhlwjIAAJDjqqqqVFpaKq/Xm9xWXl6uYDCo2tpa9evXL7n905/+tP7xj3/oS1/6klwul0zT1H333afi4uKUz1tWVtgj9f+onW6XJMl0uVRenplz4PAydX3RNbS/87gGzqL9neV0+xOWpck2YjdRBnNaAACAHNfS0tIhKJOUfB4KhTpsr6mpUVVVlb7//e9rwoQJ+v3vf6958+bp0UcfVVlZWUrn3bevQZlYaykcjkiSLMtWdXVDz58Ah2QYsQ9Jmbq+ODTa33lcA2fR/s7KZPsnyu4KwrJ0JYdhRh2uCAAAgLN8Pl+nUCzx3O/3d9h+1113aezYsfryl78sSfrhD3+o8847T3/+85/11a9+NaXz2rYy8kHGtmIjB2xlpnx0TaauL7qG9nce18BZtL+znG5/JvhPVzIs428PAADIbRUVFaqpqVEkEkluq6qqkt/vV1FRUYd933rrLR177LHJ56Zp6thjj9XOnTt7rb6Hl7i/41YZAIBcxB1AuhJhmehZBgAActu4cePkdru1Zs2a5LZVq1Zp/PjxHSb3l6QBAwZoy5YtHba99957Gjp0aG9UNTVM7gwAQE4iLEtXYs4yJvgHAAA5LhAIaMaMGVq4cKHWrVun5557TsuWLdNll10mKdbLrLW1VZL0+c9/Xn/84x/12GOP6YMPPtBdd92lnTt36qKLLnLyLXSUvL8jLQMAIBcxZ1ma7HjPMlbDBAAAkObNm6eFCxdq9uzZKigo0Jw5czRt2jRJ0tSpU7Vo0SLNnDlTn/70p9XU1KT77rtPu3fv1rhx47R8+fKUJ/fvDTZhGQAAOYmwLE2GGV9SXIRlAAAAgUBAixcv1uLFizu9tnHjxg7PZ82apVmzZvVW1VIXn5PWMAjLAADIRQzDTFdyGCZzlgEAAGQXO/5fwjIAAHIRYVm6zMQE/6yGCQAAkF3i93dkZQAA5CTCsjQZidUw6VkGAACQXezEl6HcKgMAkIu4A0gXq2ECAABkKUYOAACQywjL0hWf4N9ggn8AAICsYtgMwwQAIJcRlqWpbRgmYRkAAEB2SYRl3CoDAJCLuANIV3yCf5OeZQAAANmFUZgAAOQ0wrJ0MWcZAABAlkrc3zEOEwCAXERYliYj3rOMOcsAAACyFWEZAAC5iLAsXcmeZfTTBwAAyCrJCf4JywAAyEWEZWky4qthmoo6XBMAAAD0pLavQgnLAADIRYRlaUoMwzSZswwAACCrGPQsAwAgpxGWpclIDMNkuSQAAIAsk7i/IywDACAXEZalKzkMk55lAAAA2SUWltn0LAMAICcRlqXJMOKrYdrMWQYAAJBVWMAJAICclnJYFgwGNX/+fE2ZMkVTp07VsmXLDnvMG2+8oXPOOafT9ilTpuiYY47p8KepqSnt8/SqZM8ybqYAAACyC8MwAQDIZe5UD1iyZInWr1+v5cuXa+fOnbrppps0ePBgTZ8+/YD7b9y4UTfccIN8Pl+H7Xv27FFDQ4Oee+45+f3+5Pa8vLy0ztPbzMQE/wzDBAAAyC7J+f0JywAAyEUphWXNzc1asWKFli5dqsrKSlVWVmrTpk166KGHDhhiPfzww1q8eLGGDRumxsbGDq9t2bJF/fv317Bhw7p9HicYzFkGAACQlQyxGiYAALkspWGYGzZsUCQS0cSJE5PbJk+erLVr18qyOodGL730khYvXqzLL7+802ubN2/WyJEje+Q8jkishmn3kfoAAACghzAMEwCAXJZSz7KqqiqVlpbK6/Umt5WXlysYDKq2tlb9+vXrsP8vf/lLSdIjjzzSqawtW7aopaVFl156qd577z2NGzdO8+fP18iRI1M+z6Fk6gtB09U2DNPiPqrXJa4rX/g6g/Z3Fu3vPK6BszLZ/lxTSGo3wT+/EAAA5KKUwrKWlpYOAZak5PNQKJTSibdu3aq6ujp95zvfUUFBgZYuXarLL79cTz75ZI+ep6ysMKX9u2pvUYGkWFjWrzwz58DhZer6omtof2fR/s7jGjiL9kfmMAwTAIBcllJY5vP5OoVVieftJ+nvit/85jcKh8PKz8+XJN11110666yz9Pzzz/foefbta8jI6t9NTWFJsTktqqsbev4EOCTDiH1IytT1xaHR/s6i/Z3HNXBWJts/UTZyHT3LAADIZSmFZRUVFaqpqVEkEpHbHTu0qqpKfr9fRUVFKZ3Y6/V26D3m8/k0dOhQ7dmzR5MmTeqx89i2MvJBxjBjN08uWQrxQckxmbq+6Bra31m0v/O4Bs6i/ZExiV8ssjIAAHJSShP8jxs3Tm63W2vWrEluW7VqlcaPHy/T7HpRtm3rk5/8ZIe5zJqbm/XBBx9o1KhRPXaeTDLMWIhnsBomAABAVmnLyPrGfScAAOhdKd0BBAIBzZgxQwsXLtS6dev03HPPadmyZbrsssskxXp/tba2HrYcwzB09tln6xe/+IVef/11bdq0SXPnztXAgQN11llnHfY8fYFpxlbDNAnLAAAAsgzDMAEAyGUpDcOUpHnz5mnhwoWaPXu2CgoKNGfOHE2bNk2SNHXqVC1atEgzZ848bDk33nij3G63vvvd76qxsVGnnnqq7r//frlcrsOep08wYjmji7AMAAAguzAMEwCAnGbYdnbP9lFdnZnJl3fu2KoJfzlTYblU+80Pev4EOCTDkMrLCzN2fXFotL+zaH/ncQ2clcn2T5SNI0Om/g7ueOBSTWx4Xs8MuUGTZtzY8yfAIfFvrLNof+dxDZxF+zurr9znMRFDmgxXrFOey6ZnGQAAQHaJ3Z0bBl3LAADIRYRlaUosNGAaLMUFAACQTYzkMEzCMgAAchFhWZqM+AT/kiR6lwEAAGQRJvgHACCXEZalqWNYFnWuIgAAAMgMepYBAJCTCMvSZBrtmo6wDAAAIGsYTLEBAEBOIyxLU2KCf0nMWQYAAJBVmOAfAIBcRliWJtNoNwzTijhXEQAAAPQw5iwDACCXEZalyXC1NZ1lMQwTAAAga7AaJgAAOY2wLE1muwn+LYvVMAEAALKFQc8yAAByGmFZmtqHZTY9ywAAALIPPcsAAMhJhGVpcpmmLDt2A8UwTAAAgCxi07MMAIBcRliWJsOQovHmo2cZAABANmE1TAAAchlhWZpcpiEr/m2jFSUsAwAAyBbJOcsIywAAyEmEZWkyDENWvPmY4B8AACAbEZYBAJCLCMvS5Go3DFNWxNnKAAAAoOfY9CwDACCXEZalKdazjAn+AQAAshdhGQAAuYiwrBsSwzBtm2GYAAAA2YPVMAEAyGWEZd2QnLMsyjBMAACAbGGwGiYAADmNsKwbEmGZ6FkGAACQPZizDACAnEZY1g1tPcuYswwAACBbGAzDBAAgpxGWdQNzlgEAAGSjRM8yZ2sBAACcQVjWDVa8a75tMWcZAABA9uFWGQCAXMQdQDcke5ZZ9CwDAADIFgZzlgEAkNMIy7qhLSxjzjIAAIDskVgN0+FqAAAARxCWdYNNzzIAAICs0zbBP7fKAADkIu4AuoGeZQAAAFmMrmUAAOQkwrJusIzEapiEZQAAANnDPvwuAAAgaxGWdQPDMAEAALJPYoJ/g55lAADkJMKybkgOw6RnGQAAyHHBYFDz58/XlClTNHXqVC1btuyA+1166aU65phjOv2ZN29eL9f4UBKrYXKrDABALnI7XYEjmWWYsXupKGEZAADIbUuWLNH69eu1fPly7dy5UzfddJMGDx6s6dOnd9jvF7/4hcLhcPL52rVr9a1vfUtf+tKXervKB5WY4J9+ZQAA5CbCsm6w47dQthiGCQAAcldzc7NWrFihpUuXqrKyUpWVldq0aZMeeuihTmFZSUlJ8nE0GtVPf/pTXX311Ro/fnwv17oLSMsAAMhJKfct72oX+/beeOMNnXPOOR222bat+++/X5/4xCc0adIkzZ49W5s3b06+/vbbb3fqnj9z5sxUq5tRluGSJNn0LAMAADlsw4YNikQimjhxYnLb5MmTtXbtWlmHmNv1kUceUV1dna655preqGaXJeYsYxgmAAC5KeWeZV3tYp+wceNG3XDDDfL5fB22P/zww1q2bJkWLVqkESNG6Ne//rWuueYarVy5UoFAQJs3b9a4ceO0dOnStsq6+1pHuHjPMuYsAwAAOayqqkqlpaXyer3JbeXl5QoGg6qtrVW/fv06HWPbtn7961/rsssuU35+flrnzdz8+4lhmEYGz4GDSbQ5be8M2t95XANn0f7OymT7p1JmSulTKl3spVggtnjxYg0bNkyNjY0dXnv00Ud15ZVX6uMf/7gkaeHChTr55JP15ptv6vTTT9eWLVs0evRo9e/fP5Uq9qpEzzLZDMMEAAC5q6WlpUNQJin5PBQKHfCY119/Xbt379bnP//5tM9bVlaY9rGHUhu/mfbn+VRenplz4PAydX3RNbS/87gGzqL9neV0+6cUlh2si/29994ry7Jkmh27qr/00ktavHixGhsbdc8993R4be7cuRo6dGjyuWEYsm1bDQ0NkqQtW7bomGOOSfkN9abkapgWPcsAAEDu8vl8nUKxxHO/33/AY5555hmdeeaZHeYwS9W+fQ1KjJjsSXa80NbWsKqrG3r+BDgkw4h9SMrU9cWh0f7O4xo4i/Z3VibbP1F2V6QUlqXaxf6Xv/ylpNh8FB81ZcqUDs9XrFihSCSiyZMnS4qFZZZl6YILLlBDQ4POPPNMzZ07VwUFBalUOWNdJw1DsuOFG1aULpq9jK6xzqL9nUX7O49r4Ky+0j0fbSoqKlRTU6NIJJKcNqOqqkp+v19FRUUHPObll1/Wdddd163z2rYy8kHGaDcMkw9KzsnU9UXX0P7O4xo4i/Z3ltPtn1JYlk4X+65Yu3atFi9erKuuukr9+/dXOBzW9u3bNXToUN1xxx2qr6/XokWLdOONN+pXv/pVSmVnsuveNsWGYfr9brroO8Tprpm5jvZ3Fu3vPK6Bs2j/vmPcuHFyu91as2ZN8gvRVatWafz48Z1GHkjS/v37tX379uSXpH2PHf8v6SkAALkopbAsnS72h7N69Wpdc801OvPMM3XDDTdIkjwej1577TX5fD55PB5J0p133qmLL75Ye/bsUUVFRZfLz1TXyVjPstjNX3NzC130exldY51F+zuL9nce18BZfaV7PtoEAgHNmDFDCxcu1B133KG9e/cmF3KSYr3MCgsLk/eLmzZtks/n6zAlR1+SWA2TnoYAAOSmlMKydLrYH8rrr7+ur3/96zr99NP1k5/8pMM3jx8dbjl69GhJSjksy2TXvURYJsviw5JDnO6ametof2fR/s7jGjiL9u9b5s2bp4ULF2r27NkqKCjQnDlzNG3aNEnS1KlTtWjRIs2cOVOStG/fPhUVFcno82lU515xAAAg+6V0B9C+i33CobrYH8q7776ra6+9VmeccYZ+9rOfJXuQSdLmzZs1ceJEbd++PbntnXfekdvt1vDhw1M6TyYlJ/hnNUwAAJDjAoGAFi9erNWrV+vll1/W5Zdfnnxt48aNyaBMkj796U/rlVdecaCWXZOcs6zPh3kAACATUkq42nexX7dunZ577jktW7ZMl112maRYL7PW1tYulfX9739fgwYN0rx581RTU6Oqqqrk8aNGjdLw4cN1yy236N1339Ubb7yhW265RbNmzVJxcXHq7zJDbCM2Z5nBapgAAABZJN5lkbAMAICclHLf8nnz5qmyslKzZ8/Wbbfd1qmL/cqVKw9bRlVVlVavXq3Nmzfr7LPP1tSpU5N/Vq5cKdM09atf/UoFBQX68pe/rG9+85s67bTTNH/+/NTfYSbFb6Bsm7AMAAAgWxiEZQAA5LSU5iyT2rrYL168uNNrGzduPOAxM2fO7ND1vn///gfdN2HQoEG65557Uq1er7Liq2GKYZgAAABZw05mZYRlAADkImYt7YbEBP82wzABAACyRrJnmQjLAADIRYRl3ZFYDZOeZQAAAFmIsAwAgFxEWNYNNmEZAABA1mE1TAAAchthWTckVsMUwzABAACyBhP8AwCQ2wjLuoOeZQAAAFmInmUAAOQywrJuaBuGSc8yAACAbJGMyAjLAADISYRl3cAwTAAAgCxksxomAAC5jLCsWxI3UPYh9wIAAMCRgwn+AQDIbYRl3WCbsZ5lBj3LAAAAsg5hGQAAuYmwrFuY4B8AACDbJFfDZBgmAAA5ibCsOxJzlhGWAQAAZI1kWEbPMgAAchJhWTfYZqz5DFbDBAAAyCKEZQAA5DLCsu4wGIYJAACQbYzkT8IyAAByEWFZdzAMEwAAIOuwGiYAALmNsKwb7HhYZtphh2sCAACAHmMzDBMAgFxGWNYNLZ4SSVIgUudsRQAAANBjkj3LHK4HAABwBmFZNzR7+0mSCsL7Ha4JAAAAepzBrTIAALmIO4BuaPaWS5LyI/scrgkAAAB6CnOWAQCQ2wjLuqHFWyZJKozQswwAACBbJMIy5iwDACA3EZZ1Q7MvFpb5rBYp1ORwbQAAANCTDGYtAwAgJxGWdYPlylez7ZMkmS1VDtcGAAAAPYGeZQAA5DbCsm7w+9yqsoslSWbTXodrAwAAgJ7AnGUAAOQ2wrJuKA54VKUSSZLZTFgGAACQDRIRGWEZAAC5ibCsG0oC3raeZc0MwwQAAMgOdvy/3CoDAJCLuAPohuKAR1V2iSTJVb/N2coAAACgRySGYZp0LAMAICcRlnVDSZ5Hr1njJEn+d/4go7XW2QoBAACg25jgHwCA3EZY1g3FAY+esk7WRvsomaF6FT11lUx6mAEAAGQFw+BWGQCAXMQdQDcU53lky9S80JWyPAXy7nxd/R6cqpI/flr5//yhvFuflqt2q2RFnK4qAAAAuijZs0z0LAMAIBe5na7AkazA65ZpSG/aY/Xep/+sYW/eIe/2F+WpWidP1TppzX2SJNsdUHjQyQqOOEetx31JcvsdrjkAAAAOi2GYAADkJMKybjBNQ4V+t+paIqryj1DRZx+S2bhLnp2vyfPhq/LsWSNX3VYZkRZ5t78o7/YXlf/vnyk4arqaTv++bG+B028BAAAAH5HoWUZWBgBAbiIs66YiXywsa2iNDbW0CgYpOPYiBcdeFNvBtuTa/668219SYM39cjXtVuDt/5Xnw/9T02nzFRp1HndiAAAAfUjizow5ywAAyE0p3wEEg0HNnz9fU6ZM0dSpU7Vs2bLDHvPGG2/onHPO6bT9iSee0Cc/+UlNmDBB3/zmN7V///7ka7Zt66677tKpp56qk08+WUuWLJFlWalWN+MK/R5JUl3rQeYlM0xFy45Vy4lf1f5L/6m6zyxXNL9C7rr3Vfz0V1Xy6MVy1WzpxRoDAADgkGxWwwQAIJelHJYtWbJE69ev1/Lly3Xrrbfqnnvu0dNPP33Q/Tdu3KgbbrhBduKmI27dunVasGCBrrvuOv3hD39QfX295s2bl3z9t7/9rZ544gndc889uvvuu/X444/rt7/9barVzbhif6xzXkMwfPidXT6FRpyjmi+9oKYpN8h2B+TZ9S+V/mGa/OuWtd2YAQAAwDHJYZhM8A8AQE5KKSxrbm7WihUrtGDBAlVWVurcc8/V1VdfrYceeuiA+z/88MO65JJLVFZW1um1Bx98UOedd55mzJihY489VkuWLNGLL76o7du3S5IeeOABXX/99ZoyZYpOPfVUfe973zvoeZxUGA/L6g/Ws+wAbG+hmk+5Ufu/9IJCR50lIxpU4cvfV/4rCyW77/WeAwAAyCXJYZgmwzABAMhFKd0BbNiwQZFIRBMnTkxumzx5stauXXvAIZIvvfSSFi9erMsvv7zTa2vXrtWUKVOSzwcNGqTBgwdr7dq12rNnj3bt2qWTTjqpw3k+/PBD7d27N5UqZ1yiZ9lBh2EeglU4RHXnP6jGj90sScpb9xsV/v3bBGYAAAAOSvQsAwAAuSmlCf6rqqpUWloqr9eb3FZeXq5gMKja2lr169evw/6//OUvJUmPPPJIp7L27t2rAQMGdNhWVlam3bt3q6qqSpI6vF5eXi5J2r17d6fjDiVTU00kyq0o8kmStu1vTu9chqHWSV+XVTBQhc99S/6Nf5aV11/Np9/cc5XNQom2ZioRZ9D+zqL9ncc1cFYm259ripjEapj8QgAAkItSCstaWlo6BGWSks9DoVBKJ25tbT1gWaFQSK2trR3K7s55ysoKU9o/VacfM1D3vPS+3trTqPLybpyr/FIp3yM9+jXlrb5XeYPGSCdf03MVzVKZvr44NNrfWbS/87gGzqL9kSltq2ESlgEAkItSCst8Pl+nsCrx3O/3p3Tig5UVCAQ6BGM+n6/DeQKBQErn2bevISPz5htG7CZ9WJ5LLkPaVdeq9VurNLAotXboYMhnFDj1v5T/2p2yn56n2oJKRQeM77lKZ5FE+2fq+uLQaH9n0f7O4xo4K5PtnygbuS05DNNgzjIAAHJRSmFZRUWFampqFIlE5HbHDq2qqpLf71dRUVFKJ66oqFB1dXWHbdXV1erfv78qKiqSZQ8dOjT5WJL69++f0nlsO7OLTPo9Lo3pX6CNexu1bmeDKgq7EZZJap70Tbn3rJbvvWdU+LdvqubzT0uevB6qbfbJ9PXFodH+zqL9ncc1cBbtj0xhNUwAAHJbSl+XjRs3Tm63W2vWrEluW7VqlcaPHy8zxdWCJkyYoFWrViWf79q1S7t27dKECRNUUVGhwYMHd3h91apVGjx4cErzlfWWE4fEgsK/beiBxQcMQw2fuEvR/IFy125Vwcvf736ZAAAA6LK2YZiOVgMAADgkpYQrEAhoxowZWrhwodatW6fnnntOy5Yt02WXXSYp1vsrMd/Y4Xzxi1/UX/7yF61YsUIbNmzQ3LlzdfbZZ2vYsGHJ1++66y69/vrrev311/WTn/wkeZ6+ZuaEQTIkvbB5nzbubex2eba/VA3n3i1bhgLvPCzv5ie6X0kAAAB0SdtqmKRlAADkopQnYpg3b54qKys1e/Zs3XbbbZozZ46mTZsmSZo6dapWrlzZpXImTpyoH/zgB/rv//5vffGLX1RxcbEWLVqUfP2qq67Spz/9aV133XW64YYbdOGFF+ryyy9Ptbq9YlRZvj55TGx46H89/raqGoPdLjM85GNqnnydJKnw+bky63d0u0wAAAB0RXwYpklYBgBALjJsO7tn+6iuztwE/+Xlhcny9zWFdOX/rtbO+qD65Xl07ekjdH5lhdyubkwMGw2r5NGZ8uxZrfCgk1Q7Y4VkpjTNXNb6aPujd9H+zqL9ncc1cFYm2z9RNo4Mmfo7WHLPcHmMqDbNekUlA0b0/AlwSPwb6yza33lcA2fR/s7qK/d5LPHTQ8ryvfrvWSdoTHm+9jeHdfuzm/S5376hpa9+oPf3N6dXqMuj+nPvkeUpkGfXv5X3r5/0bKUBAADQCRP8AwCQ2wjLetDQkoD+58sT9e2zR6kk4NGHda26//8+0KzfvqEvPbBKS1/9QHsaUhuiaRUPV+PHF0uS8lf9gvnLAAAAMiw5Z1mKC1gBAIDswJi+HuZzm/rS5KGaMX6Qnt9UrWc3Vum1D2q0qapJm6qa9JtXP9AZo8t08YRBOnl4qcwuLLMUPPpCNe9dp7w196no799WTckoRcuP64V3AwAAkHuMAzwCAAC5g7AsQ/K8Ln2mskKfqaxQbUtYr2zdp8fX79GbO+r0wuZ9emHzPg0p9mvWiYM1c8IgBTyuQ5bXdNo8ufe9I+/2l1S88krVzHpSdqCsl94NAABAbrBtW6bBMEwAAHIZfct7QUnAo/MrB+q+L0zQw7Mn6/MnDla+16UP61r1sxe36sKl/9L/rtqhiHWI2etMt+qn/beiRcPlatih4r9+SUZrba+9BwAAgFzQ/m7MZDVMAAByEmFZLxtdnq8bzxmjp75+quafe7SGFPtV0xLWT1/YqssefFPrd9Uf9FjbX6q685fLCpTLU/2Wih//ioxQQy/WHgAAILvZ7b+87MJ0GQAAIPsQljkk4HHpohMG6U9XnqT55x6tYr9bm6qadPXv1+h/Xt8m6yBrpEZLx6j2wt/L8pXIs3eNip+YTQ8zAACAHmLbVrtnhGUAAOQiwjKHuU0jFppdcZKmHdNfUVv671fe17ceWa/61vABj4mWjVPdhb+X5S2SZ9e/VPqHaXJVv93LNQcAAMg+drsvLA2GYQIAkJMIy/qIkjyPfvSZY3XLtLHyuU29+n6NZj+0Wpurmg64f6T/eNXN+KMixSPkatypkkc/J8/O13u51gAAANmlQ1jGrTIAADmJO4A+xDAMfXb8QC374okaXOTTjtpWXfG/q/XsxqoD7h/pf7xqZz2p0KBTZIbqVfzXL8n/9u+lgwzhBAAAyJRgMKj58+drypQpmjp1qpYtW3bQfTdu3KgvfvGLOuGEE3TBBRfotdde68WaHlqHsIw5ywAAyEmEZX3Q2AEFWv6VSTpleIlaI5bmP/GOfvHSVkUPsFqm7StW3WcfVHDkp2REgyp8/kYV/v1bMlr2O1BzAACQq5YsWaL169dr+fLluvXWW3XPPffo6aef7rRfQ0ODrrzySo0ZM0aPP/64zj33XF133XXat2+fA7XurP2cZQzDBAAgNxGW9VElAY9+PnO8LjtpmCTpgX/v0A2P/Ee1LQeYx8wdUP15S9V46n/JNkz5N/5Z/R46Q4G1v5bCLb1ccwAAkGuam5u1YsUKLViwQJWVlTr33HN19dVX66GHHuq076OPPqq8vDwtXLhQw4cP1/XXX6/hw4dr/fr1DtT8QNp/OUlYBgBALiIs68NcpqE5Z47UHeePk99t6vUPajX7wTe1cW9j550NUy2Tr1PtRX9WpOw4mcE6FbyyUGUPnKK8138sV+3W3n8DAAAgJ2zYsEGRSEQTJ05Mbps8ebLWrl0ry7I67Puvf/1L55xzjlwuV3Lbn//8Z5111lm9Vt9DsSyGYQIAkOvcTlcAh3fuMf01sl+ebvzrW9pR26qrfr9GXz99hC6ZOFhuV8e8MzLoJNV8/in53/m98t78pVz125T/xs+V/8bPFSk7TqGjzlJo2FkKDz5JcvkcekcAACCbVFVVqbS0VF6vN7mtvLxcwWBQtbW16tevX3L79u3bdcIJJ+iWW27RP/7xDw0ZMkQ33XSTJk+enPJ5M5FlGUbH1TDJy3pfos1pe2fQ/s7jGjiL9ndWJts/lTIJy44QY/rna/mXJ+qWlRv0f+/V6OcvbtVf1+/W1z82XGcfXS6z/VU3XWqt/Ipax10i79an5d/wR3m3vSj3vrfl3ve28lb/Srbbr9Dg0xQa/gmFRpwrq2ioc28OAAAc0VpaWjoEZZKSz0OhUIftzc3Nuv/++3XZZZdp6dKlevLJJ3XVVVfpqaee0qBBg1I6b1lZYfcqfgANDW1hWVm/QvkLev4c6JpMXF90He3vPK6Bs2h/Zznd/oRlR5Aiv0c/veh4Pb5+t+55+X29t69ZNz3+jkaW5enC4wfqU8f2V3lBu95ipluhMecrNOZ8GS375d3+krzbX5Rn20tyNe+Rb9vz8m17Xnr5FkXKjlNw5LkKjZymSP/xksEIXQAA0DU+n69TKJZ47vf7O2x3uVwaN26crr/+eknScccdp3/+85/6y1/+oq9//espnXffvoYeXwS8qalRidvz2tpmuVtdh9wfPc8wYh+SMnF9cXi0v/O4Bs6i/Z2VyfZPlN0VhGVHGNMwdOH4Qfr40eV6aNWH+sObH+q9fc362Ytb9bMXt2pkvzxNHFqsyoGFGlTs06AivyoKffIE+ik4doaCY2dIti3X/g3yfvCCvO8/J8/ufyd7neW/8XNF8ysUGvkpBUedp/DgUyWXx+m3DQAA+rCKigrV1NQoEonI7Y7dXlZVVcnv96uoqKjDvv3799eoUaM6bBsxYoR27dqV8nltWz1+I21FO07wzwcl52Ti+qLraH/ncQ2cRfs7y+n2Jyw7QhX5Pbr29BH6yuShembDXq18e6/+s6te7+1v1nv7m/XIurYbTkNSeYFX/Qt8Kg14VJrnUWnApxH9LlLlmZdpZF5Qge3/kO+9Z+XZ9oJcTXsUWP+AAusfkOUrVmjkp9RS+WVFBqY+lwgAAMh+48aNk9vt1po1azRlyhRJ0qpVqzR+/HiZZsfe6ieeeKL+/e9/d9i2detWnX/++b1W30Ox7PYLEjBhDQAAuYiw7AhX6HfrcycO1udOHKzalrDWflinN3fUaUt1k3bXB7W7IahgxFJVY0hVjaEDlpHncWnsgDE6ZsCJOvbUBTrZXq+R+1+Q/71nZLbsk3/DH+Xf8EeFKyaq6bT5Cg85rZffJQAA6MsCgYBmzJihhQsX6o477tDevXu1bNkyLVq0SFKsl1lhYaH8fr8uueQSPfjgg/rFL36hz372s3rssce0fft2XXjhhQ6/ixhb7b7GNgnLAADIRYRlWaQk4NFZY8p11pjy5DbbtlXTEtau+qD2NYVU0xxSTXNY1U0hbapq0jt7GtQcjmrNh/Va82F9/KgC9cu7SKePuFwXlX6gk+qfUcGWv8qzZ7VKHpul1jEXqPGMH8rOKz9wRQAAQM6ZN2+eFi5cqNmzZ6ugoEBz5szRtGnTJElTp07VokWLNHPmTA0ZMkS//vWvdfvtt+v+++/X6NGjdf/996uiosLhdxBjWO1Ww6RnGQAAOcmw7ewehVtdnZlJ+QxDKi8vzFj5vSVq2Xpvf7Pe3duojXsbtWFP7E9zOJrcx+MydMFIl25w/VlHbVshw7ZkBcrUcPYShUZ9ypF6Z0v7H6lof2fR/s7jGjgrk+2fKBtHhkz8DtTVVmvMQydKkqq+vlly+Q99AHoc/8Y6i/Z3HtfAWbS/s/rKfR49y3KcyzQ0pjxfY8rz9enjYt/ohqOWVu+o0z/f269Xtu7XtpoWPbI5okd0oT5Tfpru0H+ruHGTip+6Si3jLlHjmT+S3NxIAgCAI1/7G3PDMMXnJAAAco95+F2QazwuUycPL9W3zx6tP195kh78yiR9bsIg+d2mnqweoJOqb9Zf8z8nW4YC7zysksdmyWja63S1AQAAui3LB10AAIAuICzDYR1TUaCbPnm0/nrNybrspGGyXV5dv2+mZkfmq8VVKM+e1Sr90/ly1W51uqoAAADdYrMaJgAAOY+wDF1WmufVnDNH6veXTdbJR5XopUilzmteqA/NIXI17lTJIxfLtW+D09UEAABIX8dxmM7VAwAAOIawDCkb3i9P93xuvG477xhVeYbqs803a6OGy2ypUslfviCz9j2nqwgAAJCWjsMwCcsAAMhFhGVIi2EY+vRxFXrw0kkaUDFEn29doLfsETJb9qn4iUtltOxzuooAAAAps0XPMgAAch1hGbplaElA931hgsaPOkqXB+dqu91f7rr3VfzEZTJCDU5XDwAAICW2Rc8yAAByHWEZui3gcenHF1Zq0rFHa3boJtXYBfLsXaviJy6TQk1OVw8AACAF9CwDACDXEZahR7hNQwunH6Nho8brK6H5qrPz5dn1bxU/81XJijpdPQAAgC7pOGcZAADIRYRl6DFul6k7zh+ngqNO1OzQTWq2ffJue1F5//6p01UDAADoEtu2JEmWTa8yAAByFWEZepTXberHF1bKHjRJ88NXSZLy3vi5PB8873DNAAAADi/Rs4z+ZQAA5K6Uw7JgMKj58+drypQpmjp1qpYtW3bQfd9++23NmjVLEyZM0MUXX6z169cnXzvmmGMO+Oexxx6TJD377LOdXrv++utTf4fodQGPSz+bebzeLvuUHoycI0O2Cp6dI7PhQ6erBgAAcEiGEmEZPcsAAMhV7lQPWLJkidavX6/ly5dr586duummmzR48GBNnz69w37Nzc366le/qgsuuEB33nmnfv/73+trX/uann32WeXl5emVV17psP///M//6KmnntI555wjSdq8ebM+/vGP64c//GFyH5/Pl857hAMKfG79/OLxuvZ/v6rxre9pQnCrClZeo/qLH5HcfqerBwAAcECWTVgGAECuS6lnWXNzs1asWKEFCxaosrJS5557rq6++mo99NBDnfZduXKlfD6f5s6dq9GjR2vBggXKz8/X008/LUnq379/8k9ra6t+97vf6Uc/+pEKCwslSVu2bNHYsWM77FdUVNQDbxm9pTzfq5/OmqT5ru9pv10gX/U65b0wX2LiXAAA0EfZFvcpAADkupR6lm3YsEGRSEQTJ05Mbps8ebLuvfdeWZYl02zL3tauXavJkyfLiC+5bRiGJk2apDVr1mjmzJkdyr377rt12mmn6WMf+1hy25YtWzo8T1emVvxOlMuK4oc2rDSgeZ/7hG78ww26X3cof+MfFS0/TqGJV3erXNrfWbS/s2h/53ENnJXJ9ueaIjFbmc0vAwAAOSulsKyqqkqlpaXyer3JbeXl5QoGg6qtrVW/fv067DtmzJgOx5eVlWnTpk0dtu3cuVNPPPGEHn744eQ227b13nvv6ZVXXtF9992naDSq6dOn6/rrr+9w7q4oKytMaf9UZbr8bDC1vFDm7Ct11/+8r5tcD6nonwtll5XJmHRZt8um/Z1F+zuL9nce18BZtD8ygmGYAADkvJTCspaWlk5hVeJ5KBTq0r4f3e9Pf/qTjj/+eE2YMCG5befOncnjf/azn2nHjh360Y9+pNbWVt18882pVFn79jVkZNSfYcRu0jNVfrYZW+zTjunf02+e2q+r3E9Jf71eDc1RBY/9XFrl0f7Oov2dRfs7j2vgrEy2f6Js5C5WwwQAACmFZT6fr1PYlXju9/u7tO9H93vmmWd0ySWXdNg2ZMgQvf766youLpZhGBo3bpwsy9KNN96oefPmyeVydbnOtp3ZKbIyXX42+fjY/nq05Tb97oWwLnU/p8LnviWFm9V6fPo9zGh/Z9H+zqL9ncc1cBbtj8ywJNGzDACAXJbSBP8VFRWqqalRJBJJbquqqpLf7+80+X5FRYWqq6s7bKuurtaAAQOSz3ft2qXNmzcnV8Bsr6SkJDnfmSSNHj1awWBQdXV1qVQZfcxFEwZr1ykL9dvIpyRJhS/OV+DNXzlcKwAAgJi2+f0JywAAyFUphWXjxo2T2+3WmjVrkttWrVql8ePHd5jcX5ImTJig1atXt3Vlt229+eabHYZbrl27VoMGDdLgwYM7HPvyyy/rlFNOUUtLS3LbO++8o5KSkg7zouHIdPkpw/X+xAX6RWSGJKng1duV98YvnK0UAACAxJxlAAAgtbAsEAhoxowZWrhwodatW6fnnntOy5Yt02WXxYbRVVVVqbW1VZI0ffp01dfX6/bbb9fmzZt1++23q6WlReedd16yvE2bNmn06NGdzjNx4kT5fD7dfPPN2rp1q1588UUtWbJEV1/dvRUU0TcYhqFvnjFS24//lpaEvyBJyn99sfJfXSTZlsO1AwAAOc1ODMMEAAC5KqWwTJLmzZunyspKzZ49W7fddpvmzJmjadOmSZKmTp2qlStXSpIKCgp03333adWqVZo5c6bWrl2r+++/X3l5ecmyqqurVVxc3OkcBQUF+s1vfqP9+/fr4osv1oIFC/SFL3yBsCyLGIah73x8tNaPuFJLwp+XJOW9+d8qeuZaKdLqcO0AAECusulZBgBAzjNsO7unxq2uztxqmOXlhRkrP1c0h6K65uE1On7fU1rs/bU8iig48lOqn36fZB58/Qna31m0v7Nof+dxDZyVyfZPlI0jQyZ+Bz7Ysl5Tnp6uRuWp9bp3+TvuAP6NdRbt7zyugbNof2f1lfu8lHuWAT0pz+vS/7voeL2c90nNDs1VWB753ntGeauYwwwAADiAYZgAAOQ8wjI4rqLQp8UXHKfX7eP1vdA1kqS8f/9U7l3/drhmAAAg19jJnwzDBAAgVxGWoU8YP7hI104dqb9YU/WYdYYM21LRs3NkBOudrhoAAMghtpWIywjLAADIVYRl6DMuPWmoThleoptDs7XLHChXww4VvDhPDBQHAAC9x273XwAAkIsIy9BnmIahm6eNle0t1LUt35All/yb/iLfxj87XTUAAJAr4nOWAQCA3EVYhj5lYJFf1585UmvsMfpZ9HOSpIKXFsisfc/hmgEAgFxgJ3uWMQwTAIBcRViGPmfGCYM0ZVix7glfoPWe8TLDTSp69jopGnK6agAAIMu1zf5AWAYAQK4iLEOfYxqGFkwbK6/brWsavqpWd5E8e9cq//UfO101AACQ5WwrNgyTnmUAAOQuwjL0SUNLAvrmGSO1S2WaG7xakpS3+lfybHvR4ZoBAIBsZjDBPwAAOY+wDH3W5ycO1sQhRfpreIqe9J4nSSp67lsymqscrhkAAMhWdmIcpkHPMgAAchVhGfos0zD0g08fq0KfW9+p/4L2+EbKbKlS0d+/xUpVAAAgQ5jgHwCAXEdYhj5tYJFfN39qrILy6iv11ypq+uTd9qL8a5c5XTUAAJCFLJthmAAA5DrCMvR5nzi6XBdPGKRN9lAtti+VJOX/3x3S7vUO1wwAAGQbI7kcJj3LAADIVYRlOCJ866xRGlWWp/tbPq5VvlNkWCHpT1fICDU4XTUAAJBFbJthmAAA5DrCMhwR/B6Xbj9/nHxul66pu0KNnv5S9bsqfOY6yYo6XT0AAJA1CMsAAMh1hGU4Yowpz9e3zx6l/SrSV5pvkOXyyfvB35X/6h1OVw0AAGSJ5GqYAAAgZxGW4Ygy84RBOntMmdZER+k285uSpLw19ymw9tcO1wwAAGQFhmECAJDzCMtwRDEMQzdPG6shxX4tb5iiZb7YhP8FryyU791HHa4dAAA40tm2FX9EWAYAQK4iLMMRpzjg0T2fG6+yfK9+UDddTwY+K0kq/Pu35fngeYdrBwAAjmzxnmUGYRkAALmKsAxHpGGlAf32ipOU53HruprP6yXfWTKsiIqfulre9//udPUAAMARKjFlGTOXAQCQuwjLcMQ6YWiJ7pk1Xnlej66qu0r/9JwmIxpU0VNXy7v1KaerBwAAjkiJmIyeZQAA5CrCMhzRThhcpP/+3Hjl+f2a3XCtnjFOl2GFVfT01+Xb9BenqwcAAI40TPAPAEDOIyzDEa9yUJF++6WJGtqvUNe2XKtHrTNl2FEVPjtHvg0rnK4eAAA4gjDBPwAAICxDVhhWGtBvvzRRJ48o03dCX9X/Rj4hw7ZU9PdvK+9f/0+Khp2uIgAAOAIwZxkAACAsQ9Yo8Ln104uO1+yTh2t+5Coti0yXJOX/+/+p3/+eLd/GR6Tkt8UAAAAHkEjLWA0TAICcRViGrOI2DX3zjJH62UXj9XP3Fboh9A1V28Vy1X+goueuV/Fjs+Sq2eJ0NQEAQJ8V+2KNOcsAAMhdhGXISqeP6qcHL52sXUPP15nBn+rH4c+rRT55d76u0j9MU94bv2BoJgAA6MxmNUwAAHIdYRmy1sAiv+753Hh9Z9p4Pej5nM4NLtFL0fEyokHlv75YpSs+LfeeNU5XEwAA9CH2R34CAIDcQ1iGrGYYhi4cP0iPXnWyPjFloq625ulboW9ov10g9753VPLnC+X94B9OVxMAAPQV9CwDACDnEZYhJxT63Zpz5iituOJkWcddrE+F7tLfopNl2FF5n/y6Pnh3ldNVBAAAfYAd71NmM8E/AAA5i7AMOWVwsV+3fOoY3X/lOXry6Nv1mjVOAbtZA/92lZb86W/auKfR6SoCAAAn0bMMAICcl3JYFgwGNX/+fE2ZMkVTp07VsmXLDrrv22+/rVmzZmnChAm6+OKLtX79+g6vT5kyRcccc0yHP01NTSmfB0jV0JKAbj7vePlnLdde9xANNao1f/f1uv2hR3XDI//R0+/sVXMo6nQ1AQBAL7NtZisDACDXuVM9YMmSJVq/fr2WL1+unTt36qabbtLgwYM1ffr0Dvs1Nzfrq1/9qi644ALdeeed+v3vf6+vfe1revbZZ5WXl6c9e/aooaFBzz33nPx+f/K4vLy8lM4DdMeQgYNlfOVxNf51tkr3/0e/8y7SJe/frFveq5HPbeqs0WW66IRBmjysWAbDMQAAyHqDi32SJL/H5XBNAACAU1LqWdbc3KwVK1ZowYIFqqys1Lnnnqurr75aDz30UKd9V65cKZ/Pp7lz52r06NFasGCB8vPz9fTTT0uStmzZov79+2vYsGHq379/8o9hGCmdB+guO3+AWmc+rHD/8So36vV43g/0XwUr5Yo0628bq3TtinX67NJ/6f89v0Wrd9QpavGNMwAAH5XKqIBrr7220+iC559/vhdre3CDCmNh2YCigMM1AQAATkmpZ9mGDRsUiUQ0ceLE5LbJkyfr3nvvlWVZMs227G3t2rWaPHlysjeOYRiaNGmS1qxZo5kzZ2rz5s0aOXJkt88D9ATbV6y6z/6viv/6ZQWq1unrelBXFq/UU0Vf0I/2nKrdDdLv3/xQv3/zQ5Xle3XO0eU6bWSpJgwuVqE/5Q6aAABknVRGBWzZskU//vGPddpppyW3FRcX92Z1D4E5ywAAyHUpfcqvqqpSaWmpvF5vclt5ebmCwaBqa2vVr1+/DvuOGTOmw/FlZWXatGmTpNhNUktLiy699FK99957GjdunObPn6+RI0emdJ7DydTIuUS5jMxzRkbaP1CqulmPy/fuo8r790/lrftAF1b9Sp913a+6/qP1tkZpbX2B9rd61fwfv15Y59dKBVRSXKJhFQM0dMQxqhx5lMryvYc/1xGO339n0f7O4xo4K5PtzzVNT2JUwNKlS1VZWanKykpt2rRJDz30UKewLBQKaceOHRo/frz69+/vUI0PITFnGb8LAADkrJTCspaWlg4BlqTk81Ao1KV9E/tt3bpVdXV1+s53vqOCggItXbpUl19+uZ588smUznM4ZWWFKe2fqkyXj0PLSPsPuEI67SvS2t9LL/9ERs37Kml4Vx/Tu/qYIcnzkf1bJL0f+1PzfIF2uoaosfho2f3HKX/YCRo05kSVVQyVkYU9Ivn9dxbt7zyugbNo/74jlVEBW7dulWEYGjZsWLfPm5nAtK1nGeGpM/hCwlm0v/O4Bs6i/Z3VV74UTSks8/l8ncKqxPP2k/Qfat/Efr/5zW8UDoeVn58vSbrrrrt01lln6fnnn0/pPIezb1+DMrGokWHEbtIzVT4OrVfa/6iLpC/NkNm0S+696+Su+o+MlhoZ4UYZ4SYZoSZFWxsUammQ0Vqrouh+lRqNKrU2SjUbpZonpHcl/V2qV752uoepLn+kjMJBKioqUeGAUcrrP1JWyQjZvr4y9KRr+P13Fu3vPK6BszLZ/omykZpURgVs3bpVBQUFmjt3rv71r39p4MCBmjNnjs4666yUz5uRa7U/PleZYfC74DDa31m0v/O4Bs6i/Z3ldPunFJZVVFSopqZGkUhEbnfs0KqqKvn9fhUVFXXat7q6usO26upqDRgwQFKsp1j7Gyqfz6ehQ4dqz549mjRpUpfPczi2rYx+kMl0+Ti0zLe/oWj+YEVHDlZw5KFXYq0KN6t17ybtfG+9grvfVkHduxoQfE+DrD0qMppUFNkg1W2Q6uIHvN12bJNZpObAINkFg+QtGSJPyWBZeQNkBcplBcpkBcok0yMZku0pkO0tkIwu9lSzrVgjRYMy7KhsT74UaZVMt+Tq3pBRfv+dRfs7j2vgLNq/70hlVMDWrVvV2tqqqVOn6qtf/aqeffZZXXvttfrDH/6g8ePHp3TeTASmnvpmxb7CMgjEHcIXEs6i/Z3HNXAW7e+svvKlaEph2bhx4+R2u7VmzRpNmTJFkrRq1SqNHz++06T7EyZM0NKlS2XbtgzDkG3bevPNN/X1r39dtm3r3HPP1Te+8Q3NnDlTUmyuiw8++ECjRo1K6TxAn+HJk3/IBI0aMqHD5l2tzdq77R017nxbVtW7ijRVK9pSp7LIbh1l7NUAo1b5Vr3ym+qlpo3SnsOfypYh21sg21sY++P2ywg3y4gGJRmyDUOSIRmGXI27ZUSaD1iO5cmPHWNFY+Gb6Zbt8kkur+z4H7l8kuGSIi0yoq2SZcmQJclWqcuvaPGI+J+RsZ8lIxUtOkpyp9YLFABw5EplVMA3vvENXXrppckJ/Y899li99dZb+uMf/5hyWJaRwDQ5Z5lBIOsw2t9ZtL/zuAbOov2d5XT7pxSWBQIBzZgxQwsXLtQdd9yhvXv3atmyZVq0aJGkWO+vwsJC+f1+TZ8+XT/5yU90++2365JLLtHDDz+slpYWnXfeeTIMQ2effbZ+8YtfaMiQIerXr59+/vOfa+DAgTrrrLPkcrkOeR7gSOLz52nY2MnS2MkdtreGo9q4v1nP7qlW7a7Natm/TdH6XfK37tFA1ajcqFO5Uacyo1791CCXLJmy5DGiMmTLCDVIoYZu1c0MN7U9saNSNBoP3LrGJcnVsEPa8UqH7bYMRfuNVWjo6YoMmKBoyWhFi0fI9pd0q74AgL4pldEHpml2Wvly1KhR2rx5c6/V95CSd+ZMVgMAQK5KKSyTpHnz5mnhwoWaPXu2CgoKNGfOHE2bNk2SNHXqVC1atEgzZ85UQUGB7rvvPt1666364x//qGOOOUb333+/8vLyJEk33nij3G63vvvd76qxsVGnnnqq7r//frlcrsOeB8gGfo9Lx1YU6tiKQumEkcntLeGodtW3and9UJvrW7WrPph8vrshqIamRuVZzSo0mlWo2E+fwmqWXyE79lfakC0z1v9MtSpU2Fcivz9P+YGABvnCCuTlq9RnqMLTrPy8fBXm+VXsM1XilYo9lgJmVKYVkqIhGdGQZEdluwOS2y/bcMkwTZX2K1Ttnl0ya9+Xq+59uWrfk6vuPblq35MZbpR7/0a592/s8J4tb5GiRUfJKhqmaOFQWYVDFC0cIttfKstXIttXLMtfIrkDvXkpAADdlMqogP/6r/+SYRgdvgTdsGGDxo4d26t1Pri2nmUAACA3Gbad3R0Lq6szN8F/eXlhxsrHoeVy+1u2rbqWsKqbQqpqDKm6KaR9H3lc2xJWXUtYda2RtM7hNg2VBDyxP3kelfg9Kgm41S/Pq/4FXlUU+TR2WKm8kYgKvG4Z7T9Q2LaM5ip5dv1L3g9flWv/Brlrtspsqer6e/SVKDzopNhQU5cvFtK5fLLdfsnll+32tRsu6osNF43v02mb6ZXtbtseG1Z6ZH8AyuXf/76Ca+CsTLZ/omyk7vvf/77efPPN5KiAm266SYsWLdK0adM6jD7429/+pu985zv60Y9+pIkTJ+rxxx/X0qVL9eSTT2ro0KEpnTMTvwPerc+o+KmrpCFTVH3RY/wddwD/xjqL9nce18BZtL+z+sp9Xso9ywA4yzQMleZ5VZrn1dH9D71vxLLV0BpWTUtYtS1h1bZEVNsciv2Mb6uJB2uJ5y1hSxHLVnVTLHw7HL/b1IBCnwYUeDWg0Kf+BT4NKPCpovA0DTz2bFUU+lQccMuItMhVv12uhu0y67fJ1fChXA07ZDbukhGsldlaKyNYJ8OOygzWyvf+sz3UYp19NEBLzM0WC+Z8H9nW9tN2eSXT02Fet9hPfzycSxzbvhz/AbYn5oJjDkYA2aOrow+mTZumW2+9Vb/61a+0c+dOHX300fr1r3+dclCWOfQsAwAg1xGWAVnMbbYFa13VGo7Ge6ZFkmFaIkhL9GCragyqqimkmuawWiOWttW0aFtNy0HL9LlNDSjwqqLQpwGFQzWgYHQsYKvwaUChVwMKfCrN88iUZIQb5dq3Qe5978iItMqIBKVoa+xxtFWKtMqIhmJzq0VD8ddDMqKx7YoGZUSCbY/jf9ozrJCM0OGDwExrC+18svLKY+/NiipaOERGuFm2v1RGpFmWr0RmS7WihUNlhJtlBuuk8uEq3v+hbMOlaOFQmc17Y6udegsVKT1aZss+mU27YvsmhtNa4dhcPIaZ/GMbZmwBB8OI/zRlGy7JTGx3xfYxXR95Pf7cdLUrI3acHX9shBsly5JcHtmmJ7aiq8st23Anr1/svGay7GT5iTqaLkmmbPNg+8TrKlO22f59JOr10ePavxcjvt9B3r8S+5od36NhxLb7W2W0tBy43snHh/iw/dGvyvhgjiNcIBDQ4sWLtXjx4k6vbdzYcVj+rFmzNGvWrN6qWoqYswwAgFxHWAagA7/HpYEelwYWHXyfRPfVHbtqVdUY0p6GoPY2BlXV0PZ4T0Psz/7msIIRS9trW7W9tvWgZbriQz/75XlUEvCpNHCSSvM8sT8BTyz0iw8L7ZfnUaHvI8M/D8W2JSvcIWAzoq3xEKn9tmA8YGsXtEWCkhUPm+KhU/KY5LZ2x0aC7UK6kBQ/lxENxh6rLSBpC+0aZLZUt7VFw/YDvg3PntVtT3a/Ic9B3q6va62CHlB2mNdtGcnAry1IjAVqRrhdwGy6JVmx3odWJDZvnxWW7cmTEQ3J9hRIkVbZ3gIZkRbZ3kIZkVbZnnwpGoxvb5XtyYsd7/JJtiXbFf8tMT0yW/bL9gRkG27J45csK75/WGawXpa/RIYVkeUrjoXT4SZZgTIZVji2LdQoIxqS5SuJB7MFMlprY0Ghr1BGa51sT6Ctl6ZhygjWyvbG/jExQo2yAv1kWBEZ4eZY2dGQFGmRlddfRqRFRjQoy18mI9wUq7+/REawPn6OYhnBOtkur8IjzpHKT87AFQXUYTVMAACQmwjLAKTN73FpaElAQ0sOPiF/KGKpqqktPNtTH1RVY0h7G4Pa2xjS3oag9jWFFLVs7YvPudYViXCtyOdWvs+lfK9LeV638r2udn/cyvO6lO9zKc/jVoHPpXyvV3nevNjr+S753GbXQ7fuah/axXu/JXrLuRp2yHbnyXb75Kp9T7YnT2brftmefJnN1bICZTKbdsv2l0punwrDu9XgHSyFmuRq2K5o8UjZLq9cdR/IbPxQVv5AWQWDZflL4+FFvGeXYcZWPrVtybYkOyrDtuKPY89lJbZF488tGYnHthV/PbZfYtuBntuefNmmW0Y0HAsVrXAsyLEibcNYpXbH2u2Ot9rOZ0dlJF6zopLstjq0q3vH5+3KsTpu/+j7NdqVKVnt3m/7OiS2tdWjffB5MIbs2PtVrOiD/qZZsd97IxIPlBMr1QbrYj9b9sV+Nu9J9bcuK1mr75X+632nq4GsRc8yAAByHWEZgIzyuk0NKQ5oSPHBA7VI1NK+5rBqm8OqaQlpf3N8PrXm+J/E45bY0M+mUDTlcO1gXIaU73Mrz+OKh25uBTymfG6XvC5TPo8pn8uUzx3743Wb8iceJ153uzrt42u/T/ynx2XISIRE3sIOUUu0/Li29hh00iHrbBhSYXmhgkw66hjDkMrLClRdVSfbsg4S8sWDxXgA1yFYtO1Y7yvZkmwZVkS2YcbCMpc3NgzX5ZURbpLt8skINUpuv4xQg2x3IPnTDDXIdvvb9o80x0LRSGusZ5cViZ3Disjy95MRbpJh27FyTXesB5fLK8tXJLOlRjLdsd5bnrxY+S37ZLs8sZ5nviLJMGUG62UbpsxQYyyMtcKx8/uK40NvIzIisV5ztq843vvMJdtbkCwvVnZ1bJVdl1dmc7Vsb75swx0Lib1FsXYJ1sn2FcfC1nBTrLxIiyIVE1VArx9kCj3LAADIeYRlABzndpmqKPSporBrAwhDESs2l1pzWA3BiJpCUTWFImoORdUUiqo5FNvWGIrGtgUjag5H1RSM7dcU325LitpSfWtE9a0RqSGz71OSvC5D3nh45nW1BWux522v+dymPPHX3aahYMRSsd+jcNRS2LI0rH+h6htaZdm2yvK9ilq2ghFLHpep/gVeNcffY9iyZBpG/I9kxH+a7X66TEOuxM/EH0PJx4l93O0ef3SfDscbhkzTkDu+zTQllxE/3ox9+GwKRuUyDfncplzmEfqBNDF3meGS4oNiD5Rdkmf2PMOQCpyuBHLAEfpvEwAA6DbCMgBHHK87tXDtQCzbVmvYSoZnTfFQrSkUVWskqmDYUjBiKRS11BqJPQ5GLIUiloKRaPJ5cnu7/ULJ7VGFoh2jklDUVigalRTtZivs7ubxfUcieEsEcaZhyLJthaKW8jwuGYYh27aToZphxPZPPM7zuhSOWnIZhgIel4JRSx7TUNiy5XOZCluW/O7YPn6PSxHLls9lKGJJPrehqC15zNigSne7n1Lso3LivG7TkGVLHlfsZ2G+V03NIfncpsJRO/bTsuV3m4pEbXndpqKWLY/LiP80Zdl28v2Z8V4rtmJhoi1bbtNUxLLkdZmxerrNWL3dsfflTzz3uBSMJJ4nzt12nMdlyrJibWbH30NTMCKfO7YCa6IuXlesjs3hqPK9sbbJ87gUisZ+h/N9bkWitvK8ZnKl3HyPS1E7ds7mUFSmERuS3RSKymMacrsMeczYeZpCEQW8LklSS9hSgTd2bDBsqdDvVihqKRyxVeh3KxixFLEsFfrcag1bsmQr3+tWazj2dyXgcaklHJXbNHTi0GKVl2f8VxM5yqBnGQAAOY+wDEBOMuMhS57Xpf4ZPI9t2wpHY8FPKNoWpMWe2wolwrX4a6F2P4MRK3msz22qtiUsv9uU22WqMWrLjkRlWbb2N4eTvdKaQ1HVNIfjc7W5YyGQHQsHrXh9opbdts2WopatqG0rYtmy4o+jlq2opeRjK/564nE0/jiaOD75vO1xV3tURS07Hh12PqIuGum5i4GsMaDAq9cXfNLpaiBL2YmQzDCdrQgAAHAMYRkAZJBhGPK6Y0FWz5UZW420uo/PWWbZsfAtYrUL5Sw72WPIsu1kTzzLjgVtlhXbzzAU67kUjsqyYz28EuGebCli2zJiD9UciiR7VLWGLXndhsLRWI+uUMSW2zTUGrHkdhkKRSy5zI4/TdNQOGrJMBQPA2MFR+ONG7VsmUasV6BpxPZ1uwy5vB6FWkMKRqzkUNnkz/i53KapYDS2PRy1OvQqs5LXLhZMmoYUifcGC0dj9QtGYr3FWiPR2M+wJa/bVGs4Kp/bVEvEkrfduULx4xLHR+MnsWypwBfrjWbbStYlFK9bomeY2zTUEi/b4zLVFIzI7TLVEooqz+uSaUjN4ahMI1a3fK9LUSveC9DrVihiKWrZCluWDEl5XreaQxEZhiG/21Rj/Bw+t6mG1oi87thcfg2tUfk9plyGocZQRH5PrDdacyiqPE+s91tr/HyhqK2JQ4t7b2EO5Jzw0NMVGvFJeU++zOmqAAAAhxCWAQAywjQMmS5DbtfB9wl4DvFiH3akBJbZipwMmWT7S1V//v+ovLxQqu6FySwBAECfQ/9yAAAAAAAAII6wDAAAAAAAAIgjLAMAAAAAAADiCMsAAAAAAACAOMIyAAAAAAAAII6wDAAAAAAAAIgjLAMAAAAAAADiCMsAAAAAAACAOMIyAAAAAAAAII6wDAAAAAAAAIgjLAMAAAAAAADiCMsAAAAAAACAOMIyAAAAAAAAII6wDAAAAAAAAIhzO12BTDOMzJabqfJxaLS/s2h/Z9H+zuMaOCuT7c81PbJwn5edaH9n0f7O4xo4i/Z3Vl+5zzNs27Z7vgoAAAAAAADAkYdhmAAAAAAAAEAcYRkAAAAAAAAQR1gGAAAAAAAAxBGWAQAAAAAAAHGEZQAAAAAAAEAcYRkAAAAAAAAQR1gGAAAAAAAAxBGWAQAAAAAAAHGEZQAAAAAAAEAcYRkAAAAAAAAQR1iWomAwqPnz52vKlCmaOnWqli1b5nSVslYoFNL555+v119/Pbnt/7d3ryFNr3EcwL8eoiyiqxeqN0GxsFrbnJikdhFLUMuD1puKigKDJhaV2YWyMIiycpQhRpBR0dUSLCh6kZHds1p4CV1Xy9KNiix1JvudF8f+tNPpmND+f7fz/cBe7Hn+xeP3yfbl2di/oaEBS5YsgdFoRGJiIioqKjz+zM2bN5GcnAyDwYBFixahoaFB7WX7vKamJmRmZiIyMhKxsbHYsWMHXC4XAOavhpcvX2LZsmUwmUyYPn06Dh06pMwxf3Wlp6dj/fr1yvOamhrMmzcPBoMBaWlpqKqq8rj+woULiI+Ph8FggMViwfv379Vesl+4cuUKxo0b5/HIzMwEwD0g72PPUw97njbY87TFntd7sOdpw5d6Hg/LemjXrl2oqqrCkSNHkJOTg4KCAly6dEnrZfkdl8uF1atXo76+XhkTEVgsFgQFBaGkpAQpKSnIyMhAY2MjAKCxsREWiwWpqak4e/Yshg0bhhUrVkBEtPoxfI6IIDMzE21tbTh+/Djy8/Nx9epVWK1W5q8Ct9uN9PR0DB06FOfPn8e2bdtQWFiIsrIy5q+yixcv4tq1a8rz1tZWpKenIyIiAufOnYPJZMLy5cvR2toKAHj8+DE2bdqEjIwMnDp1Cp8+fcKGDRu0Wr5Ps9vtmDFjBioqKpTH9u3buQekCvY8dbDnaYM9T1vseb0He552fKrnCf2yL1++iF6vl9u3bytjBw4ckIULF2q4Kv9TX18vc+bMkdmzZ4tOp1PyvnnzphiNRvny5Yty7eLFi2Xfvn0iImK1Wj32orW1VUwmk8d+0X+z2+2i0+nE4XAoY2VlZRITE8P8VdDU1CQrV66UlpYWZcxisUhOTg7zV9GHDx9k6tSpkpaWJtnZ2SIicubMGYmLixO32y0iIm63W2bOnCklJSUiIpKVlaVcKyLS2Ngo48aNk1evXqn/A/i4NWvWyJ49e34Y5x6Qt7HnqYM9Tzvsedpiz+sd2PO05Us9j58s64EnT56gs7MTJpNJGTObzbDZbHC73RquzL/cvXsXkydPxqlTpzzGbTYbxo8fjwEDBihjZrMZjx49UuYjIiKUuf79+2PChAnKPHUvODgYhw4dQlBQkMf458+fmb8KQkJCYLVaMXDgQIgIKisrce/ePURGRjJ/Fe3cuRMpKSkYO3asMmaz2WA2mxEQEAAACAgIQHh4+E/zHzFiBEaOHAmbzabq2v3B06dPMXr06B/GuQfkbex56mDP0w57nrbY83oH9jxt+VLP42FZDzgcDgwdOhR9+/ZVxoKCguByufDx40ftFuZn5s+fj40bN6J///4e4w6HAyEhIR5jw4cPx7t3735pnro3aNAgxMbGKs/dbjeOHTuGqKgo5q+yuLg4zJ8/HyaTCQkJCcxfJbdu3cL9+/exYsUKj/Hu8m1ubmb+v4GI4Pnz56ioqEBCQgLi4+Oxe/dudHR0cA/I69jz1MGepx32vN6DPU8b7Hna8rWe18drf7Mfamtr8yhQAJTnHR0dWizpf+Vn+X/Lvrt56rm8vDzU1NTg7NmzKC4uZv4q2rdvH5xOJ7Zu3YodO3bw378KXC4XcnJysGXLFgQGBnrMdZdve3s78/8NGhsblaytVitev36N7du3o729nXtAXseepy2+zqmPPU877HnqY8/Tnq/1PB6W9UC/fv1+2Ixvz//5C0e/X79+/X54Z7ejo0PJ/mf7M2jQILWW6Ffy8vJw5MgR5OfnQ6fTMX+V6fV6AH+/sK9duxZpaWloa2vzuIb5/14FBQWYOHGix7vu3/ws3+7y/+cnJ+i/jRo1Cnfu3MHgwYMREBCAsLAwuN1uZGVlITIykntAXsWepy32DHWx52mLPU997Hna87Wex8OyHggNDcWHDx/Q2dmJPn3+js7hcCAwMJD/UakgNDQUdrvdY8zpdCofxwwNDYXT6fxhPiwsTLU1+ovc3FycOHECeXl5SEhIAMD81eB0OvHo0SPEx8crY2PHjsXXr18RHByMZ8+e/XA98/99Ll68CKfTqXxf0bcX5MuXLyM5Oflf8+0u/+DgYBVW7l+GDBni8XzMmDFwuVwIDg7mHpBXsedpiz1DPex52mDP0xZ7Xu/gSz2P31nWA2FhYejTp4/HFylWVlZCr9fjjz8YpbcZDAZUV1ejvb1dGausrITBYFDmKysrlbm2tjbU1NQo8/RrCgoKcPLkSezduxdJSUnKOPP3vtevXyMjIwNNTU3KWFVVFYYNGwaz2cz8vezo0aMoKytDaWkpSktLERcXh7i4OJSWlsJgMODhw4fKLdpFBA8ePPhp/m/fvsXbt2+Zfw9dv34dkydP9nh3vba2FkOGDIHZbOYekFex52mLPUMd7HnaYc/TFnue9nyu53ntPpt+avPmzZKUlCQ2m02uXLki4eHhcvnyZa2X5be+v6V4Z2enJCYmyqpVq6Surk6KiorEaDTKmzdvRESkoaFB9Hq9FBUVSV1dnaxcuVJmz56t3H6Wume32yUsLEzy8/OlubnZ48H8va+zs1NSU1Nl6dKlUl9fL+Xl5TJlyhQpLi5m/hrIzs5WblHd0tIiUVFRkpubK/X19ZKbmyvR0dHKLd4fPHggEyZMkNOnT0ttba0sXLhQli9fruXyfVJLS4vExsbK6tWr5enTp1JeXi4xMTFy8OBB7gGpgj1PXex56mLP0xZ7Xu/Cnqc+X+t5PCzrodbWVlm3bp0YjUaJiYmRw4cPa70kv/Z9iRIRefHihSxYsEAmTpwoSUlJcuPGDY/ry8vLZdasWTJp0iRZvHixvHr1Su0l+7SioiLR6XT/+hBh/mp49+6dWCwWCQ8Pl+joaCksLFSKEPNX1/clSkTEZrPJn3/+KXq9XubOnSvV1dUe15eUlMi0adPEaDSKxWKR9+/fq71kv1BXVydLliwRo9Eo0dHRsn//fuV3gHtA3saepy72PHWx52mPPa/3YM/Thi/1vACRrs+5ERERERERERER/c/xCxiIiIiIiIiIiIi68LCMiIiIiIiIiIioCw/LiIiIiIiIiIiIuvCwjIiIiIiIiIiIqAsPy4iIiIiIiIiIiLrwsIyIiIiIiIiIiKgLD8uIiIiIiIiIiIi68LCMiIiIiIiIiIioCw/LiIiIiIiIiIiIuvCwjIiIiIiIiIiIqAsPy4iIiIiIiIiIiLr8BWU6VRwz7431AAAAAElFTkSuQmCC\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "sns.lineplot(stats['train_loss'], ax=axs[0])\n",
    "sns.lineplot(stats['val_loss'], ax=axs[0])\n",
    "sns.lineplot(stats['train_acc'], ax=axs[1])\n",
    "sns.lineplot(stats['val_acc'], ax=axs[1])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "0.5"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = model.multiple_outputs(x_test)\n",
    "\n",
    "accuracy_score(y_test.flatten(), np.round(predictions.flatten()))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "x_train, x_val, x_test, y_train, y_val, y_test = load_monk1()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training:   0%|          | 0/1000[]\u001B[A\n",
      "Training:   0%|          | 0/1000[, loss=0.292, val_loss=0.283, val_acc=0.5]\u001B[A\n",
      "Training:   0%|          | 1/1000[, loss=0.259, val_loss=0.255, val_acc=0.5]\u001B[A\n",
      "Training:   0%|          | 2/1000[, loss=0.259, val_loss=0.255, val_acc=0.5]\u001B[A\n",
      "Training:   0%|          | 2/1000[, loss=0.252, val_loss=0.251, val_acc=0.5]\u001B[A\n",
      "Training:   0%|          | 3/1000[, loss=0.25, val_loss=0.25, val_acc=0.514]\u001B[A\n",
      "Training:   0%|          | 4/1000[, loss=0.25, val_loss=0.25, val_acc=0.514]\u001B[A\n",
      "Training:   0%|          | 4/1000[, loss=0.249, val_loss=0.25, val_acc=0.5] \u001B[A\n",
      "Training:   0%|          | 5/1000[, loss=0.248, val_loss=0.25, val_acc=0.5]\u001B[A\n",
      "Training:   1%|          | 6/1000[, loss=0.248, val_loss=0.25, val_acc=0.5]\u001B[A\n",
      "Training:   1%|          | 6/1000[, loss=0.248, val_loss=0.25, val_acc=0.5]\u001B[A\n",
      "Training:   1%|          | 7/1000[, loss=0.247, val_loss=0.25, val_acc=0.5]\u001B[A\n",
      "Training:   1%|          | 8/1000[, loss=0.247, val_loss=0.25, val_acc=0.5]\u001B[A\n",
      "Training:   1%|          | 8/1000[, loss=0.247, val_loss=0.249, val_acc=0.5]\u001B[A\n",
      "Training:   1%|          | 9/1000[, loss=0.246, val_loss=0.249, val_acc=0.5]\u001B[A\n",
      "Training:   1%|          | 10/1000[, loss=0.246, val_loss=0.249, val_acc=0.5]\u001B[A\n",
      "Training:   1%|          | 10/1000[, loss=0.245, val_loss=0.249, val_acc=0.5]\u001B[A\n",
      "Training:   1%|          | 11/1000[, loss=0.244, val_loss=0.248, val_acc=0.5]\u001B[A\n",
      "Training:   1%|          | 12/1000[, loss=0.244, val_loss=0.248, val_acc=0.5]\u001B[A\n",
      "Training:   1%|          | 12/1000[, loss=0.243, val_loss=0.247, val_acc=0.5]\u001B[A\n",
      "Training:   1%|▏         | 13/1000[, loss=0.241, val_loss=0.246, val_acc=0.509]\u001B[A\n",
      "Training:   1%|▏         | 14/1000[, loss=0.241, val_loss=0.246, val_acc=0.509]\u001B[A\n",
      "Training:   1%|▏         | 14/1000[, loss=0.239, val_loss=0.245, val_acc=0.521]\u001B[A\n",
      "Training:   2%|▏         | 15/1000[, loss=0.236, val_loss=0.244, val_acc=0.551]\u001B[A\n",
      "Training:   2%|▏         | 16/1000[, loss=0.236, val_loss=0.244, val_acc=0.551]\u001B[A\n",
      "Training:   2%|▏         | 16/1000[, loss=0.233, val_loss=0.242, val_acc=0.569]\u001B[A\n",
      "Training:   2%|▏         | 17/1000[, loss=0.23, val_loss=0.24, val_acc=0.593]  \u001B[A\n",
      "Training:   2%|▏         | 18/1000[, loss=0.23, val_loss=0.24, val_acc=0.593]\u001B[A\n",
      "Training:   2%|▏         | 18/1000[, loss=0.226, val_loss=0.238, val_acc=0.604]\u001B[A\n",
      "Training:   2%|▏         | 19/1000[, loss=0.221, val_loss=0.235, val_acc=0.63] \u001B[A\n",
      "Training:   2%|▏         | 20/1000[, loss=0.221, val_loss=0.235, val_acc=0.63]\u001B[A\n",
      "Training:   2%|▏         | 20/1000[, loss=0.216, val_loss=0.232, val_acc=0.646]\u001B[A\n",
      "Training:   2%|▏         | 21/1000[, loss=0.21, val_loss=0.229, val_acc=0.646] \u001B[A\n",
      "Training:   2%|▏         | 22/1000[, loss=0.21, val_loss=0.229, val_acc=0.646]\u001B[A\n",
      "Training:   2%|▏         | 22/1000[, loss=0.204, val_loss=0.226, val_acc=0.66]\u001B[A\n",
      "Training:   2%|▏         | 23/1000[, loss=0.198, val_loss=0.223, val_acc=0.667]\u001B[A\n",
      "Training:   2%|▏         | 24/1000[, loss=0.198, val_loss=0.223, val_acc=0.667]\u001B[A\n",
      "Training:   2%|▏         | 24/1000[, loss=0.192, val_loss=0.22, val_acc=0.674] \u001B[A\n",
      "Training:   2%|▎         | 25/1000[, loss=0.186, val_loss=0.217, val_acc=0.683]\u001B[A\n",
      "Training:   3%|▎         | 26/1000[, loss=0.186, val_loss=0.217, val_acc=0.683]\u001B[A\n",
      "Training:   3%|▎         | 26/1000[, loss=0.18, val_loss=0.214, val_acc=0.69]  \u001B[A\n",
      "Training:   3%|▎         | 27/1000[, loss=0.175, val_loss=0.212, val_acc=0.69]\u001B[A\n",
      "Training:   3%|▎         | 28/1000[, loss=0.175, val_loss=0.212, val_acc=0.69]\u001B[A\n",
      "Training:   3%|▎         | 28/1000[, loss=0.17, val_loss=0.21, val_acc=0.699] \u001B[A\n",
      "Training:   3%|▎         | 29/1000[, loss=0.166, val_loss=0.207, val_acc=0.706]\u001B[A\n",
      "Training:   3%|▎         | 30/1000[, loss=0.166, val_loss=0.207, val_acc=0.706]\u001B[A\n",
      "Training:   3%|▎         | 30/1000[, loss=0.162, val_loss=0.205, val_acc=0.708]\u001B[A\n",
      "Training:   3%|▎         | 31/1000[, loss=0.158, val_loss=0.203, val_acc=0.706]\u001B[A\n",
      "Training:   3%|▎         | 32/1000[, loss=0.158, val_loss=0.203, val_acc=0.706]\u001B[A\n",
      "Training:   3%|▎         | 32/1000[, loss=0.154, val_loss=0.201, val_acc=0.711]\u001B[A\n",
      "Training:   3%|▎         | 33/1000[, loss=0.151, val_loss=0.199, val_acc=0.711]\u001B[A\n",
      "Training:   3%|▎         | 34/1000[, loss=0.151, val_loss=0.199, val_acc=0.711]\u001B[A\n",
      "Training:   3%|▎         | 34/1000[, loss=0.147, val_loss=0.196, val_acc=0.715]\u001B[A\n",
      "Training:   4%|▎         | 35/1000[, loss=0.144, val_loss=0.193, val_acc=0.72] \u001B[A\n",
      "Training:   4%|▎         | 36/1000[, loss=0.144, val_loss=0.193, val_acc=0.72]\u001B[A\n",
      "Training:   4%|▎         | 36/1000[, loss=0.141, val_loss=0.191, val_acc=0.725]\u001B[A\n",
      "Training:   4%|▎         | 37/1000[, loss=0.139, val_loss=0.189, val_acc=0.727]\u001B[A\n",
      "Training:   4%|▍         | 38/1000[, loss=0.139, val_loss=0.189, val_acc=0.727]\u001B[A\n",
      "Training:   4%|▍         | 38/1000[, loss=0.136, val_loss=0.188, val_acc=0.731]\u001B[A\n",
      "Training:   4%|▍         | 39/1000[, loss=0.134, val_loss=0.185, val_acc=0.722]\u001B[A\n",
      "Training:   4%|▍         | 40/1000[, loss=0.134, val_loss=0.185, val_acc=0.722]\u001B[A\n",
      "Training:   4%|▍         | 40/1000[, loss=0.132, val_loss=0.184, val_acc=0.729]\u001B[A\n",
      "Training:   4%|▍         | 41/1000[, loss=0.13, val_loss=0.182, val_acc=0.729] \u001B[A\n",
      "Training:   4%|▍         | 42/1000[, loss=0.13, val_loss=0.182, val_acc=0.729]\u001B[A\n",
      "Training:   4%|▍         | 42/1000[, loss=0.128, val_loss=0.181, val_acc=0.731]\u001B[A\n",
      "Training:   4%|▍         | 43/1000[, loss=0.126, val_loss=0.18, val_acc=0.734] \u001B[A\n",
      "Training:   4%|▍         | 44/1000[, loss=0.126, val_loss=0.18, val_acc=0.734]\u001B[A\n",
      "Training:   4%|▍         | 44/1000[, loss=0.124, val_loss=0.178, val_acc=0.736]\u001B[A\n",
      "Training:   4%|▍         | 45/1000[, loss=0.123, val_loss=0.176, val_acc=0.738]\u001B[A\n",
      "Training:   5%|▍         | 46/1000[, loss=0.123, val_loss=0.176, val_acc=0.738]\u001B[A\n",
      "Training:   5%|▍         | 46/1000[, loss=0.121, val_loss=0.175, val_acc=0.743]\u001B[A\n",
      "Training:   5%|▍         | 47/1000[, loss=0.119, val_loss=0.173, val_acc=0.743]\u001B[A\n",
      "Training:   5%|▍         | 48/1000[, loss=0.119, val_loss=0.173, val_acc=0.743]\u001B[A\n",
      "Training:   5%|▍         | 48/1000[, loss=0.118, val_loss=0.172, val_acc=0.745]\u001B[A\n",
      "Training:   5%|▍         | 49/1000[, loss=0.117, val_loss=0.171, val_acc=0.748]\u001B[A\n",
      "Training:   5%|▌         | 50/1000[, loss=0.117, val_loss=0.171, val_acc=0.748]\u001B[A\n",
      "Training:   5%|▌         | 50/1000[, loss=0.115, val_loss=0.17, val_acc=0.75]  \u001B[A\n",
      "Training:   5%|▌         | 51/1000[, loss=0.114, val_loss=0.168, val_acc=0.75]\u001B[A\n",
      "Training:   5%|▌         | 52/1000[, loss=0.114, val_loss=0.168, val_acc=0.75]\u001B[A\n",
      "Training:   5%|▌         | 52/1000[, loss=0.113, val_loss=0.167, val_acc=0.75]\u001B[A\n",
      "Training:   5%|▌         | 53/1000[, loss=0.112, val_loss=0.166, val_acc=0.75]\u001B[A\n",
      "Training:   5%|▌         | 54/1000[, loss=0.112, val_loss=0.166, val_acc=0.75]\u001B[A\n",
      "Training:   5%|▌         | 54/1000[, loss=0.11, val_loss=0.165, val_acc=0.75] \u001B[A\n",
      "Training:   6%|▌         | 55/1000[, loss=0.109, val_loss=0.164, val_acc=0.75]\u001B[A\n",
      "Training:   6%|▌         | 56/1000[, loss=0.109, val_loss=0.164, val_acc=0.75]\u001B[A\n",
      "Training:   6%|▌         | 56/1000[, loss=0.108, val_loss=0.163, val_acc=0.752]\u001B[A\n",
      "Training:   6%|▌         | 57/1000[, loss=0.107, val_loss=0.162, val_acc=0.757]\u001B[A\n",
      "Training:   6%|▌         | 58/1000[, loss=0.107, val_loss=0.162, val_acc=0.757]\u001B[A\n",
      "Training:   6%|▌         | 58/1000[, loss=0.106, val_loss=0.161, val_acc=0.757]\u001B[A\n",
      "Training:   6%|▌         | 59/1000[, loss=0.105, val_loss=0.16, val_acc=0.759] \u001B[A\n",
      "Training:   6%|▌         | 60/1000[, loss=0.105, val_loss=0.16, val_acc=0.759]\u001B[A\n",
      "Training:   6%|▌         | 60/1000[, loss=0.104, val_loss=0.159, val_acc=0.757]\u001B[A\n",
      "Training:   6%|▌         | 61/1000[, loss=0.102, val_loss=0.158, val_acc=0.762]\u001B[A\n",
      "Training:   6%|▌         | 62/1000[, loss=0.102, val_loss=0.158, val_acc=0.762]\u001B[A\n",
      "Training:   6%|▌         | 62/1000[, loss=0.101, val_loss=0.157, val_acc=0.762]\u001B[A\n",
      "Training:   6%|▋         | 63/1000[, loss=0.1, val_loss=0.156, val_acc=0.762]  \u001B[A\n",
      "Training:   6%|▋         | 64/1000[, loss=0.1, val_loss=0.156, val_acc=0.762]\u001B[A\n",
      "Training:   6%|▋         | 64/1000[, loss=0.0993, val_loss=0.155, val_acc=0.764]\u001B[A\n",
      "Training:   6%|▋         | 65/1000[, loss=0.0982, val_loss=0.154, val_acc=0.764]\u001B[A\n",
      "Training:   7%|▋         | 66/1000[, loss=0.0982, val_loss=0.154, val_acc=0.764]\u001B[A\n",
      "Training:   7%|▋         | 66/1000[, loss=0.0972, val_loss=0.153, val_acc=0.764]\u001B[A\n",
      "Training:   7%|▋         | 67/1000[, loss=0.0963, val_loss=0.153, val_acc=0.762]\u001B[A\n",
      "Training:   7%|▋         | 68/1000[, loss=0.0963, val_loss=0.153, val_acc=0.762]\u001B[A\n",
      "Training:   7%|▋         | 68/1000[, loss=0.0953, val_loss=0.152, val_acc=0.762]\u001B[A\n",
      "Training:   7%|▋         | 69/1000[, loss=0.0943, val_loss=0.151, val_acc=0.759]\u001B[A\n",
      "Training:   7%|▋         | 70/1000[, loss=0.0943, val_loss=0.151, val_acc=0.759]\u001B[A\n",
      "Training:   7%|▋         | 70/1000[, loss=0.0933, val_loss=0.15, val_acc=0.759] \u001B[A\n",
      "Training:   7%|▋         | 71/1000[, loss=0.0923, val_loss=0.149, val_acc=0.759]\u001B[A\n",
      "Training:   7%|▋         | 72/1000[, loss=0.0923, val_loss=0.149, val_acc=0.759]\u001B[A\n",
      "Training:   7%|▋         | 72/1000[, loss=0.0913, val_loss=0.148, val_acc=0.764]\u001B[A\n",
      "Training:   7%|▋         | 73/1000[, loss=0.0904, val_loss=0.147, val_acc=0.766]\u001B[A\n",
      "Training:   7%|▋         | 74/1000[, loss=0.0904, val_loss=0.147, val_acc=0.766]\u001B[A\n",
      "Training:   7%|▋         | 74/1000[, loss=0.0894, val_loss=0.146, val_acc=0.771]\u001B[A\n",
      "Training:   8%|▊         | 75/1000[, loss=0.0885, val_loss=0.145, val_acc=0.78] \u001B[A\n",
      "Training:   8%|▊         | 76/1000[, loss=0.0885, val_loss=0.145, val_acc=0.78]\u001B[A\n",
      "Training:   8%|▊         | 76/1000[, loss=0.0876, val_loss=0.144, val_acc=0.782]\u001B[A\n",
      "Training:   8%|▊         | 77/1000[, loss=0.0866, val_loss=0.143, val_acc=0.78] \u001B[A\n",
      "Training:   8%|▊         | 78/1000[, loss=0.0866, val_loss=0.143, val_acc=0.78]\u001B[A\n",
      "Training:   8%|▊         | 78/1000[, loss=0.0856, val_loss=0.142, val_acc=0.78]\u001B[A\n",
      "Training:   8%|▊         | 79/1000[, loss=0.0847, val_loss=0.141, val_acc=0.78]\u001B[A\n",
      "Training:   8%|▊         | 80/1000[, loss=0.0847, val_loss=0.141, val_acc=0.78]\u001B[A\n",
      "Training:   8%|▊         | 80/1000[, loss=0.0837, val_loss=0.14, val_acc=0.782]\u001B[A\n",
      "Training:   8%|▊         | 81/1000[, loss=0.0837, val_loss=0.14, val_acc=0.782]\u001B[A\n",
      "Training:   8%|▊         | 81/1000[, loss=0.0829, val_loss=0.139, val_acc=0.782]\u001B[A\n",
      "Training:   8%|▊         | 82/1000[, loss=0.0829, val_loss=0.139, val_acc=0.782]\u001B[A\n",
      "Training:   8%|▊         | 82/1000[, loss=0.0819, val_loss=0.138, val_acc=0.787]\u001B[A\n",
      "Training:   8%|▊         | 83/1000[, loss=0.0819, val_loss=0.138, val_acc=0.787]\u001B[A\n",
      "Training:   8%|▊         | 83/1000[, loss=0.0811, val_loss=0.137, val_acc=0.789]\u001B[A\n",
      "Training:   8%|▊         | 84/1000[, loss=0.0802, val_loss=0.136, val_acc=0.789]\u001B[A\n",
      "Training:   8%|▊         | 85/1000[, loss=0.0802, val_loss=0.136, val_acc=0.789]\u001B[A\n",
      "Training:   8%|▊         | 85/1000[, loss=0.0793, val_loss=0.135, val_acc=0.796]\u001B[A\n",
      "Training:   9%|▊         | 86/1000[, loss=0.0785, val_loss=0.134, val_acc=0.796]\u001B[A\n",
      "Training:   9%|▊         | 87/1000[, loss=0.0785, val_loss=0.134, val_acc=0.796]\u001B[A\n",
      "Training:   9%|▊         | 87/1000[, loss=0.0776, val_loss=0.133, val_acc=0.796]\u001B[A\n",
      "Training:   9%|▉         | 88/1000[, loss=0.0767, val_loss=0.132, val_acc=0.803]\u001B[A\n",
      "Training:   9%|▉         | 89/1000[, loss=0.0767, val_loss=0.132, val_acc=0.803]\u001B[A\n",
      "Training:   9%|▉         | 89/1000[, loss=0.0757, val_loss=0.131, val_acc=0.806]\u001B[A\n",
      "Training:   9%|▉         | 90/1000[, loss=0.0748, val_loss=0.13, val_acc=0.81]  \u001B[A\n",
      "Training:   9%|▉         | 91/1000[, loss=0.0748, val_loss=0.13, val_acc=0.81]\u001B[A\n",
      "Training:   9%|▉         | 91/1000[, loss=0.074, val_loss=0.129, val_acc=0.812]\u001B[A\n",
      "Training:   9%|▉         | 92/1000[, loss=0.0731, val_loss=0.128, val_acc=0.824]\u001B[A\n",
      "Training:   9%|▉         | 93/1000[, loss=0.0731, val_loss=0.128, val_acc=0.824]\u001B[A\n",
      "Training:   9%|▉         | 93/1000[, loss=0.0722, val_loss=0.127, val_acc=0.826]\u001B[A\n",
      "Training:   9%|▉         | 94/1000[, loss=0.0714, val_loss=0.126, val_acc=0.826]\u001B[A\n",
      "Training:  10%|▉         | 95/1000[, loss=0.0714, val_loss=0.126, val_acc=0.826]\u001B[A\n",
      "Training:  10%|▉         | 95/1000[, loss=0.0706, val_loss=0.126, val_acc=0.836]\u001B[A\n",
      "Training:  10%|▉         | 96/1000[, loss=0.0698, val_loss=0.125, val_acc=0.838]\u001B[A\n",
      "Training:  10%|▉         | 97/1000[, loss=0.0698, val_loss=0.125, val_acc=0.838]\u001B[A\n",
      "Training:  10%|▉         | 97/1000[, loss=0.069, val_loss=0.124, val_acc=0.845] \u001B[A\n",
      "Training:  10%|▉         | 98/1000[, loss=0.0683, val_loss=0.123, val_acc=0.852]\u001B[A\n",
      "Training:  10%|▉         | 99/1000[, loss=0.0683, val_loss=0.123, val_acc=0.852]\u001B[A\n",
      "Training:  10%|▉         | 99/1000[, loss=0.0675, val_loss=0.122, val_acc=0.852]\u001B[A\n",
      "Training:  10%|█         | 100/1000[, loss=0.0669, val_loss=0.121, val_acc=0.856]\u001B[A\n",
      "Training:  10%|█         | 101/1000[, loss=0.0669, val_loss=0.121, val_acc=0.856]\u001B[A\n",
      "Training:  10%|█         | 101/1000[, loss=0.0661, val_loss=0.121, val_acc=0.859]\u001B[A\n",
      "Training:  10%|█         | 102/1000[, loss=0.0654, val_loss=0.12, val_acc=0.859] \u001B[A\n",
      "Training:  10%|█         | 103/1000[, loss=0.0654, val_loss=0.12, val_acc=0.859]\u001B[A\n",
      "Training:  10%|█         | 103/1000[, loss=0.0647, val_loss=0.119, val_acc=0.863]\u001B[A\n",
      "Training:  10%|█         | 104/1000[, loss=0.064, val_loss=0.118, val_acc=0.866] \u001B[A\n",
      "Training:  10%|█         | 105/1000[, loss=0.064, val_loss=0.118, val_acc=0.866]\u001B[A\n",
      "Training:  10%|█         | 105/1000[, loss=0.0633, val_loss=0.118, val_acc=0.863]\u001B[A\n",
      "Training:  11%|█         | 106/1000[, loss=0.0626, val_loss=0.117, val_acc=0.868]\u001B[A\n",
      "Training:  11%|█         | 107/1000[, loss=0.0626, val_loss=0.117, val_acc=0.868]\u001B[A\n",
      "Training:  11%|█         | 107/1000[, loss=0.0619, val_loss=0.117, val_acc=0.868]\u001B[A\n",
      "Training:  11%|█         | 108/1000[, loss=0.0613, val_loss=0.116, val_acc=0.866]\u001B[A\n",
      "Training:  11%|█         | 109/1000[, loss=0.0613, val_loss=0.116, val_acc=0.866]\u001B[A\n",
      "Training:  11%|█         | 109/1000[, loss=0.0605, val_loss=0.116, val_acc=0.868]\u001B[A\n",
      "Training:  11%|█         | 110/1000[, loss=0.0598, val_loss=0.115, val_acc=0.866]\u001B[A\n",
      "Training:  11%|█         | 111/1000[, loss=0.0598, val_loss=0.115, val_acc=0.866]\u001B[A\n",
      "Training:  11%|█         | 111/1000[, loss=0.0592, val_loss=0.114, val_acc=0.866]\u001B[A\n",
      "Training:  11%|█         | 112/1000[, loss=0.0586, val_loss=0.114, val_acc=0.868]\u001B[A\n",
      "Training:  11%|█▏        | 113/1000[, loss=0.0586, val_loss=0.114, val_acc=0.868]\u001B[A\n",
      "Training:  11%|█▏        | 113/1000[, loss=0.0579, val_loss=0.113, val_acc=0.868]\u001B[A\n",
      "Training:  11%|█▏        | 114/1000[, loss=0.0573, val_loss=0.113, val_acc=0.868]\u001B[A\n",
      "Training:  12%|█▏        | 115/1000[, loss=0.0573, val_loss=0.113, val_acc=0.868]\u001B[A\n",
      "Training:  12%|█▏        | 115/1000[, loss=0.0567, val_loss=0.112, val_acc=0.868]\u001B[A\n",
      "Training:  12%|█▏        | 116/1000[, loss=0.0561, val_loss=0.112, val_acc=0.866]\u001B[A\n",
      "Training:  12%|█▏        | 117/1000[, loss=0.0561, val_loss=0.112, val_acc=0.866]\u001B[A\n",
      "Training:  12%|█▏        | 117/1000[, loss=0.0554, val_loss=0.111, val_acc=0.866]\u001B[A\n",
      "Training:  12%|█▏        | 118/1000[, loss=0.0549, val_loss=0.111, val_acc=0.866]\u001B[A\n",
      "Training:  12%|█▏        | 119/1000[, loss=0.0549, val_loss=0.111, val_acc=0.866]\u001B[A\n",
      "Training:  12%|█▏        | 119/1000[, loss=0.0544, val_loss=0.11, val_acc=0.866] \u001B[A\n",
      "Training:  12%|█▏        | 120/1000[, loss=0.0539, val_loss=0.11, val_acc=0.866]\u001B[A\n",
      "Training:  12%|█▏        | 121/1000[, loss=0.0539, val_loss=0.11, val_acc=0.866]\u001B[A\n",
      "Training:  12%|█▏        | 121/1000[, loss=0.0532, val_loss=0.109, val_acc=0.863]\u001B[A\n",
      "Training:  12%|█▏        | 122/1000[, loss=0.0527, val_loss=0.109, val_acc=0.861]\u001B[A\n",
      "Training:  12%|█▏        | 123/1000[, loss=0.0527, val_loss=0.109, val_acc=0.861]\u001B[A\n",
      "Training:  12%|█▏        | 123/1000[, loss=0.0522, val_loss=0.108, val_acc=0.861]\u001B[A\n",
      "Training:  12%|█▏        | 124/1000[, loss=0.0516, val_loss=0.107, val_acc=0.859]\u001B[A\n",
      "Training:  12%|█▎        | 125/1000[, loss=0.0516, val_loss=0.107, val_acc=0.859]\u001B[A\n",
      "Training:  12%|█▎        | 125/1000[, loss=0.0511, val_loss=0.107, val_acc=0.861]\u001B[A\n",
      "Training:  13%|█▎        | 126/1000[, loss=0.0507, val_loss=0.107, val_acc=0.861]\u001B[A\n",
      "Training:  13%|█▎        | 127/1000[, loss=0.0507, val_loss=0.107, val_acc=0.861]\u001B[A\n",
      "Training:  13%|█▎        | 127/1000[, loss=0.0502, val_loss=0.106, val_acc=0.861]\u001B[A\n",
      "Training:  13%|█▎        | 128/1000[, loss=0.0497, val_loss=0.106, val_acc=0.861]\u001B[A\n",
      "Training:  13%|█▎        | 129/1000[, loss=0.0497, val_loss=0.106, val_acc=0.861]\u001B[A\n",
      "Training:  13%|█▎        | 129/1000[, loss=0.0492, val_loss=0.105, val_acc=0.861]\u001B[A\n",
      "Training:  13%|█▎        | 130/1000[, loss=0.0487, val_loss=0.105, val_acc=0.861]\u001B[A\n",
      "Training:  13%|█▎        | 131/1000[, loss=0.0487, val_loss=0.105, val_acc=0.861]\u001B[A\n",
      "Training:  13%|█▎        | 131/1000[, loss=0.0482, val_loss=0.104, val_acc=0.863]\u001B[A\n",
      "Training:  13%|█▎        | 132/1000[, loss=0.0478, val_loss=0.104, val_acc=0.863]\u001B[A\n",
      "Training:  13%|█▎        | 133/1000[, loss=0.0478, val_loss=0.104, val_acc=0.863]\u001B[A\n",
      "Training:  13%|█▎        | 133/1000[, loss=0.0474, val_loss=0.103, val_acc=0.863]\u001B[A\n",
      "Training:  13%|█▎        | 134/1000[, loss=0.0469, val_loss=0.103, val_acc=0.863]\u001B[A\n",
      "Training:  14%|█▎        | 135/1000[, loss=0.0469, val_loss=0.103, val_acc=0.863]\u001B[A\n",
      "Training:  14%|█▎        | 135/1000[, loss=0.0464, val_loss=0.102, val_acc=0.866]\u001B[A\n",
      "Training:  14%|█▎        | 136/1000[, loss=0.0461, val_loss=0.102, val_acc=0.868]\u001B[A\n",
      "Training:  14%|█▎        | 137/1000[, loss=0.0461, val_loss=0.102, val_acc=0.868]\u001B[A\n",
      "Training:  14%|█▎        | 137/1000[, loss=0.0456, val_loss=0.101, val_acc=0.868]\u001B[A\n",
      "Training:  14%|█▍        | 138/1000[, loss=0.0451, val_loss=0.101, val_acc=0.87] \u001B[A\n",
      "Training:  14%|█▍        | 139/1000[, loss=0.0451, val_loss=0.101, val_acc=0.87]\u001B[A\n",
      "Training:  14%|█▍        | 139/1000[, loss=0.0447, val_loss=0.1, val_acc=0.87]  \u001B[A\n",
      "Training:  14%|█▍        | 140/1000[, loss=0.0443, val_loss=0.0996, val_acc=0.87]\u001B[A\n",
      "Training:  14%|█▍        | 141/1000[, loss=0.0443, val_loss=0.0996, val_acc=0.87]\u001B[A\n",
      "Training:  14%|█▍        | 141/1000[, loss=0.0439, val_loss=0.0992, val_acc=0.873]\u001B[A\n",
      "Training:  14%|█▍        | 142/1000[, loss=0.0434, val_loss=0.0985, val_acc=0.873]\u001B[A\n",
      "Training:  14%|█▍        | 143/1000[, loss=0.0434, val_loss=0.0985, val_acc=0.873]\u001B[A\n",
      "Training:  14%|█▍        | 143/1000[, loss=0.043, val_loss=0.0981, val_acc=0.87]  \u001B[A\n",
      "Training:  14%|█▍        | 144/1000[, loss=0.0427, val_loss=0.0974, val_acc=0.873]\u001B[A\n",
      "Training:  14%|█▍        | 145/1000[, loss=0.0427, val_loss=0.0974, val_acc=0.873]\u001B[A\n",
      "Training:  14%|█▍        | 145/1000[, loss=0.0422, val_loss=0.0969, val_acc=0.87] \u001B[A\n",
      "Training:  15%|█▍        | 146/1000[, loss=0.0418, val_loss=0.0963, val_acc=0.87]\u001B[A\n",
      "Training:  15%|█▍        | 147/1000[, loss=0.0418, val_loss=0.0963, val_acc=0.87]\u001B[A\n",
      "Training:  15%|█▍        | 147/1000[, loss=0.0414, val_loss=0.0958, val_acc=0.87]\u001B[A\n",
      "Training:  15%|█▍        | 148/1000[, loss=0.041, val_loss=0.0952, val_acc=0.87] \u001B[A\n",
      "Training:  15%|█▍        | 149/1000[, loss=0.041, val_loss=0.0952, val_acc=0.87]\u001B[A\n",
      "Training:  15%|█▍        | 149/1000[, loss=0.0406, val_loss=0.0946, val_acc=0.873]\u001B[A\n",
      "Training:  15%|█▌        | 150/1000[, loss=0.0402, val_loss=0.0942, val_acc=0.873]\u001B[A\n",
      "Training:  15%|█▌        | 151/1000[, loss=0.0402, val_loss=0.0942, val_acc=0.873]\u001B[A\n",
      "Training:  15%|█▌        | 151/1000[, loss=0.0399, val_loss=0.0935, val_acc=0.873]\u001B[A\n",
      "Training:  15%|█▌        | 152/1000[, loss=0.0394, val_loss=0.0931, val_acc=0.873]\u001B[A\n",
      "Training:  15%|█▌        | 153/1000[, loss=0.0394, val_loss=0.0931, val_acc=0.873]\u001B[A\n",
      "Training:  15%|█▌        | 153/1000[, loss=0.039, val_loss=0.0923, val_acc=0.873] \u001B[A\n",
      "Training:  15%|█▌        | 154/1000[, loss=0.0387, val_loss=0.0918, val_acc=0.873]\u001B[A\n",
      "Training:  16%|█▌        | 155/1000[, loss=0.0387, val_loss=0.0918, val_acc=0.873]\u001B[A\n",
      "Training:  16%|█▌        | 155/1000[, loss=0.0382, val_loss=0.0912, val_acc=0.873]\u001B[A\n",
      "Training:  16%|█▌        | 156/1000[, loss=0.0379, val_loss=0.0907, val_acc=0.873]\u001B[A\n",
      "Training:  16%|█▌        | 157/1000[, loss=0.0379, val_loss=0.0907, val_acc=0.873]\u001B[A\n",
      "Training:  16%|█▌        | 157/1000[, loss=0.0376, val_loss=0.09, val_acc=0.875]  \u001B[A\n",
      "Training:  16%|█▌        | 158/1000[, loss=0.0371, val_loss=0.0895, val_acc=0.875]\u001B[A\n",
      "Training:  16%|█▌        | 159/1000[, loss=0.0371, val_loss=0.0895, val_acc=0.875]\u001B[A\n",
      "Training:  16%|█▌        | 159/1000[, loss=0.0368, val_loss=0.0888, val_acc=0.877]\u001B[A\n",
      "Training:  16%|█▌        | 160/1000[, loss=0.0364, val_loss=0.0883, val_acc=0.877]\u001B[A\n",
      "Training:  16%|█▌        | 161/1000[, loss=0.0364, val_loss=0.0883, val_acc=0.877]\u001B[A\n",
      "Training:  16%|█▌        | 161/1000[, loss=0.0361, val_loss=0.0876, val_acc=0.877]\u001B[A\n",
      "Training:  16%|█▌        | 162/1000[, loss=0.0357, val_loss=0.0871, val_acc=0.877]\u001B[A\n",
      "Training:  16%|█▋        | 163/1000[, loss=0.0357, val_loss=0.0871, val_acc=0.877]\u001B[A\n",
      "Training:  16%|█▋        | 163/1000[, loss=0.0353, val_loss=0.0862, val_acc=0.877]\u001B[A\n",
      "Training:  16%|█▋        | 164/1000[, loss=0.035, val_loss=0.0857, val_acc=0.877] \u001B[A\n",
      "Training:  16%|█▋        | 165/1000[, loss=0.035, val_loss=0.0857, val_acc=0.877]\u001B[A\n",
      "Training:  16%|█▋        | 165/1000[, loss=0.0346, val_loss=0.0851, val_acc=0.877]\u001B[A\n",
      "Training:  17%|█▋        | 166/1000[, loss=0.0342, val_loss=0.0844, val_acc=0.877]\u001B[A\n",
      "Training:  17%|█▋        | 167/1000[, loss=0.0342, val_loss=0.0844, val_acc=0.877]\u001B[A\n",
      "Training:  17%|█▋        | 167/1000[, loss=0.0339, val_loss=0.0838, val_acc=0.877]\u001B[A\n",
      "Training:  17%|█▋        | 168/1000[, loss=0.0335, val_loss=0.0829, val_acc=0.88] \u001B[A\n",
      "Training:  17%|█▋        | 169/1000[, loss=0.0335, val_loss=0.0829, val_acc=0.88]\u001B[A\n",
      "Training:  17%|█▋        | 169/1000[, loss=0.0332, val_loss=0.0823, val_acc=0.88]\u001B[A\n",
      "Training:  17%|█▋        | 170/1000[, loss=0.0328, val_loss=0.0817, val_acc=0.882]\u001B[A\n",
      "Training:  17%|█▋        | 171/1000[, loss=0.0328, val_loss=0.0817, val_acc=0.882]\u001B[A\n",
      "Training:  17%|█▋        | 171/1000[, loss=0.0325, val_loss=0.0809, val_acc=0.884]\u001B[A\n",
      "Training:  17%|█▋        | 172/1000[, loss=0.032, val_loss=0.0803, val_acc=0.884] \u001B[A\n",
      "Training:  17%|█▋        | 173/1000[, loss=0.032, val_loss=0.0803, val_acc=0.884]\u001B[A\n",
      "Training:  17%|█▋        | 173/1000[, loss=0.0317, val_loss=0.0795, val_acc=0.887]\u001B[A\n",
      "Training:  17%|█▋        | 174/1000[, loss=0.0314, val_loss=0.0787, val_acc=0.889]\u001B[A\n",
      "Training:  18%|█▊        | 175/1000[, loss=0.0314, val_loss=0.0787, val_acc=0.889]\u001B[A\n",
      "Training:  18%|█▊        | 175/1000[, loss=0.031, val_loss=0.078, val_acc=0.889]  \u001B[A\n",
      "Training:  18%|█▊        | 176/1000[, loss=0.0307, val_loss=0.0773, val_acc=0.889]\u001B[A\n",
      "Training:  18%|█▊        | 177/1000[, loss=0.0307, val_loss=0.0773, val_acc=0.889]\u001B[A\n",
      "Training:  18%|█▊        | 177/1000[, loss=0.0304, val_loss=0.0766, val_acc=0.891]\u001B[A\n",
      "Training:  18%|█▊        | 178/1000[, loss=0.0301, val_loss=0.0758, val_acc=0.891]\u001B[A\n",
      "Training:  18%|█▊        | 179/1000[, loss=0.0301, val_loss=0.0758, val_acc=0.891]\u001B[A\n",
      "Training:  18%|█▊        | 179/1000[, loss=0.0297, val_loss=0.075, val_acc=0.891] \u001B[A\n",
      "Training:  18%|█▊        | 180/1000[, loss=0.0293, val_loss=0.0743, val_acc=0.894]\u001B[A\n",
      "Training:  18%|█▊        | 181/1000[, loss=0.0293, val_loss=0.0743, val_acc=0.894]\u001B[A\n",
      "Training:  18%|█▊        | 181/1000[, loss=0.0291, val_loss=0.0737, val_acc=0.894]\u001B[A\n",
      "Training:  18%|█▊        | 182/1000[, loss=0.0287, val_loss=0.0727, val_acc=0.896]\u001B[A\n",
      "Training:  18%|█▊        | 183/1000[, loss=0.0287, val_loss=0.0727, val_acc=0.896]\u001B[A\n",
      "Training:  18%|█▊        | 183/1000[, loss=0.0283, val_loss=0.0719, val_acc=0.896]\u001B[A\n",
      "Training:  18%|█▊        | 184/1000[, loss=0.028, val_loss=0.0711, val_acc=0.896] \u001B[A\n",
      "Training:  18%|█▊        | 185/1000[, loss=0.028, val_loss=0.0711, val_acc=0.896]\u001B[A\n",
      "Training:  18%|█▊        | 185/1000[, loss=0.0276, val_loss=0.0704, val_acc=0.896]\u001B[A\n",
      "Training:  19%|█▊        | 186/1000[, loss=0.0273, val_loss=0.0696, val_acc=0.896]\u001B[A\n",
      "Training:  19%|█▊        | 187/1000[, loss=0.0273, val_loss=0.0696, val_acc=0.896]\u001B[A\n",
      "Training:  19%|█▊        | 187/1000[, loss=0.027, val_loss=0.0688, val_acc=0.896] \u001B[A\n",
      "Training:  19%|█▉        | 188/1000[, loss=0.0266, val_loss=0.0677, val_acc=0.898]\u001B[A\n",
      "Training:  19%|█▉        | 189/1000[, loss=0.0266, val_loss=0.0677, val_acc=0.898]\u001B[A\n",
      "Training:  19%|█▉        | 189/1000[, loss=0.0263, val_loss=0.0669, val_acc=0.9]  \u001B[A\n",
      "Training:  19%|█▉        | 190/1000[, loss=0.0259, val_loss=0.0661, val_acc=0.903]\u001B[A\n",
      "Training:  19%|█▉        | 191/1000[, loss=0.0259, val_loss=0.0661, val_acc=0.903]\u001B[A\n",
      "Training:  19%|█▉        | 191/1000[, loss=0.0256, val_loss=0.0653, val_acc=0.905]\u001B[A\n",
      "Training:  19%|█▉        | 192/1000[, loss=0.0252, val_loss=0.0643, val_acc=0.905]\u001B[A\n",
      "Training:  19%|█▉        | 193/1000[, loss=0.0252, val_loss=0.0643, val_acc=0.905]\u001B[A\n",
      "Training:  19%|█▉        | 193/1000[, loss=0.0249, val_loss=0.0636, val_acc=0.907]\u001B[A\n",
      "Training:  19%|█▉        | 194/1000[, loss=0.0246, val_loss=0.0627, val_acc=0.907]\u001B[A\n",
      "Training:  20%|█▉        | 195/1000[, loss=0.0246, val_loss=0.0627, val_acc=0.907]\u001B[A\n",
      "Training:  20%|█▉        | 195/1000[, loss=0.0243, val_loss=0.0618, val_acc=0.907]\u001B[A\n",
      "Training:  20%|█▉        | 196/1000[, loss=0.024, val_loss=0.0607, val_acc=0.912] \u001B[A\n",
      "Training:  20%|█▉        | 197/1000[, loss=0.024, val_loss=0.0607, val_acc=0.912]\u001B[A\n",
      "Training:  20%|█▉        | 197/1000[, loss=0.0236, val_loss=0.0597, val_acc=0.914]\u001B[A\n",
      "Training:  20%|█▉        | 198/1000[, loss=0.0233, val_loss=0.0591, val_acc=0.917]\u001B[A\n",
      "Training:  20%|█▉        | 199/1000[, loss=0.0233, val_loss=0.0591, val_acc=0.917]\u001B[A\n",
      "Training:  20%|█▉        | 199/1000[, loss=0.0229, val_loss=0.058, val_acc=0.921] \u001B[A\n",
      "Training:  20%|██        | 200/1000[, loss=0.0226, val_loss=0.0572, val_acc=0.924]\u001B[A\n",
      "Training:  20%|██        | 201/1000[, loss=0.0226, val_loss=0.0572, val_acc=0.924]\u001B[A\n",
      "Training:  20%|██        | 201/1000[, loss=0.0223, val_loss=0.0559, val_acc=0.924]\u001B[A\n",
      "Training:  20%|██        | 202/1000[, loss=0.0219, val_loss=0.0553, val_acc=0.924]\u001B[A\n",
      "Training:  20%|██        | 203/1000[, loss=0.0219, val_loss=0.0553, val_acc=0.924]\u001B[A\n",
      "Training:  20%|██        | 203/1000[, loss=0.0216, val_loss=0.0542, val_acc=0.928]\u001B[A\n",
      "Training:  20%|██        | 204/1000[, loss=0.0213, val_loss=0.0534, val_acc=0.933]\u001B[A\n",
      "Training:  20%|██        | 205/1000[, loss=0.0213, val_loss=0.0534, val_acc=0.933]\u001B[A\n",
      "Training:  20%|██        | 205/1000[, loss=0.0209, val_loss=0.0523, val_acc=0.935]\u001B[A\n",
      "Training:  21%|██        | 206/1000[, loss=0.0206, val_loss=0.0515, val_acc=0.94] \u001B[A\n",
      "Training:  21%|██        | 207/1000[, loss=0.0206, val_loss=0.0515, val_acc=0.94]\u001B[A\n",
      "Training:  21%|██        | 207/1000[, loss=0.0203, val_loss=0.0506, val_acc=0.94]\u001B[A\n",
      "Training:  21%|██        | 208/1000[, loss=0.02, val_loss=0.0498, val_acc=0.942] \u001B[A\n",
      "Training:  21%|██        | 209/1000[, loss=0.02, val_loss=0.0498, val_acc=0.942]\u001B[A\n",
      "Training:  21%|██        | 209/1000[, loss=0.0196, val_loss=0.0486, val_acc=0.944]\u001B[A\n",
      "Training:  21%|██        | 210/1000[, loss=0.0193, val_loss=0.0478, val_acc=0.944]\u001B[A\n",
      "Training:  21%|██        | 211/1000[, loss=0.0193, val_loss=0.0478, val_acc=0.944]\u001B[A\n",
      "Training:  21%|██        | 211/1000[, loss=0.0189, val_loss=0.0468, val_acc=0.951]\u001B[A\n",
      "Training:  21%|██        | 212/1000[, loss=0.0186, val_loss=0.0458, val_acc=0.951]\u001B[A\n",
      "Training:  21%|██▏       | 213/1000[, loss=0.0186, val_loss=0.0458, val_acc=0.951]\u001B[A\n",
      "Training:  21%|██▏       | 213/1000[, loss=0.0183, val_loss=0.0448, val_acc=0.956]\u001B[A\n",
      "Training:  21%|██▏       | 214/1000[, loss=0.0179, val_loss=0.0433, val_acc=0.958]\u001B[A\n",
      "Training:  22%|██▏       | 215/1000[, loss=0.0179, val_loss=0.0433, val_acc=0.958]\u001B[A\n",
      "Training:  22%|██▏       | 215/1000[, loss=0.0176, val_loss=0.0421, val_acc=0.961]\u001B[A\n",
      "Training:  22%|██▏       | 216/1000[, loss=0.0173, val_loss=0.0412, val_acc=0.961]\u001B[A\n",
      "Training:  22%|██▏       | 217/1000[, loss=0.0173, val_loss=0.0412, val_acc=0.961]\u001B[A\n",
      "Training:  22%|██▏       | 217/1000[, loss=0.0169, val_loss=0.0399, val_acc=0.961]\u001B[A\n",
      "Training:  22%|██▏       | 218/1000[, loss=0.0165, val_loss=0.0388, val_acc=0.961]\u001B[A\n",
      "Training:  22%|██▏       | 219/1000[, loss=0.0165, val_loss=0.0388, val_acc=0.961]\u001B[A\n",
      "Training:  22%|██▏       | 219/1000[, loss=0.0162, val_loss=0.0375, val_acc=0.961]\u001B[A\n",
      "Training:  22%|██▏       | 220/1000[, loss=0.0158, val_loss=0.0362, val_acc=0.965]\u001B[A\n",
      "Training:  22%|██▏       | 221/1000[, loss=0.0158, val_loss=0.0362, val_acc=0.965]\u001B[A\n",
      "Training:  22%|██▏       | 221/1000[, loss=0.0154, val_loss=0.035, val_acc=0.965] \u001B[A\n",
      "Training:  22%|██▏       | 222/1000[, loss=0.015, val_loss=0.0336, val_acc=0.968]\u001B[A\n",
      "Training:  22%|██▏       | 223/1000[, loss=0.015, val_loss=0.0336, val_acc=0.968]\u001B[A\n",
      "Training:  22%|██▏       | 223/1000[, loss=0.0147, val_loss=0.0327, val_acc=0.972]\u001B[A\n",
      "Training:  22%|██▏       | 224/1000[, loss=0.0143, val_loss=0.0314, val_acc=0.972]\u001B[A\n",
      "Training:  22%|██▎       | 225/1000[, loss=0.0143, val_loss=0.0314, val_acc=0.972]\u001B[A\n",
      "Training:  22%|██▎       | 225/1000[, loss=0.0139, val_loss=0.03, val_acc=0.977]  \u001B[A\n",
      "Training:  23%|██▎       | 226/1000[, loss=0.0135, val_loss=0.0289, val_acc=0.988]\u001B[A\n",
      "Training:  23%|██▎       | 227/1000[, loss=0.0135, val_loss=0.0289, val_acc=0.988]\u001B[A\n",
      "Training:  23%|██▎       | 227/1000[, loss=0.0132, val_loss=0.028, val_acc=0.993] \u001B[A\n",
      "Training:  23%|██▎       | 228/1000[, loss=0.0128, val_loss=0.0268, val_acc=1]   \u001B[A\n",
      "Training:  23%|██▎       | 229/1000[, loss=0.0128, val_loss=0.0268, val_acc=1]\u001B[A\n",
      "Training:  23%|██▎       | 229/1000[, loss=0.0125, val_loss=0.0257, val_acc=1]\u001B[A\n",
      "Training:  23%|██▎       | 230/1000[, loss=0.0122, val_loss=0.0249, val_acc=1]\u001B[A\n",
      "Training:  23%|██▎       | 231/1000[, loss=0.0122, val_loss=0.0249, val_acc=1]\u001B[A\n",
      "Training:  23%|██▎       | 231/1000[, loss=0.0119, val_loss=0.0239, val_acc=1]\u001B[A\n",
      "Training:  23%|██▎       | 232/1000[, loss=0.0116, val_loss=0.023, val_acc=1] \u001B[A\n",
      "Training:  23%|██▎       | 233/1000[, loss=0.0116, val_loss=0.023, val_acc=1]\u001B[A\n",
      "Training:  23%|██▎       | 233/1000[, loss=0.0113, val_loss=0.0222, val_acc=1]\u001B[A\n",
      "Training:  23%|██▎       | 234/1000[, loss=0.011, val_loss=0.0213, val_acc=1] \u001B[A\n",
      "Training:  24%|██▎       | 235/1000[, loss=0.011, val_loss=0.0213, val_acc=1]\u001B[A\n",
      "Training:  24%|██▎       | 235/1000[, loss=0.0107, val_loss=0.0205, val_acc=1]\u001B[A\n",
      "Training:  24%|██▎       | 236/1000[, loss=0.0104, val_loss=0.0199, val_acc=1]\u001B[A\n",
      "Training:  24%|██▎       | 237/1000[, loss=0.0104, val_loss=0.0199, val_acc=1]\u001B[A\n",
      "Training:  24%|██▎       | 237/1000[, loss=0.0102, val_loss=0.0195, val_acc=1]\u001B[A\n",
      "Training:  24%|██▍       | 238/1000[, loss=0.00988, val_loss=0.0186, val_acc=1]\u001B[A\n",
      "Training:  24%|██▍       | 239/1000[, loss=0.00988, val_loss=0.0186, val_acc=1]\u001B[A\n",
      "Training:  24%|██▍       | 239/1000[, loss=0.00966, val_loss=0.018, val_acc=1] \u001B[A\n",
      "Training:  24%|██▍       | 240/1000[, loss=0.00943, val_loss=0.0175, val_acc=1]\u001B[A\n",
      "Training:  24%|██▍       | 241/1000[, loss=0.00943, val_loss=0.0175, val_acc=1]\u001B[A\n",
      "Training:  24%|██▍       | 241/1000[, loss=0.00917, val_loss=0.0169, val_acc=1]\u001B[A\n",
      "Training:  24%|██▍       | 242/1000[, loss=0.00897, val_loss=0.0163, val_acc=1]\u001B[A\n",
      "Training:  24%|██▍       | 243/1000[, loss=0.00897, val_loss=0.0163, val_acc=1]\u001B[A\n",
      "Training:  24%|██▍       | 243/1000[, loss=0.00876, val_loss=0.016, val_acc=1] \u001B[A\n",
      "Training:  24%|██▍       | 244/1000[, loss=0.00876, val_loss=0.016, val_acc=1]\u001B[A\n",
      "Training:  24%|██▍       | 244/1000[, loss=0.00855, val_loss=0.0156, val_acc=1]\u001B[A\n",
      "Training:  24%|██▍       | 245/1000[, loss=0.00832, val_loss=0.0149, val_acc=1]\u001B[A\n",
      "Training:  25%|██▍       | 246/1000[, loss=0.00832, val_loss=0.0149, val_acc=1]\u001B[A\n",
      "Training:  25%|██▍       | 246/1000[, loss=0.00808, val_loss=0.0145, val_acc=1]\u001B[A\n",
      "Training:  25%|██▍       | 247/1000[, loss=0.00808, val_loss=0.0145, val_acc=1]\u001B[A\n",
      "Training:  25%|██▍       | 247/1000[, loss=0.00793, val_loss=0.0142, val_acc=1]\u001B[A\n",
      "Training:  25%|██▍       | 248/1000[, loss=0.00775, val_loss=0.0137, val_acc=1]\u001B[A\n",
      "Training:  25%|██▍       | 249/1000[, loss=0.00775, val_loss=0.0137, val_acc=1]\u001B[A\n",
      "Training:  25%|██▍       | 249/1000[, loss=0.00753, val_loss=0.0133, val_acc=1]\u001B[A\n",
      "Training:  25%|██▌       | 250/1000[, loss=0.0074, val_loss=0.0131, val_acc=1] \u001B[A\n",
      "Training:  25%|██▌       | 251/1000[, loss=0.0074, val_loss=0.0131, val_acc=1]\u001B[A\n",
      "Training:  25%|██▌       | 251/1000[, loss=0.00722, val_loss=0.0126, val_acc=1]\u001B[A\n",
      "Training:  25%|██▌       | 252/1000[, loss=0.00701, val_loss=0.0123, val_acc=1]\u001B[A\n",
      "Training:  25%|██▌       | 253/1000[, loss=0.00701, val_loss=0.0123, val_acc=1]\u001B[A\n",
      "Training:  25%|██▌       | 253/1000[, loss=0.00691, val_loss=0.012, val_acc=1] \u001B[A\n",
      "Training:  25%|██▌       | 254/1000[, loss=0.00672, val_loss=0.0117, val_acc=1]\u001B[A\n",
      "Training:  26%|██▌       | 255/1000[, loss=0.00672, val_loss=0.0117, val_acc=1]\u001B[A\n",
      "Training:  26%|██▌       | 255/1000[, loss=0.00657, val_loss=0.0114, val_acc=1]\u001B[A\n",
      "Training:  26%|██▌       | 256/1000[, loss=0.00644, val_loss=0.0111, val_acc=1]\u001B[A\n",
      "Training:  26%|██▌       | 257/1000[, loss=0.00644, val_loss=0.0111, val_acc=1]\u001B[A\n",
      "Training:  26%|██▌       | 257/1000[, loss=0.00626, val_loss=0.0108, val_acc=1]\u001B[A\n",
      "Training:  26%|██▌       | 258/1000[, loss=0.00618, val_loss=0.0106, val_acc=1]\u001B[A\n",
      "Training:  26%|██▌       | 259/1000[, loss=0.00618, val_loss=0.0106, val_acc=1]\u001B[A\n",
      "Training:  26%|██▌       | 259/1000[, loss=0.00601, val_loss=0.0103, val_acc=1]\u001B[A\n",
      "Training:  26%|██▌       | 260/1000[, loss=0.00588, val_loss=0.0102, val_acc=1]\u001B[A\n",
      "Training:  26%|██▌       | 261/1000[, loss=0.00588, val_loss=0.0102, val_acc=1]\u001B[A\n",
      "Training:  26%|██▌       | 261/1000[, loss=0.0058, val_loss=0.00993, val_acc=1]\u001B[A\n",
      "Training:  26%|██▌       | 262/1000[, loss=0.00562, val_loss=0.00962, val_acc=1]\u001B[A\n",
      "Training:  26%|██▋       | 263/1000[, loss=0.00562, val_loss=0.00962, val_acc=1]\u001B[A\n",
      "Training:  26%|██▋       | 263/1000[, loss=0.00553, val_loss=0.00954, val_acc=1]\u001B[A\n",
      "Training:  26%|██▋       | 264/1000[, loss=0.00542, val_loss=0.00917, val_acc=1]\u001B[A\n",
      "Training:  26%|██▋       | 265/1000[, loss=0.00542, val_loss=0.00917, val_acc=1]\u001B[A\n",
      "Training:  26%|██▋       | 265/1000[, loss=0.00527, val_loss=0.00901, val_acc=1]\u001B[A\n",
      "Training:  27%|██▋       | 266/1000[, loss=0.00522, val_loss=0.00878, val_acc=1]\u001B[A\n",
      "Training:  27%|██▋       | 267/1000[, loss=0.00522, val_loss=0.00878, val_acc=1]\u001B[A\n",
      "Training:  27%|██▋       | 267/1000[, loss=0.00507, val_loss=0.0086, val_acc=1] \u001B[A\n",
      "Training:  27%|██▋       | 268/1000[, loss=0.00501, val_loss=0.00844, val_acc=1]\u001B[A\n",
      "Training:  27%|██▋       | 269/1000[, loss=0.00501, val_loss=0.00844, val_acc=1]\u001B[A\n",
      "Training:  27%|██▋       | 269/1000[, loss=0.00488, val_loss=0.00828, val_acc=1]\u001B[A\n",
      "Training:  27%|██▋       | 270/1000[, loss=0.0048, val_loss=0.00825, val_acc=1] \u001B[A\n",
      "Training:  27%|██▋       | 271/1000[, loss=0.0048, val_loss=0.00825, val_acc=1]\u001B[A\n",
      "Training:  27%|██▋       | 271/1000[, loss=0.0047, val_loss=0.00792, val_acc=1]\u001B[A\n",
      "Training:  27%|██▋       | 272/1000[, loss=0.00462, val_loss=0.0079, val_acc=1]\u001B[A\n",
      "Training:  27%|██▋       | 273/1000[, loss=0.00462, val_loss=0.0079, val_acc=1]\u001B[A\n",
      "Training:  27%|██▋       | 273/1000[, loss=0.00452, val_loss=0.0076, val_acc=1]\u001B[A\n",
      "Training:  27%|██▋       | 274/1000[, loss=0.00444, val_loss=0.00755, val_acc=1]\u001B[A\n",
      "Training:  28%|██▊       | 275/1000[, loss=0.00444, val_loss=0.00755, val_acc=1]\u001B[A\n",
      "Training:  28%|██▊       | 275/1000[, loss=0.00434, val_loss=0.00744, val_acc=1]\u001B[A\n",
      "Training:  28%|██▊       | 276/1000[, loss=0.00428, val_loss=0.00723, val_acc=1]\u001B[A\n",
      "Training:  28%|██▊       | 277/1000[, loss=0.00428, val_loss=0.00723, val_acc=1]\u001B[A\n",
      "Training:  28%|██▊       | 277/1000[, loss=0.0042, val_loss=0.00719, val_acc=1] \u001B[A\n",
      "Training:  28%|██▊       | 278/1000[, loss=0.00413, val_loss=0.00696, val_acc=1]\u001B[A\n",
      "Training:  28%|██▊       | 279/1000[, loss=0.00413, val_loss=0.00696, val_acc=1]\u001B[A\n",
      "Training:  28%|██▊       | 279/1000[, loss=0.00406, val_loss=0.00692, val_acc=1]\u001B[A\n",
      "Training:  28%|██▊       | 280/1000[, loss=0.00398, val_loss=0.00663, val_acc=1]\u001B[A\n",
      "Training:  28%|██▊       | 281/1000[, loss=0.00398, val_loss=0.00663, val_acc=1]\u001B[A\n",
      "Training:  28%|██▊       | 281/1000[, loss=0.00392, val_loss=0.00667, val_acc=1]\u001B[A\n",
      "Training:  28%|██▊       | 282/1000[, loss=0.00384, val_loss=0.00659, val_acc=1]\u001B[A\n",
      "Training:  28%|██▊       | 283/1000[, loss=0.00384, val_loss=0.00659, val_acc=1]\u001B[A\n",
      "Training:  28%|██▊       | 283/1000[, loss=0.00378, val_loss=0.0064, val_acc=1] \u001B[A\n",
      "Training:  28%|██▊       | 284/1000[, loss=0.00372, val_loss=0.00634, val_acc=1]\u001B[A\n",
      "Training:  28%|██▊       | 285/1000[, loss=0.00372, val_loss=0.00634, val_acc=1]\u001B[A\n",
      "Training:  28%|██▊       | 285/1000[, loss=0.00367, val_loss=0.00614, val_acc=1]\u001B[A\n",
      "Training:  29%|██▊       | 286/1000[, loss=0.00359, val_loss=0.00612, val_acc=1]\u001B[A\n",
      "Training:  29%|██▊       | 287/1000[, loss=0.00359, val_loss=0.00612, val_acc=1]\u001B[A\n",
      "Training:  29%|██▊       | 287/1000[, loss=0.00355, val_loss=0.00598, val_acc=1]\u001B[A\n",
      "Training:  29%|██▉       | 288/1000[, loss=0.00348, val_loss=0.00593, val_acc=1]\u001B[A\n",
      "Training:  29%|██▉       | 289/1000[, loss=0.00348, val_loss=0.00593, val_acc=1]\u001B[A\n",
      "Training:  29%|██▉       | 289/1000[, loss=0.00342, val_loss=0.00588, val_acc=1]\u001B[A\n",
      "Training:  29%|██▉       | 290/1000[, loss=0.00339, val_loss=0.0058, val_acc=1] \u001B[A\n",
      "Training:  29%|██▉       | 291/1000[, loss=0.00339, val_loss=0.0058, val_acc=1]\u001B[A\n",
      "Training:  29%|██▉       | 291/1000[, loss=0.00332, val_loss=0.00568, val_acc=1]\u001B[A\n",
      "Training:  29%|██▉       | 292/1000[, loss=0.00329, val_loss=0.00564, val_acc=1]\u001B[A\n",
      "Training:  29%|██▉       | 293/1000[, loss=0.00329, val_loss=0.00564, val_acc=1]\u001B[A\n",
      "Training:  29%|██▉       | 293/1000[, loss=0.00322, val_loss=0.00552, val_acc=1]\u001B[A\n",
      "Training:  29%|██▉       | 294/1000[, loss=0.00319, val_loss=0.00551, val_acc=1]\u001B[A\n",
      "Training:  30%|██▉       | 295/1000[, loss=0.00319, val_loss=0.00551, val_acc=1]\u001B[A\n",
      "Training:  30%|██▉       | 295/1000[, loss=0.00313, val_loss=0.00531, val_acc=1]\u001B[A\n",
      "Training:  30%|██▉       | 296/1000[, loss=0.00309, val_loss=0.00533, val_acc=1]\u001B[A\n",
      "Training:  30%|██▉       | 297/1000[, loss=0.00309, val_loss=0.00533, val_acc=1]\u001B[A\n",
      "Training:  30%|██▉       | 297/1000[, loss=0.00305, val_loss=0.00516, val_acc=1]\u001B[A\n",
      "Training:  30%|██▉       | 298/1000[, loss=0.00301, val_loss=0.0052, val_acc=1] \u001B[A\n",
      "Training:  30%|██▉       | 299/1000[, loss=0.00301, val_loss=0.0052, val_acc=1]\u001B[A\n",
      "Training:  30%|██▉       | 299/1000[, loss=0.00295, val_loss=0.00511, val_acc=1]\u001B[A\n",
      "Training:  30%|███       | 300/1000[, loss=0.00292, val_loss=0.00499, val_acc=1]\u001B[A\n",
      "Training:  30%|███       | 301/1000[, loss=0.00292, val_loss=0.00499, val_acc=1]\u001B[A\n",
      "Training:  30%|███       | 301/1000[, loss=0.00287, val_loss=0.00495, val_acc=1]\u001B[A\n",
      "Training:  30%|███       | 302/1000[, loss=0.00284, val_loss=0.00494, val_acc=1]\u001B[A\n",
      "Training:  30%|███       | 303/1000[, loss=0.00284, val_loss=0.00494, val_acc=1]\u001B[A\n",
      "Training:  30%|███       | 303/1000[, loss=0.0028, val_loss=0.00484, val_acc=1] \u001B[A\n",
      "Training:  30%|███       | 304/1000[, loss=0.00276, val_loss=0.0048, val_acc=1]\u001B[A\n",
      "Training:  30%|███       | 305/1000[, loss=0.00276, val_loss=0.0048, val_acc=1]\u001B[A\n",
      "Training:  30%|███       | 305/1000[, loss=0.00272, val_loss=0.00464, val_acc=1]\u001B[A\n",
      "Training:  31%|███       | 306/1000[, loss=0.00268, val_loss=0.00467, val_acc=1]\u001B[A\n",
      "Training:  31%|███       | 307/1000[, loss=0.00268, val_loss=0.00467, val_acc=1]\u001B[A\n",
      "Training:  31%|███       | 307/1000[, loss=0.00264, val_loss=0.00462, val_acc=1]\u001B[A\n",
      "Training:  31%|███       | 308/1000[, loss=0.00262, val_loss=0.00454, val_acc=1]\u001B[A\n",
      "Training:  31%|███       | 309/1000[, loss=0.00262, val_loss=0.00454, val_acc=1]\u001B[A\n",
      "Training:  31%|███       | 309/1000[, loss=0.00257, val_loss=0.0045, val_acc=1] \u001B[A\n",
      "Training:  31%|███       | 310/1000[, loss=0.00254, val_loss=0.00449, val_acc=1]\u001B[A\n",
      "Training:  31%|███       | 311/1000[, loss=0.00254, val_loss=0.00449, val_acc=1]\u001B[A\n",
      "Training:  31%|███       | 311/1000[, loss=0.00252, val_loss=0.0044, val_acc=1] \u001B[A\n",
      "Training:  31%|███       | 312/1000[, loss=0.00248, val_loss=0.00439, val_acc=1]\u001B[A\n",
      "Training:  31%|███▏      | 313/1000[, loss=0.00248, val_loss=0.00439, val_acc=1]\u001B[A\n",
      "Training:  31%|███▏      | 313/1000[, loss=0.00245, val_loss=0.00434, val_acc=1]\u001B[A\n",
      "Training:  31%|███▏      | 314/1000[, loss=0.00242, val_loss=0.00424, val_acc=1]\u001B[A\n",
      "Training:  32%|███▏      | 315/1000[, loss=0.00242, val_loss=0.00424, val_acc=1]\u001B[A\n",
      "Training:  32%|███▏      | 315/1000[, loss=0.00239, val_loss=0.00427, val_acc=1]\u001B[A\n",
      "Training:  32%|███▏      | 316/1000[, loss=0.00236, val_loss=0.00421, val_acc=1]\u001B[A\n",
      "Training:  32%|███▏      | 317/1000[, loss=0.00236, val_loss=0.00421, val_acc=1]\u001B[A\n",
      "Training:  32%|███▏      | 317/1000[, loss=0.00233, val_loss=0.0041, val_acc=1] \u001B[A\n",
      "Training:  32%|███▏      | 318/1000[, loss=0.0023, val_loss=0.00412, val_acc=1]\u001B[A\n",
      "Training:  32%|███▏      | 319/1000[, loss=0.0023, val_loss=0.00412, val_acc=1]\u001B[A\n",
      "Training:  32%|███▏      | 319/1000[, loss=0.00227, val_loss=0.004, val_acc=1] \u001B[A\n",
      "Training:  32%|███▏      | 320/1000[, loss=0.00224, val_loss=0.00399, val_acc=1]\u001B[A\n",
      "Training:  32%|███▏      | 321/1000[, loss=0.00224, val_loss=0.00399, val_acc=1]\u001B[A\n",
      "Training:  32%|███▏      | 321/1000[, loss=0.00221, val_loss=0.00396, val_acc=1]\u001B[A\n",
      "Training:  32%|███▏      | 322/1000[, loss=0.00219, val_loss=0.00387, val_acc=1]\u001B[A\n",
      "Training:  32%|███▏      | 323/1000[, loss=0.00219, val_loss=0.00387, val_acc=1]\u001B[A\n",
      "Training:  32%|███▏      | 323/1000[, loss=0.00216, val_loss=0.00386, val_acc=1]\u001B[A\n",
      "Training:  32%|███▏      | 324/1000[, loss=0.00214, val_loss=0.00388, val_acc=1]\u001B[A\n",
      "Training:  32%|███▎      | 325/1000[, loss=0.00214, val_loss=0.00388, val_acc=1]\u001B[A\n",
      "Training:  32%|███▎      | 325/1000[, loss=0.00211, val_loss=0.00376, val_acc=1]\u001B[A\n",
      "Training:  33%|███▎      | 326/1000[, loss=0.00208, val_loss=0.00374, val_acc=1]\u001B[A\n",
      "Training:  33%|███▎      | 327/1000[, loss=0.00208, val_loss=0.00374, val_acc=1]\u001B[A\n",
      "Training:  33%|███▎      | 327/1000[, loss=0.00206, val_loss=0.00372, val_acc=1]\u001B[A\n",
      "Training:  33%|███▎      | 328/1000[, loss=0.00204, val_loss=0.00364, val_acc=1]\u001B[A\n",
      "Training:  33%|███▎      | 329/1000[, loss=0.00204, val_loss=0.00364, val_acc=1]\u001B[A\n",
      "Training:  33%|███▎      | 329/1000[, loss=0.00201, val_loss=0.00364, val_acc=1]\u001B[A\n",
      "Training:  33%|███▎      | 330/1000[, loss=0.00199, val_loss=0.00362, val_acc=1]\u001B[A\n",
      "Training:  33%|███▎      | 331/1000[, loss=0.00199, val_loss=0.00362, val_acc=1]\u001B[A\n",
      "Training:  33%|███▎      | 331/1000[, loss=0.00197, val_loss=0.0036, val_acc=1] \u001B[A\n",
      "Training:  33%|███▎      | 332/1000[, loss=0.00195, val_loss=0.00351, val_acc=1]\u001B[A\n",
      "Training:  33%|███▎      | 333/1000[, loss=0.00195, val_loss=0.00351, val_acc=1]\u001B[A\n",
      "Training:  33%|███▎      | 333/1000[, loss=0.00193, val_loss=0.00352, val_acc=1]\u001B[A\n",
      "Training:  33%|███▎      | 334/1000[, loss=0.0019, val_loss=0.00343, val_acc=1] \u001B[A\n",
      "Training:  34%|███▎      | 335/1000[, loss=0.0019, val_loss=0.00343, val_acc=1]\u001B[A\n",
      "Training:  34%|███▎      | 335/1000[, loss=0.00188, val_loss=0.00344, val_acc=1]\u001B[A\n",
      "Training:  34%|███▎      | 336/1000[, loss=0.00186, val_loss=0.0034, val_acc=1] \u001B[A\n",
      "Training:  34%|███▎      | 337/1000[, loss=0.00186, val_loss=0.0034, val_acc=1]\u001B[A\n",
      "Training:  34%|███▎      | 337/1000[, loss=0.00185, val_loss=0.00334, val_acc=1]\u001B[A\n",
      "Training:  34%|███▍      | 338/1000[, loss=0.00182, val_loss=0.00335, val_acc=1]\u001B[A\n",
      "Training:  34%|███▍      | 339/1000[, loss=0.00182, val_loss=0.00335, val_acc=1]\u001B[A\n",
      "Training:  34%|███▍      | 339/1000[, loss=0.0018, val_loss=0.0033, val_acc=1]  \u001B[A\n",
      "Training:  34%|███▍      | 340/1000[, loss=0.00179, val_loss=0.00327, val_acc=1]\u001B[A\n",
      "Training:  34%|███▍      | 341/1000[, loss=0.00179, val_loss=0.00327, val_acc=1]\u001B[A\n",
      "Training:  34%|███▍      | 341/1000[, loss=0.00177, val_loss=0.00325, val_acc=1]\u001B[A\n",
      "Training:  34%|███▍      | 342/1000[, loss=0.00175, val_loss=0.00324, val_acc=1]\u001B[A\n",
      "Training:  34%|███▍      | 343/1000[, loss=0.00175, val_loss=0.00324, val_acc=1]\u001B[A\n",
      "Training:  34%|███▍      | 343/1000[, loss=0.00173, val_loss=0.00317, val_acc=1]\u001B[A\n",
      "Training:  34%|███▍      | 344/1000[, loss=0.00171, val_loss=0.00315, val_acc=1]\u001B[A\n",
      "Training:  34%|███▍      | 345/1000[, loss=0.00171, val_loss=0.00315, val_acc=1]\u001B[A\n",
      "Training:  34%|███▍      | 345/1000[, loss=0.00169, val_loss=0.00315, val_acc=1]\u001B[A\n",
      "Training:  35%|███▍      | 346/1000[, loss=0.00168, val_loss=0.0031, val_acc=1] \u001B[A\n",
      "Training:  35%|███▍      | 347/1000[, loss=0.00168, val_loss=0.0031, val_acc=1]\u001B[A\n",
      "Training:  35%|███▍      | 347/1000[, loss=0.00166, val_loss=0.00307, val_acc=1]\u001B[A\n",
      "Training:  35%|███▍      | 348/1000[, loss=0.00165, val_loss=0.00307, val_acc=1]\u001B[A\n",
      "Training:  35%|███▍      | 349/1000[, loss=0.00165, val_loss=0.00307, val_acc=1]\u001B[A\n",
      "Training:  35%|███▍      | 349/1000[, loss=0.00163, val_loss=0.00304, val_acc=1]\u001B[A\n",
      "Training:  35%|███▌      | 350/1000[, loss=0.00162, val_loss=0.00303, val_acc=1]\u001B[A\n",
      "Training:  35%|███▌      | 351/1000[, loss=0.00162, val_loss=0.00303, val_acc=1]\u001B[A\n",
      "Training:  35%|███▌      | 351/1000[, loss=0.0016, val_loss=0.00297, val_acc=1] \u001B[A\n",
      "Training:  35%|███▌      | 352/1000[, loss=0.00158, val_loss=0.00294, val_acc=1]\u001B[A\n",
      "Training:  35%|███▌      | 353/1000[, loss=0.00158, val_loss=0.00294, val_acc=1]\u001B[A\n",
      "Training:  35%|███▌      | 353/1000[, loss=0.00157, val_loss=0.0029, val_acc=1] \u001B[A\n",
      "Training:  35%|███▌      | 354/1000[, loss=0.00156, val_loss=0.00291, val_acc=1]\u001B[A\n",
      "Training:  36%|███▌      | 355/1000[, loss=0.00156, val_loss=0.00291, val_acc=1]\u001B[A\n",
      "Training:  36%|███▌      | 355/1000[, loss=0.00154, val_loss=0.00287, val_acc=1]\u001B[A\n",
      "Training:  36%|███▌      | 356/1000[, loss=0.00153, val_loss=0.00289, val_acc=1]\u001B[A\n",
      "Training:  36%|███▌      | 357/1000[, loss=0.00153, val_loss=0.00289, val_acc=1]\u001B[A\n",
      "Training:  36%|███▌      | 357/1000[, loss=0.00151, val_loss=0.00286, val_acc=1]\u001B[A\n",
      "Training:  36%|███▌      | 358/1000[, loss=0.0015, val_loss=0.00281, val_acc=1] \u001B[A\n",
      "Training:  36%|███▌      | 359/1000[, loss=0.0015, val_loss=0.00281, val_acc=1]\u001B[A\n",
      "Training:  36%|███▌      | 359/1000[, loss=0.00148, val_loss=0.00279, val_acc=1]\u001B[A\n",
      "Training:  36%|███▌      | 360/1000[, loss=0.00147, val_loss=0.00277, val_acc=1]\u001B[A\n",
      "Training:  36%|███▌      | 361/1000[, loss=0.00147, val_loss=0.00277, val_acc=1]\u001B[A\n",
      "Training:  36%|███▌      | 361/1000[, loss=0.00145, val_loss=0.00273, val_acc=1]\u001B[A\n",
      "Training:  36%|███▌      | 362/1000[, loss=0.00145, val_loss=0.00275, val_acc=1]\u001B[A\n",
      "Training:  36%|███▋      | 363/1000[, loss=0.00145, val_loss=0.00275, val_acc=1]\u001B[A\n",
      "Training:  36%|███▋      | 363/1000[, loss=0.00143, val_loss=0.00274, val_acc=1]\u001B[A\n",
      "Training:  36%|███▋      | 364/1000[, loss=0.00142, val_loss=0.00269, val_acc=1]\u001B[A\n",
      "Training:  36%|███▋      | 365/1000[, loss=0.00142, val_loss=0.00269, val_acc=1]\u001B[A\n",
      "Training:  36%|███▋      | 365/1000[, loss=0.0014, val_loss=0.00268, val_acc=1] \u001B[A\n",
      "Training:  37%|███▋      | 366/1000[, loss=0.00139, val_loss=0.00266, val_acc=1]\u001B[A\n",
      "Training:  37%|███▋      | 367/1000[, loss=0.00139, val_loss=0.00266, val_acc=1]\u001B[A\n",
      "Training:  37%|███▋      | 367/1000[, loss=0.00138, val_loss=0.00261, val_acc=1]\u001B[A\n",
      "Training:  37%|███▋      | 368/1000[, loss=0.00137, val_loss=0.00262, val_acc=1]\u001B[A\n",
      "Training:  37%|███▋      | 369/1000[, loss=0.00137, val_loss=0.00262, val_acc=1]\u001B[A\n",
      "Training:  37%|███▋      | 369/1000[, loss=0.00136, val_loss=0.0026, val_acc=1] \u001B[A\n",
      "Training:  37%|███▋      | 370/1000[, loss=0.00135, val_loss=0.00259, val_acc=1]\u001B[A\n",
      "Training:  37%|███▋      | 371/1000[, loss=0.00135, val_loss=0.00259, val_acc=1]\u001B[A\n",
      "Training:  37%|███▋      | 371/1000[, loss=0.00133, val_loss=0.00256, val_acc=1]\u001B[A\n",
      "Training:  37%|███▋      | 372/1000[, loss=0.00132, val_loss=0.00255, val_acc=1]\u001B[A\n",
      "Training:  37%|███▋      | 373/1000[, loss=0.00132, val_loss=0.00255, val_acc=1]\u001B[A\n",
      "Training:  37%|███▋      | 373/1000[, loss=0.00131, val_loss=0.00253, val_acc=1]\u001B[A\n",
      "Training:  37%|███▋      | 374/1000[, loss=0.0013, val_loss=0.00251, val_acc=1] \u001B[A\n",
      "Training:  38%|███▊      | 375/1000[, loss=0.0013, val_loss=0.00251, val_acc=1]\u001B[A\n",
      "Training:  38%|███▊      | 375/1000[, loss=0.00129, val_loss=0.00248, val_acc=1]\u001B[A\n",
      "Training:  38%|███▊      | 376/1000[, loss=0.00128, val_loss=0.00248, val_acc=1]\u001B[A\n",
      "Training:  38%|███▊      | 377/1000[, loss=0.00128, val_loss=0.00248, val_acc=1]\u001B[A\n",
      "Training:  38%|███▊      | 377/1000[, loss=0.00127, val_loss=0.00247, val_acc=1]\u001B[A\n",
      "Training:  38%|███▊      | 378/1000[, loss=0.00126, val_loss=0.00245, val_acc=1]\u001B[A\n",
      "Training:  38%|███▊      | 379/1000[, loss=0.00126, val_loss=0.00245, val_acc=1]\u001B[A\n",
      "Training:  38%|███▊      | 379/1000[, loss=0.00125, val_loss=0.00241, val_acc=1]\u001B[A\n",
      "Training:  38%|███▊      | 380/1000[, loss=0.00124, val_loss=0.00241, val_acc=1]\u001B[A\n",
      "Training:  38%|███▊      | 381/1000[, loss=0.00124, val_loss=0.00241, val_acc=1]\u001B[A\n",
      "Training:  38%|███▊      | 381/1000[, loss=0.00123, val_loss=0.00238, val_acc=1]\u001B[A\n",
      "Training:  38%|███▊      | 382/1000[, loss=0.00122, val_loss=0.00237, val_acc=1]\u001B[A\n",
      "Training:  38%|███▊      | 383/1000[, loss=0.00122, val_loss=0.00237, val_acc=1]\u001B[A\n",
      "Training:  38%|███▊      | 383/1000[, loss=0.00121, val_loss=0.00235, val_acc=1]\u001B[A\n",
      "Training:  38%|███▊      | 384/1000[, loss=0.0012, val_loss=0.00235, val_acc=1] \u001B[A\n",
      "Training:  38%|███▊      | 385/1000[, loss=0.0012, val_loss=0.00235, val_acc=1]\u001B[A\n",
      "Training:  38%|███▊      | 385/1000[, loss=0.00119, val_loss=0.00235, val_acc=1]\u001B[A\n",
      "Training:  39%|███▊      | 386/1000[, loss=0.00119, val_loss=0.00236, val_acc=1]\u001B[A\n",
      "Training:  39%|███▊      | 387/1000[, loss=0.00119, val_loss=0.00236, val_acc=1]\u001B[A\n",
      "Training:  39%|███▊      | 387/1000[, loss=0.00117, val_loss=0.00228, val_acc=1]\u001B[A\n",
      "Training:  39%|███▉      | 388/1000[, loss=0.00117, val_loss=0.00229, val_acc=1]\u001B[A\n",
      "Training:  39%|███▉      | 389/1000[, loss=0.00117, val_loss=0.00229, val_acc=1]\u001B[A\n",
      "Training:  39%|███▉      | 389/1000[, loss=0.00116, val_loss=0.00229, val_acc=1]\u001B[A\n",
      "Training:  39%|███▉      | 390/1000[, loss=0.00115, val_loss=0.00225, val_acc=1]\u001B[A\n",
      "Training:  39%|███▉      | 391/1000[, loss=0.00115, val_loss=0.00225, val_acc=1]\u001B[A\n",
      "Training:  39%|███▉      | 391/1000[, loss=0.00114, val_loss=0.00225, val_acc=1]\u001B[A\n",
      "Training:  39%|███▉      | 392/1000[, loss=0.00113, val_loss=0.00223, val_acc=1]\u001B[A\n",
      "Training:  39%|███▉      | 393/1000[, loss=0.00113, val_loss=0.00223, val_acc=1]\u001B[A\n",
      "Training:  39%|███▉      | 393/1000[, loss=0.00112, val_loss=0.00221, val_acc=1]\u001B[A\n",
      "Training:  39%|███▉      | 394/1000[, loss=0.00111, val_loss=0.00222, val_acc=1]\u001B[A\n",
      "Training:  40%|███▉      | 395/1000[, loss=0.00111, val_loss=0.00222, val_acc=1]\u001B[A\n",
      "Training:  40%|███▉      | 395/1000[, loss=0.00111, val_loss=0.0022, val_acc=1] \u001B[A\n",
      "Training:  40%|███▉      | 396/1000[, loss=0.0011, val_loss=0.0022, val_acc=1] \u001B[A\n",
      "Training:  40%|███▉      | 397/1000[, loss=0.0011, val_loss=0.0022, val_acc=1]\u001B[A\n",
      "Training:  40%|███▉      | 397/1000[, loss=0.00109, val_loss=0.00218, val_acc=1]\u001B[A\n",
      "Training:  40%|███▉      | 398/1000[, loss=0.00108, val_loss=0.00217, val_acc=1]\u001B[A\n",
      "Training:  40%|███▉      | 399/1000[, loss=0.00108, val_loss=0.00217, val_acc=1]\u001B[A\n",
      "Training:  40%|███▉      | 399/1000[, loss=0.00107, val_loss=0.00214, val_acc=1]\u001B[A\n",
      "Training:  40%|████      | 400/1000[, loss=0.00107, val_loss=0.00214, val_acc=1]\u001B[A\n",
      "Training:  40%|████      | 401/1000[, loss=0.00107, val_loss=0.00214, val_acc=1]\u001B[A\n",
      "Training:  40%|████      | 401/1000[, loss=0.00106, val_loss=0.00211, val_acc=1]\u001B[A\n",
      "Training:  40%|████      | 402/1000[, loss=0.00105, val_loss=0.00211, val_acc=1]\u001B[A\n",
      "Training:  40%|████      | 403/1000[, loss=0.00105, val_loss=0.00211, val_acc=1]\u001B[A\n",
      "Training:  40%|████      | 403/1000[, loss=0.00104, val_loss=0.00211, val_acc=1]\u001B[A\n",
      "Training:  40%|████      | 404/1000[, loss=0.00104, val_loss=0.0021, val_acc=1] \u001B[A\n",
      "Training:  40%|████      | 405/1000[, loss=0.00104, val_loss=0.0021, val_acc=1]\u001B[A\n",
      "Training:  40%|████      | 405/1000[, loss=0.00103, val_loss=0.00207, val_acc=1]\u001B[A\n",
      "Training:  41%|████      | 406/1000[, loss=0.00102, val_loss=0.00207, val_acc=1]\u001B[A\n",
      "Training:  41%|████      | 407/1000[, loss=0.00102, val_loss=0.00207, val_acc=1]\u001B[A\n",
      "Training:  41%|████      | 407/1000[, loss=0.00102, val_loss=0.00204, val_acc=1]\u001B[A\n",
      "Training:  41%|████      | 408/1000[, loss=0.00101, val_loss=0.00204, val_acc=1]\u001B[A\n",
      "Training:  41%|████      | 409/1000[, loss=0.00101, val_loss=0.00204, val_acc=1]\u001B[A\n",
      "Training:  41%|████      | 409/1000[, loss=0.001, val_loss=0.00202, val_acc=1]  \u001B[A\n",
      "Training:  41%|████      | 410/1000[, loss=0.000997, val_loss=0.00202, val_acc=1]\u001B[A\n",
      "Training:  41%|████      | 411/1000[, loss=0.000997, val_loss=0.00202, val_acc=1]\u001B[A\n",
      "Training:  41%|████      | 411/1000[, loss=0.000989, val_loss=0.00201, val_acc=1]\u001B[A\n",
      "Training:  41%|████      | 412/1000[, loss=0.000982, val_loss=0.00201, val_acc=1]\u001B[A\n",
      "Training:  41%|████▏     | 413/1000[, loss=0.000982, val_loss=0.00201, val_acc=1]\u001B[A\n",
      "Training:  41%|████▏     | 413/1000[, loss=0.000976, val_loss=0.00199, val_acc=1]\u001B[A\n",
      "Training:  41%|████▏     | 414/1000[, loss=0.000969, val_loss=0.00197, val_acc=1]\u001B[A\n",
      "Training:  42%|████▏     | 415/1000[, loss=0.000969, val_loss=0.00197, val_acc=1]\u001B[A\n",
      "Training:  42%|████▏     | 415/1000[, loss=0.000962, val_loss=0.00197, val_acc=1]\u001B[A\n",
      "Training:  42%|████▏     | 416/1000[, loss=0.00096, val_loss=0.00197, val_acc=1] \u001B[A\n",
      "Training:  42%|████▏     | 417/1000[, loss=0.00096, val_loss=0.00197, val_acc=1]\u001B[A\n",
      "Training:  42%|████▏     | 417/1000[, loss=0.00095, val_loss=0.00195, val_acc=1]\u001B[A\n",
      "Training:  42%|████▏     | 418/1000[, loss=0.000944, val_loss=0.00195, val_acc=1]\u001B[A\n",
      "Training:  42%|████▏     | 419/1000[, loss=0.000944, val_loss=0.00195, val_acc=1]\u001B[A\n",
      "Training:  42%|████▏     | 419/1000[, loss=0.000939, val_loss=0.00191, val_acc=1]\u001B[A\n",
      "Training:  42%|████▏     | 420/1000[, loss=0.000934, val_loss=0.00193, val_acc=1]\u001B[A\n",
      "Training:  42%|████▏     | 421/1000[, loss=0.000934, val_loss=0.00193, val_acc=1]\u001B[A\n",
      "Training:  42%|████▏     | 421/1000[, loss=0.000926, val_loss=0.00191, val_acc=1]\u001B[A\n",
      "Training:  42%|████▏     | 422/1000[, loss=0.000921, val_loss=0.00191, val_acc=1]\u001B[A\n",
      "Training:  42%|████▏     | 423/1000[, loss=0.000921, val_loss=0.00191, val_acc=1]\u001B[A\n",
      "Training:  42%|████▏     | 423/1000[, loss=0.000915, val_loss=0.0019, val_acc=1] \u001B[A\n",
      "Training:  42%|████▏     | 424/1000[, loss=0.000909, val_loss=0.00187, val_acc=1]\u001B[A\n",
      "Training:  42%|████▎     | 425/1000[, loss=0.000909, val_loss=0.00187, val_acc=1]\u001B[A\n",
      "Training:  42%|████▎     | 425/1000[, loss=0.000903, val_loss=0.00186, val_acc=1]\u001B[A\n",
      "Training:  43%|████▎     | 426/1000[, loss=0.000899, val_loss=0.00186, val_acc=1]\u001B[A\n",
      "Training:  43%|████▎     | 427/1000[, loss=0.000899, val_loss=0.00186, val_acc=1]\u001B[A\n",
      "Training:  43%|████▎     | 427/1000[, loss=0.000893, val_loss=0.00185, val_acc=1]\u001B[A\n",
      "Training:  43%|████▎     | 428/1000[, loss=0.000887, val_loss=0.00185, val_acc=1]\u001B[A\n",
      "Training:  43%|████▎     | 429/1000[, loss=0.000887, val_loss=0.00185, val_acc=1]\u001B[A\n",
      "Training:  43%|████▎     | 429/1000[, loss=0.000881, val_loss=0.00185, val_acc=1]\u001B[A\n",
      "Training:  43%|████▎     | 430/1000[, loss=0.000879, val_loss=0.00185, val_acc=1]\u001B[A\n",
      "Training:  43%|████▎     | 431/1000[, loss=0.000879, val_loss=0.00185, val_acc=1]\u001B[A\n",
      "Training:  43%|████▎     | 431/1000[, loss=0.000871, val_loss=0.00182, val_acc=1]\u001B[A\n",
      "Training:  43%|████▎     | 432/1000[, loss=0.000866, val_loss=0.00181, val_acc=1]\u001B[A\n",
      "Training:  43%|████▎     | 433/1000[, loss=0.000866, val_loss=0.00181, val_acc=1]\u001B[A\n",
      "Training:  43%|████▎     | 433/1000[, loss=0.00086, val_loss=0.0018, val_acc=1]  \u001B[A\n",
      "Training:  43%|████▎     | 434/1000[, loss=0.000855, val_loss=0.00179, val_acc=1]\u001B[A\n",
      "Training:  44%|████▎     | 435/1000[, loss=0.000855, val_loss=0.00179, val_acc=1]\u001B[A\n",
      "Training:  44%|████▎     | 435/1000[, loss=0.00085, val_loss=0.00176, val_acc=1] \u001B[A\n",
      "Training:  44%|████▎     | 436/1000[, loss=0.000847, val_loss=0.00178, val_acc=1]\u001B[A\n",
      "Training:  44%|████▎     | 437/1000[, loss=0.000847, val_loss=0.00178, val_acc=1]\u001B[A\n",
      "Training:  44%|████▎     | 437/1000[, loss=0.00084, val_loss=0.00178, val_acc=1] \u001B[A\n",
      "Training:  44%|████▍     | 438/1000[, loss=0.000836, val_loss=0.00177, val_acc=1]\u001B[A\n",
      "Training:  44%|████▍     | 439/1000[, loss=0.000836, val_loss=0.00177, val_acc=1]\u001B[A\n",
      "Training:  44%|████▍     | 439/1000[, loss=0.00083, val_loss=0.00174, val_acc=1] \u001B[A\n",
      "Training:  44%|████▍     | 440/1000[, loss=0.000827, val_loss=0.00174, val_acc=1]\u001B[A\n",
      "Training:  44%|████▍     | 441/1000[, loss=0.000827, val_loss=0.00174, val_acc=1]\u001B[A\n",
      "Training:  44%|████▍     | 441/1000[, loss=0.00082, val_loss=0.00172, val_acc=1] \u001B[A\n",
      "Training:  44%|████▍     | 442/1000[, loss=0.000817, val_loss=0.00173, val_acc=1]\u001B[A\n",
      "Training:  44%|████▍     | 443/1000[, loss=0.000817, val_loss=0.00173, val_acc=1]\u001B[A\n",
      "Training:  44%|████▍     | 443/1000[, loss=0.000812, val_loss=0.00173, val_acc=1]\u001B[A\n",
      "Training:  44%|████▍     | 444/1000[, loss=0.000808, val_loss=0.00173, val_acc=1]\u001B[A\n",
      "Training:  44%|████▍     | 445/1000[, loss=0.000808, val_loss=0.00173, val_acc=1]\u001B[A\n",
      "Training:  44%|████▍     | 445/1000[, loss=0.000802, val_loss=0.0017, val_acc=1] \u001B[A\n",
      "Training:  45%|████▍     | 446/1000[, loss=0.000798, val_loss=0.0017, val_acc=1]\u001B[A\n",
      "Training:  45%|████▍     | 447/1000[, loss=0.000798, val_loss=0.0017, val_acc=1]\u001B[A\n",
      "Training:  45%|████▍     | 447/1000[, loss=0.000793, val_loss=0.00169, val_acc=1]\u001B[A\n",
      "Training:  45%|████▍     | 448/1000[, loss=0.00079, val_loss=0.0017, val_acc=1]  \u001B[A\n",
      "Training:  45%|████▍     | 449/1000[, loss=0.00079, val_loss=0.0017, val_acc=1]\u001B[A\n",
      "Training:  45%|████▍     | 449/1000[, loss=0.000783, val_loss=0.00167, val_acc=1]\u001B[A\n",
      "Training:  45%|████▌     | 450/1000[, loss=0.000781, val_loss=0.00167, val_acc=1]\u001B[A\n",
      "Training:  45%|████▌     | 451/1000[, loss=0.000781, val_loss=0.00167, val_acc=1]\u001B[A\n",
      "Training:  45%|████▌     | 451/1000[, loss=0.000776, val_loss=0.00167, val_acc=1]\u001B[A\n",
      "Training:  45%|████▌     | 452/1000[, loss=0.000771, val_loss=0.00164, val_acc=1]\u001B[A\n",
      "Training:  45%|████▌     | 453/1000[, loss=0.000771, val_loss=0.00164, val_acc=1]\u001B[A\n",
      "Training:  45%|████▌     | 453/1000[, loss=0.000767, val_loss=0.00165, val_acc=1]\u001B[A\n",
      "Training:  45%|████▌     | 454/1000[, loss=0.000764, val_loss=0.00165, val_acc=1]\u001B[A\n",
      "Training:  46%|████▌     | 455/1000[, loss=0.000764, val_loss=0.00165, val_acc=1]\u001B[A\n",
      "Training:  46%|████▌     | 455/1000[, loss=0.000759, val_loss=0.00163, val_acc=1]\u001B[A\n",
      "Training:  46%|████▌     | 456/1000[, loss=0.000755, val_loss=0.00162, val_acc=1]\u001B[A\n",
      "Training:  46%|████▌     | 457/1000[, loss=0.000755, val_loss=0.00162, val_acc=1]\u001B[A\n",
      "Training:  46%|████▌     | 457/1000[, loss=0.000751, val_loss=0.00162, val_acc=1]\u001B[A\n",
      "Training:  46%|████▌     | 458/1000[, loss=0.000747, val_loss=0.00161, val_acc=1]\u001B[A\n",
      "Training:  46%|████▌     | 459/1000[, loss=0.000747, val_loss=0.00161, val_acc=1]\u001B[A\n",
      "Training:  46%|████▌     | 459/1000[, loss=0.000742, val_loss=0.00161, val_acc=1]\u001B[A\n",
      "Training:  46%|████▌     | 460/1000[, loss=0.00074, val_loss=0.0016, val_acc=1]  \u001B[A\n",
      "Training:  46%|████▌     | 461/1000[, loss=0.00074, val_loss=0.0016, val_acc=1]\u001B[A\n",
      "Training:  46%|████▌     | 461/1000[, loss=0.000735, val_loss=0.00159, val_acc=1]\u001B[A\n",
      "Training:  46%|████▌     | 462/1000[, loss=0.000731, val_loss=0.00159, val_acc=1]\u001B[A\n",
      "Training:  46%|████▋     | 463/1000[, loss=0.000731, val_loss=0.00159, val_acc=1]\u001B[A\n",
      "Training:  46%|████▋     | 463/1000[, loss=0.000726, val_loss=0.00157, val_acc=1]\u001B[A\n",
      "Training:  46%|████▋     | 464/1000[, loss=0.000723, val_loss=0.00157, val_acc=1]\u001B[A\n",
      "Training:  46%|████▋     | 465/1000[, loss=0.000723, val_loss=0.00157, val_acc=1]\u001B[A\n",
      "Training:  46%|████▋     | 465/1000[, loss=0.000719, val_loss=0.00158, val_acc=1]\u001B[A\n",
      "Training:  47%|████▋     | 466/1000[, loss=0.000716, val_loss=0.00156, val_acc=1]\u001B[A\n",
      "Training:  47%|████▋     | 467/1000[, loss=0.000716, val_loss=0.00156, val_acc=1]\u001B[A\n",
      "Training:  47%|████▋     | 467/1000[, loss=0.000711, val_loss=0.00155, val_acc=1]\u001B[A\n",
      "Training:  47%|████▋     | 468/1000[, loss=0.000709, val_loss=0.00155, val_acc=1]\u001B[A\n",
      "Training:  47%|████▋     | 469/1000[, loss=0.000709, val_loss=0.00155, val_acc=1]\u001B[A\n",
      "Training:  47%|████▋     | 469/1000[, loss=0.000705, val_loss=0.00154, val_acc=1]\u001B[A\n",
      "Training:  47%|████▋     | 470/1000[, loss=0.000701, val_loss=0.00153, val_acc=1]\u001B[A\n",
      "Training:  47%|████▋     | 471/1000[, loss=0.000701, val_loss=0.00153, val_acc=1]\u001B[A\n",
      "Training:  47%|████▋     | 471/1000[, loss=0.000697, val_loss=0.00153, val_acc=1]\u001B[A\n",
      "Training:  47%|████▋     | 472/1000[, loss=0.000695, val_loss=0.00152, val_acc=1]\u001B[A\n",
      "Training:  47%|████▋     | 473/1000[, loss=0.000695, val_loss=0.00152, val_acc=1]\u001B[A\n",
      "Training:  47%|████▋     | 473/1000[, loss=0.00069, val_loss=0.00151, val_acc=1] \u001B[A\n",
      "Training:  47%|████▋     | 474/1000[, loss=0.000686, val_loss=0.00151, val_acc=1]\u001B[A\n",
      "Training:  48%|████▊     | 475/1000[, loss=0.000686, val_loss=0.00151, val_acc=1]\u001B[A\n",
      "Training:  48%|████▊     | 475/1000[, loss=0.000685, val_loss=0.0015, val_acc=1] \u001B[A\n",
      "Training:  48%|████▊     | 476/1000[, loss=0.00068, val_loss=0.00149, val_acc=1]\u001B[A\n",
      "Training:  48%|████▊     | 477/1000[, loss=0.00068, val_loss=0.00149, val_acc=1]\u001B[A\n",
      "Training:  48%|████▊     | 477/1000[, loss=0.000677, val_loss=0.0015, val_acc=1]\u001B[A\n",
      "Training:  48%|████▊     | 478/1000[, loss=0.000674, val_loss=0.00148, val_acc=1]\u001B[A\n",
      "Training:  48%|████▊     | 479/1000[, loss=0.000674, val_loss=0.00148, val_acc=1]\u001B[A\n",
      "Training:  48%|████▊     | 479/1000[, loss=0.000671, val_loss=0.00149, val_acc=1]\u001B[A\n",
      "Training:  48%|████▊     | 480/1000[, loss=0.000667, val_loss=0.00148, val_acc=1]\u001B[A\n",
      "Training:  48%|████▊     | 481/1000[, loss=0.000667, val_loss=0.00148, val_acc=1]\u001B[A\n",
      "Training:  48%|████▊     | 481/1000[, loss=0.000664, val_loss=0.00147, val_acc=1]\u001B[A\n",
      "Training:  48%|████▊     | 482/1000[, loss=0.00066, val_loss=0.00146, val_acc=1] \u001B[A\n",
      "Training:  48%|████▊     | 483/1000[, loss=0.00066, val_loss=0.00146, val_acc=1]\u001B[A\n",
      "Training:  48%|████▊     | 483/1000[, loss=0.000658, val_loss=0.00147, val_acc=1]\u001B[A\n",
      "Training:  48%|████▊     | 484/1000[, loss=0.000653, val_loss=0.00145, val_acc=1]\u001B[A\n",
      "Training:  48%|████▊     | 485/1000[, loss=0.000653, val_loss=0.00145, val_acc=1]\u001B[A\n",
      "Training:  48%|████▊     | 485/1000[, loss=0.000651, val_loss=0.00145, val_acc=1]\u001B[A\n",
      "Training:  49%|████▊     | 486/1000[, loss=0.000648, val_loss=0.00145, val_acc=1]\u001B[A\n",
      "Training:  49%|████▊     | 487/1000[, loss=0.000648, val_loss=0.00145, val_acc=1]\u001B[A\n",
      "Training:  49%|████▊     | 487/1000[, loss=0.000644, val_loss=0.00143, val_acc=1]\u001B[A\n",
      "Training:  49%|████▉     | 488/1000[, loss=0.000642, val_loss=0.00143, val_acc=1]\u001B[A\n",
      "Training:  49%|████▉     | 489/1000[, loss=0.000642, val_loss=0.00143, val_acc=1]\u001B[A\n",
      "Training:  49%|████▉     | 489/1000[, loss=0.000638, val_loss=0.00143, val_acc=1]\u001B[A\n",
      "Training:  49%|████▉     | 490/1000[, loss=0.000637, val_loss=0.00142, val_acc=1]\u001B[A\n",
      "Training:  49%|████▉     | 491/1000[, loss=0.000637, val_loss=0.00142, val_acc=1]\u001B[A\n",
      "Training:  49%|████▉     | 491/1000[, loss=0.000632, val_loss=0.00142, val_acc=1]\u001B[A\n",
      "Training:  49%|████▉     | 492/1000[, loss=0.000629, val_loss=0.00142, val_acc=1]\u001B[A\n",
      "Training:  49%|████▉     | 493/1000[, loss=0.000629, val_loss=0.00142, val_acc=1]\u001B[A\n",
      "Training:  49%|████▉     | 493/1000[, loss=0.000627, val_loss=0.00141, val_acc=1]\u001B[A\n",
      "Training:  49%|████▉     | 494/1000[, loss=0.000623, val_loss=0.00139, val_acc=1]\u001B[A\n",
      "Training:  50%|████▉     | 495/1000[, loss=0.000623, val_loss=0.00139, val_acc=1]\u001B[A\n",
      "Training:  50%|████▉     | 495/1000[, loss=0.000621, val_loss=0.0014, val_acc=1] \u001B[A\n",
      "Training:  50%|████▉     | 496/1000[, loss=0.000618, val_loss=0.00139, val_acc=1]\u001B[A\n",
      "Training:  50%|████▉     | 497/1000[, loss=0.000618, val_loss=0.00139, val_acc=1]\u001B[A\n",
      "Training:  50%|████▉     | 497/1000[, loss=0.000615, val_loss=0.00139, val_acc=1]\u001B[A\n",
      "Training:  50%|████▉     | 498/1000[, loss=0.000612, val_loss=0.00138, val_acc=1]\u001B[A\n",
      "Training:  50%|████▉     | 499/1000[, loss=0.000612, val_loss=0.00138, val_acc=1]\u001B[A\n",
      "Training:  50%|████▉     | 499/1000[, loss=0.00061, val_loss=0.00138, val_acc=1] \u001B[A\n",
      "Training:  50%|█████     | 500/1000[, loss=0.000607, val_loss=0.00137, val_acc=1]\u001B[A\n",
      "Training:  50%|█████     | 501/1000[, loss=0.000607, val_loss=0.00137, val_acc=1]\u001B[A\n",
      "Training:  50%|█████     | 501/1000[, loss=0.000604, val_loss=0.00137, val_acc=1]\u001B[A\n",
      "Training:  50%|█████     | 502/1000[, loss=0.000601, val_loss=0.00136, val_acc=1]\u001B[A\n",
      "Training:  50%|█████     | 503/1000[, loss=0.000601, val_loss=0.00136, val_acc=1]\u001B[A\n",
      "Training:  50%|█████     | 503/1000[, loss=0.000599, val_loss=0.00136, val_acc=1]\u001B[A\n",
      "Training:  50%|█████     | 504/1000[, loss=0.000596, val_loss=0.00136, val_acc=1]\u001B[A\n",
      "Training:  50%|█████     | 505/1000[, loss=0.000596, val_loss=0.00136, val_acc=1]\u001B[A\n",
      "Training:  50%|█████     | 505/1000[, loss=0.000593, val_loss=0.00135, val_acc=1]\u001B[A\n",
      "Training:  51%|█████     | 506/1000[, loss=0.000591, val_loss=0.00134, val_acc=1]\u001B[A\n",
      "Training:  51%|█████     | 507/1000[, loss=0.000591, val_loss=0.00134, val_acc=1]\u001B[A\n",
      "Training:  51%|█████     | 507/1000[, loss=0.000588, val_loss=0.00134, val_acc=1]\u001B[A\n",
      "Training:  51%|█████     | 508/1000[, loss=0.000586, val_loss=0.00135, val_acc=1]\u001B[A\n",
      "Training:  51%|█████     | 509/1000[, loss=0.000586, val_loss=0.00135, val_acc=1]\u001B[A\n",
      "Training:  51%|█████     | 509/1000[, loss=0.000582, val_loss=0.00133, val_acc=1]\u001B[A\n",
      "Training:  51%|█████     | 510/1000[, loss=0.00058, val_loss=0.00132, val_acc=1] \u001B[A\n",
      "Training:  51%|█████     | 511/1000[, loss=0.00058, val_loss=0.00132, val_acc=1]\u001B[A\n",
      "Training:  51%|█████     | 511/1000[, loss=0.000578, val_loss=0.00134, val_acc=1]\u001B[A\n",
      "Training:  51%|█████     | 512/1000[, loss=0.000575, val_loss=0.00131, val_acc=1]\u001B[A\n",
      "Training:  51%|█████▏    | 513/1000[, loss=0.000575, val_loss=0.00131, val_acc=1]\u001B[A\n",
      "Training:  51%|█████▏    | 513/1000[, loss=0.000573, val_loss=0.00132, val_acc=1]\u001B[A\n",
      "Training:  51%|█████▏    | 514/1000[, loss=0.00057, val_loss=0.00132, val_acc=1] \u001B[A\n",
      "Training:  52%|█████▏    | 515/1000[, loss=0.00057, val_loss=0.00132, val_acc=1]\u001B[A\n",
      "Training:  52%|█████▏    | 515/1000[, loss=0.000568, val_loss=0.00131, val_acc=1]\u001B[A\n",
      "Training:  52%|█████▏    | 516/1000[, loss=0.000565, val_loss=0.0013, val_acc=1] \u001B[A\n",
      "Training:  52%|█████▏    | 517/1000[, loss=0.000565, val_loss=0.0013, val_acc=1]\u001B[A\n",
      "Training:  52%|█████▏    | 517/1000[, loss=0.000563, val_loss=0.0013, val_acc=1]\u001B[A\n",
      "Training:  52%|█████▏    | 518/1000[, loss=0.00056, val_loss=0.00129, val_acc=1]\u001B[A\n",
      "Training:  52%|█████▏    | 519/1000[, loss=0.00056, val_loss=0.00129, val_acc=1]\u001B[A\n",
      "Training:  52%|█████▏    | 519/1000[, loss=0.000558, val_loss=0.00131, val_acc=1]\u001B[A\n",
      "Training:  52%|█████▏    | 520/1000[, loss=0.000556, val_loss=0.00129, val_acc=1]\u001B[A\n",
      "Training:  52%|█████▏    | 521/1000[, loss=0.000556, val_loss=0.00129, val_acc=1]\u001B[A\n",
      "Training:  52%|█████▏    | 521/1000[, loss=0.000554, val_loss=0.00129, val_acc=1]\u001B[A\n",
      "Training:  52%|█████▏    | 522/1000[, loss=0.000551, val_loss=0.00127, val_acc=1]\u001B[A\n",
      "Training:  52%|█████▏    | 523/1000[, loss=0.000551, val_loss=0.00127, val_acc=1]\u001B[A\n",
      "Training:  52%|█████▏    | 523/1000[, loss=0.000549, val_loss=0.00128, val_acc=1]\u001B[A\n",
      "Training:  52%|█████▏    | 524/1000[, loss=0.000546, val_loss=0.00128, val_acc=1]\u001B[A\n",
      "Training:  52%|█████▎    | 525/1000[, loss=0.000546, val_loss=0.00128, val_acc=1]\u001B[A\n",
      "Training:  52%|█████▎    | 525/1000[, loss=0.000544, val_loss=0.00127, val_acc=1]\u001B[A\n",
      "Training:  53%|█████▎    | 526/1000[, loss=0.000542, val_loss=0.00126, val_acc=1]\u001B[A\n",
      "Training:  53%|█████▎    | 527/1000[, loss=0.000542, val_loss=0.00126, val_acc=1]\u001B[A\n",
      "Training:  53%|█████▎    | 527/1000[, loss=0.00054, val_loss=0.00127, val_acc=1] \u001B[A\n",
      "Training:  53%|█████▎    | 528/1000[, loss=0.000537, val_loss=0.00126, val_acc=1]\u001B[A\n",
      "Training:  53%|█████▎    | 529/1000[, loss=0.000537, val_loss=0.00126, val_acc=1]\u001B[A\n",
      "Training:  53%|█████▎    | 529/1000[, loss=0.000535, val_loss=0.00126, val_acc=1]\u001B[A\n",
      "Training:  53%|█████▎    | 530/1000[, loss=0.000533, val_loss=0.00125, val_acc=1]\u001B[A\n",
      "Training:  53%|█████▎    | 531/1000[, loss=0.000533, val_loss=0.00125, val_acc=1]\u001B[A\n",
      "Training:  53%|█████▎    | 531/1000[, loss=0.000531, val_loss=0.00125, val_acc=1]\u001B[A\n",
      "Training:  53%|█████▎    | 532/1000[, loss=0.000529, val_loss=0.00125, val_acc=1]\u001B[A\n",
      "Training:  53%|█████▎    | 533/1000[, loss=0.000529, val_loss=0.00125, val_acc=1]\u001B[A\n",
      "Training:  53%|█████▎    | 533/1000[, loss=0.000526, val_loss=0.00124, val_acc=1]\u001B[A\n",
      "Training:  53%|█████▎    | 534/1000[, loss=0.000525, val_loss=0.00124, val_acc=1]\u001B[A\n",
      "Training:  54%|█████▎    | 535/1000[, loss=0.000525, val_loss=0.00124, val_acc=1]\u001B[A\n",
      "Training:  54%|█████▎    | 535/1000[, loss=0.000522, val_loss=0.00123, val_acc=1]\u001B[A\n",
      "Training:  54%|█████▎    | 536/1000[, loss=0.000521, val_loss=0.00123, val_acc=1]\u001B[A\n",
      "Training:  54%|█████▎    | 537/1000[, loss=0.000521, val_loss=0.00123, val_acc=1]\u001B[A\n",
      "Training:  54%|█████▎    | 537/1000[, loss=0.000518, val_loss=0.00122, val_acc=1]\u001B[A\n",
      "Training:  54%|█████▍    | 538/1000[, loss=0.000516, val_loss=0.00122, val_acc=1]\u001B[A\n",
      "Training:  54%|█████▍    | 539/1000[, loss=0.000516, val_loss=0.00122, val_acc=1]\u001B[A\n",
      "Training:  54%|█████▍    | 539/1000[, loss=0.000515, val_loss=0.00122, val_acc=1]\u001B[A\n",
      "Training:  54%|█████▍    | 540/1000[, loss=0.000512, val_loss=0.00122, val_acc=1]\u001B[A\n",
      "Training:  54%|█████▍    | 541/1000[, loss=0.000512, val_loss=0.00122, val_acc=1]\u001B[A\n",
      "Training:  54%|█████▍    | 541/1000[, loss=0.00051, val_loss=0.00121, val_acc=1] \u001B[A\n",
      "Training:  54%|█████▍    | 542/1000[, loss=0.000508, val_loss=0.0012, val_acc=1]\u001B[A\n",
      "Training:  54%|█████▍    | 543/1000[, loss=0.000508, val_loss=0.0012, val_acc=1]\u001B[A\n",
      "Training:  54%|█████▍    | 543/1000[, loss=0.000506, val_loss=0.00121, val_acc=1]\u001B[A\n",
      "Training:  54%|█████▍    | 544/1000[, loss=0.000504, val_loss=0.00121, val_acc=1]\u001B[A\n",
      "Training:  55%|█████▍    | 545/1000[, loss=0.000504, val_loss=0.00121, val_acc=1]\u001B[A\n",
      "Training:  55%|█████▍    | 545/1000[, loss=0.000502, val_loss=0.00119, val_acc=1]\u001B[A\n",
      "Training:  55%|█████▍    | 546/1000[, loss=0.0005, val_loss=0.0012, val_acc=1]   \u001B[A\n",
      "Training:  55%|█████▍    | 547/1000[, loss=0.0005, val_loss=0.0012, val_acc=1]\u001B[A\n",
      "Training:  55%|█████▍    | 547/1000[, loss=0.000498, val_loss=0.0012, val_acc=1]\u001B[A\n",
      "Training:  55%|█████▍    | 548/1000[, loss=0.000496, val_loss=0.00118, val_acc=1]\u001B[A\n",
      "Training:  55%|█████▍    | 549/1000[, loss=0.000496, val_loss=0.00118, val_acc=1]\u001B[A\n",
      "Training:  55%|█████▍    | 549/1000[, loss=0.000494, val_loss=0.00118, val_acc=1]\u001B[A\n",
      "Training:  55%|█████▌    | 550/1000[, loss=0.000492, val_loss=0.00117, val_acc=1]\u001B[A\n",
      "Training:  55%|█████▌    | 551/1000[, loss=0.000492, val_loss=0.00117, val_acc=1]\u001B[A\n",
      "Training:  55%|█████▌    | 551/1000[, loss=0.00049, val_loss=0.00118, val_acc=1] \u001B[A\n",
      "Training:  55%|█████▌    | 552/1000[, loss=0.000488, val_loss=0.00117, val_acc=1]\u001B[A\n",
      "Training:  55%|█████▌    | 553/1000[, loss=0.000488, val_loss=0.00117, val_acc=1]\u001B[A\n",
      "Training:  55%|█████▌    | 553/1000[, loss=0.000487, val_loss=0.00117, val_acc=1]\u001B[A\n",
      "Training:  55%|█████▌    | 554/1000[, loss=0.000484, val_loss=0.00117, val_acc=1]\u001B[A\n",
      "Training:  56%|█████▌    | 555/1000[, loss=0.000484, val_loss=0.00117, val_acc=1]\u001B[A\n",
      "Training:  56%|█████▌    | 555/1000[, loss=0.000483, val_loss=0.00116, val_acc=1]\u001B[A\n",
      "Training:  56%|█████▌    | 556/1000[, loss=0.000481, val_loss=0.00116, val_acc=1]\u001B[A\n",
      "Training:  56%|█████▌    | 557/1000[, loss=0.000481, val_loss=0.00116, val_acc=1]\u001B[A\n",
      "Training:  56%|█████▌    | 557/1000[, loss=0.00048, val_loss=0.00115, val_acc=1] \u001B[A\n",
      "Training:  56%|█████▌    | 558/1000[, loss=0.000477, val_loss=0.00115, val_acc=1]\u001B[A\n",
      "Training:  56%|█████▌    | 559/1000[, loss=0.000477, val_loss=0.00115, val_acc=1]\u001B[A\n",
      "Training:  56%|█████▌    | 559/1000[, loss=0.000476, val_loss=0.00117, val_acc=1]\u001B[A\n",
      "Training:  56%|█████▌    | 560/1000[, loss=0.000474, val_loss=0.00114, val_acc=1]\u001B[A\n",
      "Training:  56%|█████▌    | 561/1000[, loss=0.000474, val_loss=0.00114, val_acc=1]\u001B[A\n",
      "Training:  56%|█████▌    | 561/1000[, loss=0.000472, val_loss=0.00115, val_acc=1]\u001B[A\n",
      "Training:  56%|█████▌    | 562/1000[, loss=0.00047, val_loss=0.00114, val_acc=1] \u001B[A\n",
      "Training:  56%|█████▋    | 563/1000[, loss=0.00047, val_loss=0.00114, val_acc=1]\u001B[A\n",
      "Training:  56%|█████▋    | 563/1000[, loss=0.000468, val_loss=0.00114, val_acc=1]\u001B[A\n",
      "Training:  56%|█████▋    | 564/1000[, loss=0.000467, val_loss=0.00114, val_acc=1]\u001B[A\n",
      "Training:  56%|█████▋    | 565/1000[, loss=0.000467, val_loss=0.00114, val_acc=1]\u001B[A\n",
      "Training:  56%|█████▋    | 565/1000[, loss=0.000465, val_loss=0.00113, val_acc=1]\u001B[A\n",
      "Training:  57%|█████▋    | 566/1000[, loss=0.000463, val_loss=0.00113, val_acc=1]\u001B[A\n",
      "Training:  57%|█████▋    | 567/1000[, loss=0.000463, val_loss=0.00113, val_acc=1]\u001B[A\n",
      "Training:  57%|█████▋    | 567/1000[, loss=0.000462, val_loss=0.00113, val_acc=1]\u001B[A\n",
      "Training:  57%|█████▋    | 568/1000[, loss=0.00046, val_loss=0.00113, val_acc=1] \u001B[A\n",
      "Training:  57%|█████▋    | 569/1000[, loss=0.00046, val_loss=0.00113, val_acc=1]\u001B[A\n",
      "Training:  57%|█████▋    | 569/1000[, loss=0.000458, val_loss=0.00113, val_acc=1]\u001B[A\n",
      "Training:  57%|█████▋    | 570/1000[, loss=0.000457, val_loss=0.00112, val_acc=1]\u001B[A\n",
      "Training:  57%|█████▋    | 571/1000[, loss=0.000457, val_loss=0.00112, val_acc=1]\u001B[A\n",
      "Training:  57%|█████▋    | 571/1000[, loss=0.000455, val_loss=0.00112, val_acc=1]\u001B[A\n",
      "Training:  57%|█████▋    | 572/1000[, loss=0.000453, val_loss=0.00111, val_acc=1]\u001B[A\n",
      "Training:  57%|█████▋    | 573/1000[, loss=0.000453, val_loss=0.00111, val_acc=1]\u001B[A\n",
      "Training:  57%|█████▋    | 573/1000[, loss=0.000452, val_loss=0.00111, val_acc=1]\u001B[A\n",
      "Training:  57%|█████▋    | 574/1000[, loss=0.00045, val_loss=0.00112, val_acc=1] \u001B[A\n",
      "Training:  57%|█████▊    | 575/1000[, loss=0.00045, val_loss=0.00112, val_acc=1]\u001B[A\n",
      "Training:  57%|█████▊    | 575/1000[, loss=0.000448, val_loss=0.00111, val_acc=1]\u001B[A\n",
      "Training:  58%|█████▊    | 576/1000[, loss=0.000447, val_loss=0.00111, val_acc=1]\u001B[A\n",
      "Training:  58%|█████▊    | 577/1000[, loss=0.000447, val_loss=0.00111, val_acc=1]\u001B[A\n",
      "Training:  58%|█████▊    | 577/1000[, loss=0.000445, val_loss=0.0011, val_acc=1] \u001B[A\n",
      "Training:  58%|█████▊    | 578/1000[, loss=0.000444, val_loss=0.0011, val_acc=1]\u001B[A\n",
      "Training:  58%|█████▊    | 579/1000[, loss=0.000444, val_loss=0.0011, val_acc=1]\u001B[A\n",
      "Training:  58%|█████▊    | 579/1000[, loss=0.000442, val_loss=0.00111, val_acc=1]\u001B[A\n",
      "Training:  58%|█████▊    | 580/1000[, loss=0.00044, val_loss=0.0011, val_acc=1]  \u001B[A\n",
      "Training:  58%|█████▊    | 581/1000[, loss=0.00044, val_loss=0.0011, val_acc=1]\u001B[A\n",
      "Training:  58%|█████▊    | 581/1000[, loss=0.000439, val_loss=0.00109, val_acc=1]\u001B[A\n",
      "Training:  58%|█████▊    | 582/1000[, loss=0.000437, val_loss=0.00109, val_acc=1]\u001B[A\n",
      "Training:  58%|█████▊    | 583/1000[, loss=0.000437, val_loss=0.00109, val_acc=1]\u001B[A\n",
      "Training:  58%|█████▊    | 583/1000[, loss=0.000436, val_loss=0.0011, val_acc=1] \u001B[A\n",
      "Training:  58%|█████▊    | 584/1000[, loss=0.000434, val_loss=0.00109, val_acc=1]\u001B[A\n",
      "Training:  58%|█████▊    | 585/1000[, loss=0.000434, val_loss=0.00109, val_acc=1]\u001B[A\n",
      "Training:  58%|█████▊    | 585/1000[, loss=0.000433, val_loss=0.00109, val_acc=1]\u001B[A\n",
      "Training:  59%|█████▊    | 586/1000[, loss=0.000431, val_loss=0.00108, val_acc=1]\u001B[A\n",
      "Training:  59%|█████▊    | 587/1000[, loss=0.000431, val_loss=0.00108, val_acc=1]\u001B[A\n",
      "Training:  59%|█████▊    | 587/1000[, loss=0.00043, val_loss=0.00108, val_acc=1] \u001B[A\n",
      "Training:  59%|█████▉    | 588/1000[, loss=0.000428, val_loss=0.00108, val_acc=1]\u001B[A\n",
      "Training:  59%|█████▉    | 589/1000[, loss=0.000428, val_loss=0.00108, val_acc=1]\u001B[A\n",
      "Training:  59%|█████▉    | 589/1000[, loss=0.000427, val_loss=0.00108, val_acc=1]\u001B[A\n",
      "Training:  59%|█████▉    | 590/1000[, loss=0.000426, val_loss=0.00108, val_acc=1]\u001B[A\n",
      "Training:  59%|█████▉    | 591/1000[, loss=0.000426, val_loss=0.00108, val_acc=1]\u001B[A\n",
      "Training:  59%|█████▉    | 591/1000[, loss=0.000424, val_loss=0.00107, val_acc=1]\u001B[A\n",
      "Training:  59%|█████▉    | 592/1000[, loss=0.000423, val_loss=0.00107, val_acc=1]\u001B[A\n",
      "Training:  59%|█████▉    | 593/1000[, loss=0.000423, val_loss=0.00107, val_acc=1]\u001B[A\n",
      "Training:  59%|█████▉    | 593/1000[, loss=0.000421, val_loss=0.00107, val_acc=1]\u001B[A\n",
      "Training:  59%|█████▉    | 594/1000[, loss=0.00042, val_loss=0.00108, val_acc=1] \u001B[A\n",
      "Training:  60%|█████▉    | 595/1000[, loss=0.00042, val_loss=0.00108, val_acc=1]\u001B[A\n",
      "Training:  60%|█████▉    | 595/1000[, loss=0.000418, val_loss=0.00106, val_acc=1]\u001B[A\n",
      "Training:  60%|█████▉    | 596/1000[, loss=0.000417, val_loss=0.00107, val_acc=1]\u001B[A\n",
      "Training:  60%|█████▉    | 597/1000[, loss=0.000417, val_loss=0.00107, val_acc=1]\u001B[A\n",
      "Training:  60%|█████▉    | 597/1000[, loss=0.000416, val_loss=0.00106, val_acc=1]\u001B[A\n",
      "Training:  60%|█████▉    | 598/1000[, loss=0.000414, val_loss=0.00106, val_acc=1]\u001B[A\n",
      "Training:  60%|█████▉    | 599/1000[, loss=0.000414, val_loss=0.00106, val_acc=1]\u001B[A\n",
      "Training:  60%|█████▉    | 599/1000[, loss=0.000413, val_loss=0.00107, val_acc=1]\u001B[A\n",
      "Training:  60%|██████    | 600/1000[, loss=0.000412, val_loss=0.00105, val_acc=1]\u001B[A\n",
      "Training:  60%|██████    | 601/1000[, loss=0.000412, val_loss=0.00105, val_acc=1]\u001B[A\n",
      "Training:  60%|██████    | 601/1000[, loss=0.00041, val_loss=0.00106, val_acc=1] \u001B[A\n",
      "Training:  60%|██████    | 602/1000[, loss=0.000409, val_loss=0.00106, val_acc=1]\u001B[A\n",
      "Training:  60%|██████    | 603/1000[, loss=0.000409, val_loss=0.00106, val_acc=1]\u001B[A\n",
      "Training:  60%|██████    | 603/1000[, loss=0.000408, val_loss=0.00105, val_acc=1]\u001B[A\n",
      "Training:  60%|██████    | 604/1000[, loss=0.000406, val_loss=0.00105, val_acc=1]\u001B[A\n",
      "Training:  60%|██████    | 605/1000[, loss=0.000406, val_loss=0.00105, val_acc=1]\u001B[A\n",
      "Training:  60%|██████    | 605/1000[, loss=0.000405, val_loss=0.00105, val_acc=1]\u001B[A\n",
      "Training:  61%|██████    | 606/1000[, loss=0.000403, val_loss=0.00104, val_acc=1]\u001B[A\n",
      "Training:  61%|██████    | 607/1000[, loss=0.000403, val_loss=0.00104, val_acc=1]\u001B[A\n",
      "Training:  61%|██████    | 607/1000[, loss=0.000402, val_loss=0.00104, val_acc=1]\u001B[A\n",
      "Training:  61%|██████    | 608/1000[, loss=0.000401, val_loss=0.00104, val_acc=1]\u001B[A\n",
      "Training:  61%|██████    | 609/1000[, loss=0.000401, val_loss=0.00104, val_acc=1]\u001B[A\n",
      "Training:  61%|██████    | 609/1000[, loss=0.000399, val_loss=0.00104, val_acc=1]\u001B[A\n",
      "Training:  61%|██████    | 610/1000[, loss=0.000398, val_loss=0.00104, val_acc=1]\u001B[A\n",
      "Training:  61%|██████    | 611/1000[, loss=0.000398, val_loss=0.00104, val_acc=1]\u001B[A\n",
      "Training:  61%|██████    | 611/1000[, loss=0.000397, val_loss=0.00104, val_acc=1]\u001B[A\n",
      "Training:  61%|██████    | 612/1000[, loss=0.000396, val_loss=0.00103, val_acc=1]\u001B[A\n",
      "Training:  61%|██████▏   | 613/1000[, loss=0.000396, val_loss=0.00103, val_acc=1]\u001B[A\n",
      "Training:  61%|██████▏   | 613/1000[, loss=0.000394, val_loss=0.00103, val_acc=1]\u001B[A\n",
      "Training:  61%|██████▏   | 614/1000[, loss=0.000393, val_loss=0.00103, val_acc=1]\u001B[A\n",
      "Training:  62%|██████▏   | 615/1000[, loss=0.000393, val_loss=0.00103, val_acc=1]\u001B[A\n",
      "Training:  62%|██████▏   | 615/1000[, loss=0.000392, val_loss=0.00103, val_acc=1]\u001B[A\n",
      "Training:  62%|██████▏   | 616/1000[, loss=0.00039, val_loss=0.00103, val_acc=1] \u001B[A\n",
      "Training:  62%|██████▏   | 617/1000[, loss=0.00039, val_loss=0.00103, val_acc=1]\u001B[A\n",
      "Training:  62%|██████▏   | 617/1000[, loss=0.000389, val_loss=0.00103, val_acc=1]\u001B[A\n",
      "Training:  62%|██████▏   | 618/1000[, loss=0.000388, val_loss=0.00103, val_acc=1]\u001B[A\n",
      "Training:  62%|██████▏   | 619/1000[, loss=0.000388, val_loss=0.00103, val_acc=1]\u001B[A\n",
      "Training:  62%|██████▏   | 619/1000[, loss=0.000387, val_loss=0.00102, val_acc=1]\u001B[A\n",
      "Training:  62%|██████▏   | 620/1000[, loss=0.000386, val_loss=0.00102, val_acc=1]\u001B[A\n",
      "Training:  62%|██████▏   | 621/1000[, loss=0.000386, val_loss=0.00102, val_acc=1]\u001B[A\n",
      "Training:  62%|██████▏   | 621/1000[, loss=0.000384, val_loss=0.00102, val_acc=1]\u001B[A\n",
      "Training:  62%|██████▏   | 622/1000[, loss=0.000383, val_loss=0.00102, val_acc=1]\u001B[A\n",
      "Training:  62%|██████▏   | 623/1000[, loss=0.000383, val_loss=0.00102, val_acc=1]\u001B[A\n",
      "Training:  62%|██████▏   | 623/1000[, loss=0.000382, val_loss=0.00102, val_acc=1]\u001B[A\n",
      "Training:  62%|██████▏   | 624/1000[, loss=0.000381, val_loss=0.00101, val_acc=1]\u001B[A\n",
      "Training:  62%|██████▎   | 625/1000[, loss=0.000381, val_loss=0.00101, val_acc=1]\u001B[A\n",
      "Training:  62%|██████▎   | 625/1000[, loss=0.00038, val_loss=0.00101, val_acc=1] \u001B[A\n",
      "Training:  63%|██████▎   | 626/1000[, loss=0.000378, val_loss=0.00101, val_acc=1]\u001B[A\n",
      "Training:  63%|██████▎   | 627/1000[, loss=0.000378, val_loss=0.00101, val_acc=1]\u001B[A\n",
      "Training:  63%|██████▎   | 627/1000[, loss=0.000377, val_loss=0.00101, val_acc=1]\u001B[A\n",
      "Training:  63%|██████▎   | 628/1000[, loss=0.000376, val_loss=0.00101, val_acc=1]\u001B[A\n",
      "Training:  63%|██████▎   | 629/1000[, loss=0.000376, val_loss=0.00101, val_acc=1]\u001B[A\n",
      "Training:  63%|██████▎   | 629/1000[, loss=0.000375, val_loss=0.00101, val_acc=1]\u001B[A\n",
      "Training:  63%|██████▎   | 630/1000[, loss=0.000374, val_loss=0.001, val_acc=1]  \u001B[A\n",
      "Training:  63%|██████▎   | 631/1000[, loss=0.000374, val_loss=0.001, val_acc=1]\u001B[A\n",
      "Training:  63%|██████▎   | 631/1000[, loss=0.000373, val_loss=0.00101, val_acc=1]\u001B[A\n",
      "Training:  63%|██████▎   | 632/1000[, loss=0.000372, val_loss=0.001, val_acc=1]  \u001B[A\n",
      "Training:  63%|██████▎   | 633/1000[, loss=0.000372, val_loss=0.001, val_acc=1]\u001B[A\n",
      "Training:  63%|██████▎   | 633/1000[, loss=0.000371, val_loss=0.001, val_acc=1]\u001B[A\n",
      "Training:  63%|██████▎   | 634/1000[, loss=0.000369, val_loss=0.000999, val_acc=1]\u001B[A\n",
      "Training:  64%|██████▎   | 635/1000[, loss=0.000369, val_loss=0.000999, val_acc=1]\u001B[A\n",
      "Training:  64%|██████▎   | 635/1000[, loss=0.000368, val_loss=0.001, val_acc=1]   \u001B[A\n",
      "Training:  64%|██████▎   | 636/1000[, loss=0.000367, val_loss=0.000998, val_acc=1]\u001B[A\n",
      "Training:  64%|██████▎   | 637/1000[, loss=0.000367, val_loss=0.000998, val_acc=1]\u001B[A\n",
      "Training:  64%|██████▎   | 637/1000[, loss=0.000366, val_loss=0.000995, val_acc=1]\u001B[A\n",
      "Training:  64%|██████▍   | 638/1000[, loss=0.000365, val_loss=0.000997, val_acc=1]\u001B[A\n",
      "Training:  64%|██████▍   | 639/1000[, loss=0.000365, val_loss=0.000997, val_acc=1]\u001B[A\n",
      "Training:  64%|██████▍   | 639/1000[, loss=0.000364, val_loss=0.000987, val_acc=1]\u001B[A\n",
      "Training:  64%|██████▍   | 640/1000[, loss=0.000363, val_loss=0.000993, val_acc=1]\u001B[A\n",
      "Training:  64%|██████▍   | 641/1000[, loss=0.000363, val_loss=0.000993, val_acc=1]\u001B[A\n",
      "Training:  64%|██████▍   | 641/1000[, loss=0.000361, val_loss=0.000994, val_acc=1]\u001B[A\n",
      "Training:  64%|██████▍   | 642/1000[, loss=0.000361, val_loss=0.000985, val_acc=1]\u001B[A\n",
      "Training:  64%|██████▍   | 643/1000[, loss=0.000361, val_loss=0.000985, val_acc=1]\u001B[A\n",
      "Training:  64%|██████▍   | 643/1000[, loss=0.00036, val_loss=0.000986, val_acc=1] \u001B[A\n",
      "Training:  64%|██████▍   | 644/1000[, loss=0.000358, val_loss=0.000989, val_acc=1]\u001B[A\n",
      "Training:  64%|██████▍   | 645/1000[, loss=0.000358, val_loss=0.000989, val_acc=1]\u001B[A\n",
      "Training:  64%|██████▍   | 645/1000[, loss=0.000357, val_loss=0.000983, val_acc=1]\u001B[A\n",
      "Training:  65%|██████▍   | 646/1000[, loss=0.000356, val_loss=0.000982, val_acc=1]\u001B[A\n",
      "Training:  65%|██████▍   | 647/1000[, loss=0.000356, val_loss=0.000982, val_acc=1]\u001B[A\n",
      "Training:  65%|██████▍   | 647/1000[, loss=0.000355, val_loss=0.00098, val_acc=1] \u001B[A\n",
      "Training:  65%|██████▍   | 648/1000[, loss=0.000354, val_loss=0.000984, val_acc=1]\u001B[A\n",
      "Training:  65%|██████▍   | 649/1000[, loss=0.000354, val_loss=0.000984, val_acc=1]\u001B[A\n",
      "Training:  65%|██████▍   | 649/1000[, loss=0.000353, val_loss=0.000981, val_acc=1]\u001B[A\n",
      "Training:  65%|██████▌   | 650/1000[, loss=0.000352, val_loss=0.000976, val_acc=1]\u001B[A\n",
      "Training:  65%|██████▌   | 651/1000[, loss=0.000352, val_loss=0.000976, val_acc=1]\u001B[A\n",
      "Training:  65%|██████▌   | 651/1000[, loss=0.000351, val_loss=0.000977, val_acc=1]\u001B[A\n",
      "Training:  65%|██████▌   | 652/1000[, loss=0.00035, val_loss=0.000983, val_acc=1] \u001B[A\n",
      "Training:  65%|██████▌   | 653/1000[, loss=0.00035, val_loss=0.000983, val_acc=1]\u001B[A\n",
      "Training:  65%|██████▌   | 653/1000[, loss=0.000349, val_loss=0.000973, val_acc=1]\u001B[A\n",
      "Training:  65%|██████▌   | 654/1000[, loss=0.000348, val_loss=0.000969, val_acc=1]\u001B[A\n",
      "Training:  66%|██████▌   | 655/1000[, loss=0.000348, val_loss=0.000969, val_acc=1]\u001B[A\n",
      "Training:  66%|██████▌   | 655/1000[, loss=0.000347, val_loss=0.000972, val_acc=1]\u001B[A\n",
      "Training:  66%|██████▌   | 656/1000[, loss=0.000346, val_loss=0.000968, val_acc=1]\u001B[A\n",
      "Training:  66%|██████▌   | 657/1000[, loss=0.000346, val_loss=0.000968, val_acc=1]\u001B[A\n",
      "Training:  66%|██████▌   | 657/1000[, loss=0.000345, val_loss=0.000969, val_acc=1]\u001B[A\n",
      "Training:  66%|██████▌   | 658/1000[, loss=0.000344, val_loss=0.000966, val_acc=1]\u001B[A\n",
      "Training:  66%|██████▌   | 659/1000[, loss=0.000344, val_loss=0.000966, val_acc=1]\u001B[A\n",
      "Training:  66%|██████▌   | 659/1000[, loss=0.000343, val_loss=0.00097, val_acc=1] \u001B[A\n",
      "Training:  66%|██████▌   | 660/1000[, loss=0.000342, val_loss=0.000962, val_acc=1]\u001B[A\n",
      "Training:  66%|██████▌   | 661/1000[, loss=0.000342, val_loss=0.000962, val_acc=1]\u001B[A\n",
      "Training:  66%|██████▌   | 661/1000[, loss=0.000341, val_loss=0.000965, val_acc=1]\u001B[A\n",
      "Training:  66%|██████▌   | 662/1000[, loss=0.00034, val_loss=0.000967, val_acc=1] \u001B[A\n",
      "Training:  66%|██████▋   | 663/1000[, loss=0.00034, val_loss=0.000967, val_acc=1]\u001B[A\n",
      "Training:  66%|██████▋   | 663/1000[, loss=0.000339, val_loss=0.000959, val_acc=1]\u001B[A\n",
      "Training:  66%|██████▋   | 664/1000[, loss=0.000338, val_loss=0.000964, val_acc=1]\u001B[A\n",
      "Training:  66%|██████▋   | 665/1000[, loss=0.000338, val_loss=0.000964, val_acc=1]\u001B[A\n",
      "Training:  66%|██████▋   | 665/1000[, loss=0.000337, val_loss=0.000957, val_acc=1]\u001B[A\n",
      "Training:  67%|██████▋   | 666/1000[, loss=0.000337, val_loss=0.000956, val_acc=1]\u001B[A\n",
      "Training:  67%|██████▋   | 667/1000[, loss=0.000337, val_loss=0.000956, val_acc=1]\u001B[A\n",
      "Training:  67%|██████▋   | 667/1000[, loss=0.000335, val_loss=0.000953, val_acc=1]\u001B[A\n",
      "Training:  67%|██████▋   | 668/1000[, loss=0.000335, val_loss=0.000958, val_acc=1]\u001B[A\n",
      "Training:  67%|██████▋   | 669/1000[, loss=0.000335, val_loss=0.000958, val_acc=1]\u001B[A\n",
      "Training:  67%|██████▋   | 669/1000[, loss=0.000334, val_loss=0.000956, val_acc=1]\u001B[A\n",
      "Training:  67%|██████▋   | 670/1000[, loss=0.000333, val_loss=0.000951, val_acc=1]\u001B[A\n",
      "Training:  67%|██████▋   | 671/1000[, loss=0.000333, val_loss=0.000951, val_acc=1]\u001B[A\n",
      "Training:  67%|██████▋   | 671/1000[, loss=0.000332, val_loss=0.000954, val_acc=1]\u001B[A\n",
      "Training:  67%|██████▋   | 672/1000[, loss=0.000331, val_loss=0.00095, val_acc=1] \u001B[A\n",
      "Training:  67%|██████▋   | 673/1000[, loss=0.000331, val_loss=0.00095, val_acc=1]\u001B[A\n",
      "Training:  67%|██████▋   | 673/1000[, loss=0.00033, val_loss=0.000949, val_acc=1]\u001B[A\n",
      "Training:  67%|██████▋   | 674/1000[, loss=0.000329, val_loss=0.000951, val_acc=1]\u001B[A\n",
      "Training:  68%|██████▊   | 675/1000[, loss=0.000329, val_loss=0.000951, val_acc=1]\u001B[A\n",
      "Training:  68%|██████▊   | 675/1000[, loss=0.000328, val_loss=0.000946, val_acc=1]\u001B[A\n",
      "Training:  68%|██████▊   | 676/1000[, loss=0.000327, val_loss=0.000944, val_acc=1]\u001B[A\n",
      "Training:  68%|██████▊   | 677/1000[, loss=0.000327, val_loss=0.000944, val_acc=1]\u001B[A\n",
      "Training:  68%|██████▊   | 677/1000[, loss=0.000327, val_loss=0.000952, val_acc=1]\u001B[A\n",
      "Training:  68%|██████▊   | 678/1000[, loss=0.000325, val_loss=0.000941, val_acc=1]\u001B[A\n",
      "Training:  68%|██████▊   | 679/1000[, loss=0.000325, val_loss=0.000941, val_acc=1]\u001B[A\n",
      "Training:  68%|██████▊   | 679/1000[, loss=0.000325, val_loss=0.000946, val_acc=1]\u001B[A\n",
      "Training:  68%|██████▊   | 680/1000[, loss=0.000324, val_loss=0.000947, val_acc=1]\u001B[A\n",
      "Training:  68%|██████▊   | 681/1000[, loss=0.000324, val_loss=0.000947, val_acc=1]\u001B[A\n",
      "Training:  68%|██████▊   | 681/1000[, loss=0.000323, val_loss=0.000943, val_acc=1]\u001B[A\n",
      "Training:  68%|██████▊   | 682/1000[, loss=0.000322, val_loss=0.000939, val_acc=1]\u001B[A\n",
      "Training:  68%|██████▊   | 683/1000[, loss=0.000322, val_loss=0.000939, val_acc=1]\u001B[A\n",
      "Training:  68%|██████▊   | 683/1000[, loss=0.000321, val_loss=0.000947, val_acc=1]\u001B[A\n",
      "Training:  68%|██████▊   | 684/1000[, loss=0.00032, val_loss=0.000936, val_acc=1] \u001B[A\n",
      "Training:  68%|██████▊   | 685/1000[, loss=0.00032, val_loss=0.000936, val_acc=1]\u001B[A\n",
      "Training:  68%|██████▊   | 685/1000[, loss=0.000319, val_loss=0.000941, val_acc=1]\u001B[A\n",
      "Training:  69%|██████▊   | 686/1000[, loss=0.000319, val_loss=0.000932, val_acc=1]\u001B[A\n",
      "Training:  69%|██████▊   | 687/1000[, loss=0.000319, val_loss=0.000932, val_acc=1]\u001B[A\n",
      "Training:  69%|██████▊   | 687/1000[, loss=0.000318, val_loss=0.000939, val_acc=1]\u001B[A\n",
      "Training:  69%|██████▉   | 688/1000[, loss=0.000317, val_loss=0.000935, val_acc=1]\u001B[A\n",
      "Training:  69%|██████▉   | 689/1000[, loss=0.000317, val_loss=0.000935, val_acc=1]\u001B[A\n",
      "Training:  69%|██████▉   | 689/1000[, loss=0.000316, val_loss=0.000931, val_acc=1]\u001B[A\n",
      "Training:  69%|██████▉   | 690/1000[, loss=0.000315, val_loss=0.000936, val_acc=1]\u001B[A\n",
      "Training:  69%|██████▉   | 691/1000[, loss=0.000315, val_loss=0.000936, val_acc=1]\u001B[A\n",
      "Training:  69%|██████▉   | 691/1000[, loss=0.000314, val_loss=0.000934, val_acc=1]\u001B[A\n",
      "Training:  69%|██████▉   | 692/1000[, loss=0.000314, val_loss=0.000928, val_acc=1]\u001B[A\n",
      "Training:  69%|██████▉   | 693/1000[, loss=0.000314, val_loss=0.000928, val_acc=1]\u001B[A\n",
      "Training:  69%|██████▉   | 693/1000[, loss=0.000313, val_loss=0.000932, val_acc=1]\u001B[A\n",
      "Training:  69%|██████▉   | 694/1000[, loss=0.000312, val_loss=0.000934, val_acc=1]\u001B[A\n",
      "Training:  70%|██████▉   | 695/1000[, loss=0.000312, val_loss=0.000934, val_acc=1]\u001B[A\n",
      "Training:  70%|██████▉   | 695/1000[, loss=0.000311, val_loss=0.000926, val_acc=1]\u001B[A\n",
      "Training:  70%|██████▉   | 696/1000[, loss=0.00031, val_loss=0.000931, val_acc=1] \u001B[A\n",
      "Training:  70%|██████▉   | 697/1000[, loss=0.00031, val_loss=0.000931, val_acc=1]\u001B[A\n",
      "Training:  70%|██████▉   | 697/1000[, loss=0.000309, val_loss=0.000924, val_acc=1]\u001B[A\n",
      "Training:  70%|██████▉   | 698/1000[, loss=0.000309, val_loss=0.000924, val_acc=1]\u001B[A\n",
      "Training:  70%|██████▉   | 699/1000[, loss=0.000309, val_loss=0.000924, val_acc=1]\u001B[A\n",
      "Training:  70%|██████▉   | 699/1000[, loss=0.000308, val_loss=0.00092, val_acc=1] \u001B[A\n",
      "Training:  70%|███████   | 700/1000[, loss=0.000307, val_loss=0.000926, val_acc=1]\u001B[A\n",
      "Training:  70%|███████   | 701/1000[, loss=0.000307, val_loss=0.000926, val_acc=1]\u001B[A\n",
      "Training:  70%|███████   | 701/1000[, loss=0.000306, val_loss=0.000923, val_acc=1]\u001B[A\n",
      "Training:  70%|███████   | 702/1000[, loss=0.000306, val_loss=0.000926, val_acc=1]\u001B[A\n",
      "Training:  70%|███████   | 703/1000[, loss=0.000306, val_loss=0.000926, val_acc=1]\u001B[A\n",
      "Training:  70%|███████   | 703/1000[, loss=0.000305, val_loss=0.00092, val_acc=1] \u001B[A\n",
      "Training:  70%|███████   | 704/1000[, loss=0.000304, val_loss=0.00092, val_acc=1]\u001B[A\n",
      "Training:  70%|███████   | 705/1000[, loss=0.000304, val_loss=0.00092, val_acc=1]\u001B[A\n",
      "Training:  70%|███████   | 705/1000[, loss=0.000303, val_loss=0.000917, val_acc=1]\u001B[A\n",
      "Training:  71%|███████   | 706/1000[, loss=0.000302, val_loss=0.000918, val_acc=1]\u001B[A\n",
      "Training:  71%|███████   | 707/1000[, loss=0.000302, val_loss=0.000918, val_acc=1]\u001B[A\n",
      "Training:  71%|███████   | 707/1000[, loss=0.000301, val_loss=0.000919, val_acc=1]\u001B[A\n",
      "Training:  71%|███████   | 708/1000[, loss=0.000301, val_loss=0.000911, val_acc=1]\u001B[A\n",
      "Training:  71%|███████   | 709/1000[, loss=0.000301, val_loss=0.000911, val_acc=1]\u001B[A\n",
      "Training:  71%|███████   | 709/1000[, loss=0.0003, val_loss=0.000921, val_acc=1]  \u001B[A\n",
      "Training:  71%|███████   | 710/1000[, loss=0.000299, val_loss=0.000914, val_acc=1]\u001B[A\n",
      "Training:  71%|███████   | 711/1000[, loss=0.000299, val_loss=0.000914, val_acc=1]\u001B[A\n",
      "Training:  71%|███████   | 711/1000[, loss=0.000299, val_loss=0.000913, val_acc=1]\u001B[A\n",
      "Training:  71%|███████   | 712/1000[, loss=0.000298, val_loss=0.000915, val_acc=1]\u001B[A\n",
      "Training:  71%|███████▏  | 713/1000[, loss=0.000298, val_loss=0.000915, val_acc=1]\u001B[A\n",
      "Training:  71%|███████▏  | 713/1000[, loss=0.000297, val_loss=0.000911, val_acc=1]\u001B[A\n",
      "Training:  71%|███████▏  | 714/1000[, loss=0.000296, val_loss=0.000908, val_acc=1]\u001B[A\n",
      "Training:  72%|███████▏  | 715/1000[, loss=0.000296, val_loss=0.000908, val_acc=1]\u001B[A\n",
      "Training:  72%|███████▏  | 715/1000[, loss=0.000296, val_loss=0.000915, val_acc=1]\u001B[A\n",
      "Training:  72%|███████▏  | 716/1000[, loss=0.000295, val_loss=0.000905, val_acc=1]\u001B[A\n",
      "Training:  72%|███████▏  | 717/1000[, loss=0.000295, val_loss=0.000905, val_acc=1]\u001B[A\n",
      "Training:  72%|███████▏  | 717/1000[, loss=0.000294, val_loss=0.00091, val_acc=1] \u001B[A\n",
      "Training:  72%|███████▏  | 718/1000[, loss=0.000293, val_loss=0.00091, val_acc=1]\u001B[A\n",
      "Training:  72%|███████▏  | 719/1000[, loss=0.000293, val_loss=0.00091, val_acc=1]\u001B[A\n",
      "Training:  72%|███████▏  | 719/1000[, loss=0.000293, val_loss=0.000908, val_acc=1]\u001B[A\n",
      "Training:  72%|███████▏  | 720/1000[, loss=0.000292, val_loss=0.000907, val_acc=1]\u001B[A\n",
      "Training:  72%|███████▏  | 721/1000[, loss=0.000292, val_loss=0.000907, val_acc=1]\u001B[A\n",
      "Training:  72%|███████▏  | 721/1000[, loss=0.000291, val_loss=0.000902, val_acc=1]\u001B[A\n",
      "Training:  72%|███████▏  | 722/1000[, loss=0.00029, val_loss=0.000904, val_acc=1] \u001B[A\n",
      "Training:  72%|███████▏  | 723/1000[, loss=0.00029, val_loss=0.000904, val_acc=1]\u001B[A\n",
      "Training:  72%|███████▏  | 723/1000[, loss=0.00029, val_loss=0.000901, val_acc=1]\u001B[A\n",
      "Training:  72%|███████▏  | 724/1000[, loss=0.000289, val_loss=0.000904, val_acc=1]\u001B[A\n",
      "Training:  72%|███████▎  | 725/1000[, loss=0.000289, val_loss=0.000904, val_acc=1]\u001B[A\n",
      "Training:  72%|███████▎  | 725/1000[, loss=0.000288, val_loss=0.000904, val_acc=1]\u001B[A\n",
      "Training:  73%|███████▎  | 726/1000[, loss=0.000288, val_loss=0.000899, val_acc=1]\u001B[A\n",
      "Training:  73%|███████▎  | 727/1000[, loss=0.000288, val_loss=0.000899, val_acc=1]\u001B[A\n",
      "Training:  73%|███████▎  | 727/1000[, loss=0.000287, val_loss=0.000902, val_acc=1]\u001B[A\n",
      "Training:  73%|███████▎  | 728/1000[, loss=0.000286, val_loss=0.0009, val_acc=1]  \u001B[A\n",
      "Training:  73%|███████▎  | 729/1000[, loss=0.000286, val_loss=0.0009, val_acc=1]\u001B[A\n",
      "Training:  73%|███████▎  | 729/1000[, loss=0.000285, val_loss=0.000899, val_acc=1]\u001B[A\n",
      "Training:  73%|███████▎  | 730/1000[, loss=0.000285, val_loss=0.000898, val_acc=1]\u001B[A\n",
      "Training:  73%|███████▎  | 731/1000[, loss=0.000285, val_loss=0.000898, val_acc=1]\u001B[A\n",
      "Training:  73%|███████▎  | 731/1000[, loss=0.000284, val_loss=0.0009, val_acc=1]  \u001B[A\n",
      "Training:  73%|███████▎  | 732/1000[, loss=0.000284, val_loss=0.000897, val_acc=1]\u001B[A\n",
      "Training:  73%|███████▎  | 733/1000[, loss=0.000284, val_loss=0.000897, val_acc=1]\u001B[A\n",
      "Training:  73%|███████▎  | 733/1000[, loss=0.000283, val_loss=0.000896, val_acc=1]\u001B[A\n",
      "Training:  73%|███████▎  | 734/1000[, loss=0.000282, val_loss=0.000901, val_acc=1]\u001B[A\n",
      "Training:  74%|███████▎  | 735/1000[, loss=0.000282, val_loss=0.000901, val_acc=1]\u001B[A\n",
      "Training:  74%|███████▎  | 735/1000[, loss=0.000281, val_loss=0.000891, val_acc=1]\u001B[A\n",
      "Training:  74%|███████▎  | 736/1000[, loss=0.000281, val_loss=0.0009, val_acc=1]  \u001B[A\n",
      "Training:  74%|███████▎  | 737/1000[, loss=0.000281, val_loss=0.0009, val_acc=1]\u001B[A\n",
      "Training:  74%|███████▎  | 737/1000[, loss=0.00028, val_loss=0.000895, val_acc=1]\u001B[A\n",
      "Training:  74%|███████▍  | 738/1000[, loss=0.000279, val_loss=0.000891, val_acc=1]\u001B[A\n",
      "Training:  74%|███████▍  | 739/1000[, loss=0.000279, val_loss=0.000891, val_acc=1]\u001B[A\n",
      "Training:  74%|███████▍  | 739/1000[, loss=0.000279, val_loss=0.000894, val_acc=1]\u001B[A\n",
      "Training:  74%|███████▍  | 740/1000[, loss=0.000278, val_loss=0.000891, val_acc=1]\u001B[A\n",
      "Training:  74%|███████▍  | 741/1000[, loss=0.000278, val_loss=0.000891, val_acc=1]\u001B[A\n",
      "Training:  74%|███████▍  | 741/1000[, loss=0.000277, val_loss=0.000893, val_acc=1]\u001B[A\n",
      "Training:  74%|███████▍  | 742/1000[, loss=0.000277, val_loss=0.000888, val_acc=1]\u001B[A\n",
      "Training:  74%|███████▍  | 743/1000[, loss=0.000277, val_loss=0.000888, val_acc=1]\u001B[A\n",
      "Training:  74%|███████▍  | 743/1000[, loss=0.000276, val_loss=0.000892, val_acc=1]\u001B[A\n",
      "Training:  74%|███████▍  | 744/1000[, loss=0.000275, val_loss=0.000891, val_acc=1]\u001B[A\n",
      "Training:  74%|███████▍  | 745/1000[, loss=0.000275, val_loss=0.000891, val_acc=1]\u001B[A\n",
      "Training:  74%|███████▍  | 745/1000[, loss=0.000275, val_loss=0.000886, val_acc=1]\u001B[A\n",
      "Training:  75%|███████▍  | 746/1000[, loss=0.000274, val_loss=0.000892, val_acc=1]\u001B[A\n",
      "Training:  75%|███████▍  | 747/1000[, loss=0.000274, val_loss=0.000892, val_acc=1]\u001B[A\n",
      "Training:  75%|███████▍  | 747/1000[, loss=0.000274, val_loss=0.000884, val_acc=1]\u001B[A\n",
      "Training:  75%|███████▍  | 748/1000[, loss=0.000273, val_loss=0.000888, val_acc=1]\u001B[A\n",
      "Training:  75%|███████▍  | 749/1000[, loss=0.000273, val_loss=0.000888, val_acc=1]\u001B[A\n",
      "Training:  75%|███████▍  | 749/1000[, loss=0.000272, val_loss=0.000888, val_acc=1]\u001B[A\n",
      "Training:  75%|███████▌  | 750/1000[, loss=0.000272, val_loss=0.000884, val_acc=1]\u001B[A\n",
      "Training:  75%|███████▌  | 751/1000[, loss=0.000272, val_loss=0.000884, val_acc=1]\u001B[A\n",
      "Training:  75%|███████▌  | 751/1000[, loss=0.000271, val_loss=0.000886, val_acc=1]\u001B[A\n",
      "Training:  75%|███████▌  | 752/1000[, loss=0.00027, val_loss=0.000881, val_acc=1] \u001B[A\n",
      "Training:  75%|███████▌  | 753/1000[, loss=0.00027, val_loss=0.000881, val_acc=1]\u001B[A\n",
      "Training:  75%|███████▌  | 753/1000[, loss=0.00027, val_loss=0.000883, val_acc=1]\u001B[A\n",
      "Training:  75%|███████▌  | 754/1000[, loss=0.000269, val_loss=0.00088, val_acc=1]\u001B[A\n",
      "Training:  76%|███████▌  | 755/1000[, loss=0.000269, val_loss=0.00088, val_acc=1]\u001B[A\n",
      "Training:  76%|███████▌  | 755/1000[, loss=0.000269, val_loss=0.000883, val_acc=1]\u001B[A\n",
      "Training:  76%|███████▌  | 756/1000[, loss=0.000268, val_loss=0.000884, val_acc=1]\u001B[A\n",
      "Training:  76%|███████▌  | 757/1000[, loss=0.000268, val_loss=0.000884, val_acc=1]\u001B[A\n",
      "Training:  76%|███████▌  | 757/1000[, loss=0.000267, val_loss=0.000878, val_acc=1]\u001B[A\n",
      "Training:  76%|███████▌  | 758/1000[, loss=0.000267, val_loss=0.000882, val_acc=1]\u001B[A\n",
      "Training:  76%|███████▌  | 759/1000[, loss=0.000267, val_loss=0.000882, val_acc=1]\u001B[A\n",
      "Training:  76%|███████▌  | 759/1000[, loss=0.000266, val_loss=0.000883, val_acc=1]\u001B[A\n",
      "Training:  76%|███████▌  | 760/1000[, loss=0.000265, val_loss=0.000877, val_acc=1]\u001B[A\n",
      "Training:  76%|███████▌  | 761/1000[, loss=0.000265, val_loss=0.000877, val_acc=1]\u001B[A\n",
      "Training:  76%|███████▌  | 761/1000[, loss=0.000265, val_loss=0.00088, val_acc=1] \u001B[A\n",
      "Training:  76%|███████▌  | 762/1000[, loss=0.000264, val_loss=0.000876, val_acc=1]\u001B[A\n",
      "Training:  76%|███████▋  | 763/1000[, loss=0.000264, val_loss=0.000876, val_acc=1]\u001B[A\n",
      "Training:  76%|███████▋  | 763/1000[, loss=0.000264, val_loss=0.000877, val_acc=1]\u001B[A\n",
      "Training:  76%|███████▋  | 764/1000[, loss=0.000263, val_loss=0.000872, val_acc=1]\u001B[A\n",
      "Training:  76%|███████▋  | 765/1000[, loss=0.000263, val_loss=0.000872, val_acc=1]\u001B[A\n",
      "Training:  76%|███████▋  | 765/1000[, loss=0.000262, val_loss=0.00088, val_acc=1] \u001B[A\n",
      "Training:  77%|███████▋  | 766/1000[, loss=0.000262, val_loss=0.000872, val_acc=1]\u001B[A\n",
      "Training:  77%|███████▋  | 767/1000[, loss=0.000262, val_loss=0.000872, val_acc=1]\u001B[A\n",
      "Training:  77%|███████▋  | 767/1000[, loss=0.000261, val_loss=0.000876, val_acc=1]\u001B[A\n",
      "Training:  77%|███████▋  | 768/1000[, loss=0.000261, val_loss=0.000876, val_acc=1]\u001B[A\n",
      "Training:  77%|███████▋  | 769/1000[, loss=0.000261, val_loss=0.000876, val_acc=1]\u001B[A\n",
      "Training:  77%|███████▋  | 769/1000[, loss=0.00026, val_loss=0.000874, val_acc=1] \u001B[A\n",
      "Training:  77%|███████▋  | 770/1000[, loss=0.000259, val_loss=0.000874, val_acc=1]\u001B[A\n",
      "Training:  77%|███████▋  | 771/1000[, loss=0.000259, val_loss=0.000874, val_acc=1]\u001B[A\n",
      "Training:  77%|███████▋  | 771/1000[, loss=0.000259, val_loss=0.000872, val_acc=1]\u001B[A\n",
      "Training:  77%|███████▋  | 772/1000[, loss=0.000258, val_loss=0.000873, val_acc=1]\u001B[A\n",
      "Training:  77%|███████▋  | 773/1000[, loss=0.000258, val_loss=0.000873, val_acc=1]\u001B[A\n",
      "Training:  77%|███████▋  | 773/1000[, loss=0.000258, val_loss=0.000869, val_acc=1]\u001B[A\n",
      "Training:  77%|███████▋  | 774/1000[, loss=0.000257, val_loss=0.000869, val_acc=1]\u001B[A\n",
      "Training:  78%|███████▊  | 775/1000[, loss=0.000257, val_loss=0.000869, val_acc=1]\u001B[A\n",
      "Training:  78%|███████▊  | 775/1000[, loss=0.000257, val_loss=0.000869, val_acc=1]\u001B[A\n",
      "Training:  78%|███████▊  | 776/1000[, loss=0.000256, val_loss=0.000872, val_acc=1]\u001B[A\n",
      "Training:  78%|███████▊  | 777/1000[, loss=0.000256, val_loss=0.000872, val_acc=1]\u001B[A\n",
      "Training:  78%|███████▊  | 777/1000[, loss=0.000255, val_loss=0.000867, val_acc=1]\u001B[A\n",
      "Training:  78%|███████▊  | 778/1000[, loss=0.000255, val_loss=0.000871, val_acc=1]\u001B[A\n",
      "Training:  78%|███████▊  | 779/1000[, loss=0.000255, val_loss=0.000871, val_acc=1]\u001B[A\n",
      "Training:  78%|███████▊  | 779/1000[, loss=0.000254, val_loss=0.00087, val_acc=1] \u001B[A\n",
      "Training:  78%|███████▊  | 780/1000[, loss=0.000254, val_loss=0.000862, val_acc=1]\u001B[A\n",
      "Training:  78%|███████▊  | 781/1000[, loss=0.000254, val_loss=0.000862, val_acc=1]\u001B[A\n",
      "Training:  78%|███████▊  | 781/1000[, loss=0.000253, val_loss=0.00087, val_acc=1] \u001B[A\n",
      "Training:  78%|███████▊  | 782/1000[, loss=0.000253, val_loss=0.000862, val_acc=1]\u001B[A\n",
      "Training:  78%|███████▊  | 783/1000[, loss=0.000253, val_loss=0.000862, val_acc=1]\u001B[A\n",
      "Training:  78%|███████▊  | 783/1000[, loss=0.000252, val_loss=0.000867, val_acc=1]\u001B[A\n",
      "Training:  78%|███████▊  | 784/1000[, loss=0.000252, val_loss=0.000862, val_acc=1]\u001B[A\n",
      "Training:  78%|███████▊  | 785/1000[, loss=0.000252, val_loss=0.000862, val_acc=1]\u001B[A\n",
      "Training:  78%|███████▊  | 785/1000[, loss=0.000251, val_loss=0.000869, val_acc=1]\u001B[A\n",
      "Training:  79%|███████▊  | 786/1000[, loss=0.00025, val_loss=0.000863, val_acc=1] \u001B[A\n",
      "Training:  79%|███████▊  | 787/1000[, loss=0.00025, val_loss=0.000863, val_acc=1]\u001B[A\n",
      "Training:  79%|███████▊  | 787/1000[, loss=0.00025, val_loss=0.00086, val_acc=1] \u001B[A\n",
      "Training:  79%|███████▉  | 788/1000[, loss=0.000249, val_loss=0.000865, val_acc=1]\u001B[A\n",
      "Training:  79%|███████▉  | 789/1000[, loss=0.000249, val_loss=0.000865, val_acc=1]\u001B[A\n",
      "Training:  79%|███████▉  | 789/1000[, loss=0.000249, val_loss=0.00086, val_acc=1] \u001B[A\n",
      "Training:  79%|███████▉  | 790/1000[, loss=0.000248, val_loss=0.000863, val_acc=1]\u001B[A\n",
      "Training:  79%|███████▉  | 791/1000[, loss=0.000248, val_loss=0.000863, val_acc=1]\u001B[A\n",
      "Training:  79%|███████▉  | 791/1000[, loss=0.000248, val_loss=0.00086, val_acc=1] \u001B[A\n",
      "Training:  79%|███████▉  | 792/1000[, loss=0.000247, val_loss=0.000859, val_acc=1]\u001B[A\n",
      "Training:  79%|███████▉  | 793/1000[, loss=0.000247, val_loss=0.000859, val_acc=1]\u001B[A\n",
      "Training:  79%|███████▉  | 793/1000[, loss=0.000247, val_loss=0.000858, val_acc=1]\u001B[A\n",
      "Training:  79%|███████▉  | 794/1000[, loss=0.000246, val_loss=0.000862, val_acc=1]\u001B[A\n",
      "Training:  80%|███████▉  | 795/1000[, loss=0.000246, val_loss=0.000862, val_acc=1]\u001B[A\n",
      "Training:  80%|███████▉  | 795/1000[, loss=0.000246, val_loss=0.00086, val_acc=1] \u001B[A\n",
      "Training:  80%|███████▉  | 796/1000[, loss=0.000245, val_loss=0.000856, val_acc=1]\u001B[A\n",
      "Training:  80%|███████▉  | 797/1000[, loss=0.000245, val_loss=0.000856, val_acc=1]\u001B[A\n",
      "Training:  80%|███████▉  | 797/1000[, loss=0.000244, val_loss=0.000861, val_acc=1]\u001B[A\n",
      "Training:  80%|███████▉  | 798/1000[, loss=0.000244, val_loss=0.000855, val_acc=1]\u001B[A\n",
      "Training:  80%|███████▉  | 799/1000[, loss=0.000244, val_loss=0.000855, val_acc=1]\u001B[A\n",
      "Training:  80%|███████▉  | 799/1000[, loss=0.000244, val_loss=0.000859, val_acc=1]\u001B[A\n",
      "Training:  80%|████████  | 800/1000[, loss=0.000243, val_loss=0.000852, val_acc=1]\u001B[A\n",
      "Training:  80%|████████  | 801/1000[, loss=0.000243, val_loss=0.000852, val_acc=1]\u001B[A\n",
      "Training:  80%|████████  | 801/1000[, loss=0.000243, val_loss=0.000859, val_acc=1]\u001B[A\n",
      "Training:  80%|████████  | 802/1000[, loss=0.000242, val_loss=0.000857, val_acc=1]\u001B[A\n",
      "Training:  80%|████████  | 803/1000[, loss=0.000242, val_loss=0.000857, val_acc=1]\u001B[A\n",
      "Training:  80%|████████  | 803/1000[, loss=0.000241, val_loss=0.000852, val_acc=1]\u001B[A\n",
      "Training:  80%|████████  | 804/1000[, loss=0.000241, val_loss=0.000857, val_acc=1]\u001B[A\n",
      "Training:  80%|████████  | 805/1000[, loss=0.000241, val_loss=0.000857, val_acc=1]\u001B[A\n",
      "Training:  80%|████████  | 805/1000[, loss=0.00024, val_loss=0.000852, val_acc=1] \u001B[A\n",
      "Training:  81%|████████  | 806/1000[, loss=0.00024, val_loss=0.000858, val_acc=1]\u001B[A\n",
      "Training:  81%|████████  | 807/1000[, loss=0.000239, val_loss=0.000851, val_acc=1]\u001B[A\n",
      "Training:  81%|████████  | 808/1000[, loss=0.000239, val_loss=0.000851, val_acc=1]\u001B[A\n",
      "Training:  81%|████████  | 808/1000[, loss=0.000239, val_loss=0.000855, val_acc=1]\u001B[A\n",
      "Training:  81%|████████  | 809/1000[, loss=0.000238, val_loss=0.000855, val_acc=1]\u001B[A\n",
      "Training:  81%|████████  | 810/1000[, loss=0.000238, val_loss=0.000855, val_acc=1]\u001B[A\n",
      "Training:  81%|████████  | 810/1000[, loss=0.000238, val_loss=0.000851, val_acc=1]\u001B[A\n",
      "Training:  81%|████████  | 811/1000[, loss=0.000237, val_loss=0.000854, val_acc=1]\u001B[A\n",
      "Training:  81%|████████  | 812/1000[, loss=0.000237, val_loss=0.000854, val_acc=1]\u001B[A\n",
      "Training:  81%|████████  | 812/1000[, loss=0.000237, val_loss=0.000849, val_acc=1]\u001B[A\n",
      "Training:  81%|████████▏ | 813/1000[, loss=0.000237, val_loss=0.000852, val_acc=1]\u001B[A\n",
      "Training:  81%|████████▏ | 814/1000[, loss=0.000237, val_loss=0.000852, val_acc=1]\u001B[A\n",
      "Training:  81%|████████▏ | 814/1000[, loss=0.000236, val_loss=0.000848, val_acc=1]\u001B[A\n",
      "Training:  82%|████████▏ | 815/1000[, loss=0.000235, val_loss=0.000852, val_acc=1]\u001B[A\n",
      "Training:  82%|████████▏ | 816/1000[, loss=0.000235, val_loss=0.000852, val_acc=1]\u001B[A\n",
      "Training:  82%|████████▏ | 816/1000[, loss=0.000235, val_loss=0.000848, val_acc=1]\u001B[A\n",
      "Training:  82%|████████▏ | 817/1000[, loss=0.000234, val_loss=0.000848, val_acc=1]\u001B[A\n",
      "Training:  82%|████████▏ | 818/1000[, loss=0.000234, val_loss=0.000848, val_acc=1]\u001B[A\n",
      "Training:  82%|████████▏ | 818/1000[, loss=0.000234, val_loss=0.000847, val_acc=1]\u001B[A\n",
      "Training:  82%|████████▏ | 819/1000[, loss=0.000234, val_loss=0.00085, val_acc=1] \u001B[A\n",
      "Training:  82%|████████▏ | 820/1000[, loss=0.000234, val_loss=0.00085, val_acc=1]\u001B[A\n",
      "Training:  82%|████████▏ | 820/1000[, loss=0.000233, val_loss=0.000849, val_acc=1]\u001B[A\n",
      "Training:  82%|████████▏ | 821/1000[, loss=0.000233, val_loss=0.000845, val_acc=1]\u001B[A\n",
      "Training:  82%|████████▏ | 822/1000[, loss=0.000233, val_loss=0.000845, val_acc=1]\u001B[A\n",
      "Training:  82%|████████▏ | 822/1000[, loss=0.000232, val_loss=0.000849, val_acc=1]\u001B[A\n",
      "Training:  82%|████████▏ | 823/1000[, loss=0.000232, val_loss=0.000844, val_acc=1]\u001B[A\n",
      "Training:  82%|████████▏ | 824/1000[, loss=0.000232, val_loss=0.000844, val_acc=1]\u001B[A\n",
      "Training:  82%|████████▏ | 824/1000[, loss=0.000231, val_loss=0.000848, val_acc=1]\u001B[A\n",
      "Training:  82%|████████▎ | 825/1000[, loss=0.000231, val_loss=0.00084, val_acc=1] \u001B[A\n",
      "Training:  83%|████████▎ | 826/1000[, loss=0.000231, val_loss=0.00084, val_acc=1]\u001B[A\n",
      "Training:  83%|████████▎ | 826/1000[, loss=0.00023, val_loss=0.000847, val_acc=1]\u001B[A\n",
      "Training:  83%|████████▎ | 827/1000[, loss=0.00023, val_loss=0.000844, val_acc=1]\u001B[A\n",
      "Training:  83%|████████▎ | 828/1000[, loss=0.00023, val_loss=0.000844, val_acc=1]\u001B[A\n",
      "Training:  83%|████████▎ | 828/1000[, loss=0.000229, val_loss=0.000844, val_acc=1]\u001B[A\n",
      "Training:  83%|████████▎ | 829/1000[, loss=0.000229, val_loss=0.000842, val_acc=1]\u001B[A\n",
      "Training:  83%|████████▎ | 830/1000[, loss=0.000229, val_loss=0.000842, val_acc=1]\u001B[A\n",
      "Training:  83%|████████▎ | 830/1000[, loss=0.000229, val_loss=0.000843, val_acc=1]\u001B[A\n",
      "Training:  83%|████████▎ | 831/1000[, loss=0.000228, val_loss=0.000846, val_acc=1]\u001B[A\n",
      "Training:  83%|████████▎ | 832/1000[, loss=0.000228, val_loss=0.000846, val_acc=1]\u001B[A\n",
      "Training:  83%|████████▎ | 832/1000[, loss=0.000228, val_loss=0.00084, val_acc=1] \u001B[A\n",
      "Training:  83%|████████▎ | 833/1000[, loss=0.000227, val_loss=0.000843, val_acc=1]\u001B[A\n",
      "Training:  83%|████████▎ | 834/1000[, loss=0.000227, val_loss=0.000843, val_acc=1]\u001B[A\n",
      "Training:  83%|████████▎ | 834/1000[, loss=0.000227, val_loss=0.000838, val_acc=1]\u001B[A\n",
      "Training:  84%|████████▎ | 835/1000[, loss=0.000226, val_loss=0.000843, val_acc=1]\u001B[A\n",
      "Training:  84%|████████▎ | 836/1000[, loss=0.000226, val_loss=0.000843, val_acc=1]\u001B[A\n",
      "Training:  84%|████████▎ | 836/1000[, loss=0.000226, val_loss=0.000841, val_acc=1]\u001B[A\n",
      "Training:  84%|████████▎ | 837/1000[, loss=0.000225, val_loss=0.000839, val_acc=1]\u001B[A\n",
      "Training:  84%|████████▍ | 838/1000[, loss=0.000225, val_loss=0.000839, val_acc=1]\u001B[A\n",
      "Training:  84%|████████▍ | 838/1000[, loss=0.000225, val_loss=0.00084, val_acc=1] \u001B[A\n",
      "Training:  84%|████████▍ | 839/1000[, loss=0.000224, val_loss=0.000838, val_acc=1]\u001B[A\n",
      "Training:  84%|████████▍ | 840/1000[, loss=0.000224, val_loss=0.000838, val_acc=1]\u001B[A\n",
      "Training:  84%|████████▍ | 840/1000[, loss=0.000224, val_loss=0.00084, val_acc=1] \u001B[A\n",
      "Training:  84%|████████▍ | 841/1000[, loss=0.000224, val_loss=0.000837, val_acc=1]\u001B[A\n",
      "Training:  84%|████████▍ | 842/1000[, loss=0.000224, val_loss=0.000837, val_acc=1]\u001B[A\n",
      "Training:  84%|████████▍ | 842/1000[, loss=0.000223, val_loss=0.000841, val_acc=1]\u001B[A\n",
      "Training:  84%|████████▍ | 843/1000[, loss=0.000223, val_loss=0.000837, val_acc=1]\u001B[A\n",
      "Training:  84%|████████▍ | 844/1000[, loss=0.000223, val_loss=0.000837, val_acc=1]\u001B[A\n",
      "Training:  84%|████████▍ | 844/1000[, loss=0.000222, val_loss=0.00084, val_acc=1] \u001B[A\n",
      "Training:  84%|████████▍ | 845/1000[, loss=0.000222, val_loss=0.000836, val_acc=1]\u001B[A\n",
      "Training:  85%|████████▍ | 846/1000[, loss=0.000222, val_loss=0.000836, val_acc=1]\u001B[A\n",
      "Training:  85%|████████▍ | 846/1000[, loss=0.000221, val_loss=0.000836, val_acc=1]\u001B[A\n",
      "Training:  85%|████████▍ | 847/1000[, loss=0.000221, val_loss=0.000835, val_acc=1]\u001B[A\n",
      "Training:  85%|████████▍ | 848/1000[, loss=0.000221, val_loss=0.000835, val_acc=1]\u001B[A\n",
      "Training:  85%|████████▍ | 848/1000[, loss=0.00022, val_loss=0.000836, val_acc=1] \u001B[A\n",
      "Training:  85%|████████▍ | 849/1000[, loss=0.00022, val_loss=0.00084, val_acc=1] \u001B[A\n",
      "Training:  85%|████████▌ | 850/1000[, loss=0.00022, val_loss=0.00084, val_acc=1]\u001B[A\n",
      "Training:  85%|████████▌ | 850/1000[, loss=0.00022, val_loss=0.000832, val_acc=1]\u001B[A\n",
      "Training:  85%|████████▌ | 851/1000[, loss=0.000219, val_loss=0.000837, val_acc=1]\u001B[A\n",
      "Training:  85%|████████▌ | 852/1000[, loss=0.000219, val_loss=0.000837, val_acc=1]\u001B[A\n",
      "Training:  85%|████████▌ | 852/1000[, loss=0.000219, val_loss=0.000835, val_acc=1]\u001B[A\n",
      "Training:  85%|████████▌ | 853/1000[, loss=0.000218, val_loss=0.000835, val_acc=1]\u001B[A\n",
      "Training:  85%|████████▌ | 854/1000[, loss=0.000218, val_loss=0.000835, val_acc=1]\u001B[A\n",
      "Training:  85%|████████▌ | 854/1000[, loss=0.000218, val_loss=0.000834, val_acc=1]\u001B[A\n",
      "Training:  86%|████████▌ | 855/1000[, loss=0.000218, val_loss=0.000835, val_acc=1]\u001B[A\n",
      "Training:  86%|████████▌ | 856/1000[, loss=0.000218, val_loss=0.000835, val_acc=1]\u001B[A\n",
      "Training:  86%|████████▌ | 856/1000[, loss=0.000217, val_loss=0.000833, val_acc=1]\u001B[A\n",
      "Training:  86%|████████▌ | 857/1000[, loss=0.000217, val_loss=0.000834, val_acc=1]\u001B[A\n",
      "Training:  86%|████████▌ | 858/1000[, loss=0.000217, val_loss=0.000834, val_acc=1]\u001B[A\n",
      "Training:  86%|████████▌ | 858/1000[, loss=0.000216, val_loss=0.000832, val_acc=1]\u001B[A\n",
      "Training:  86%|████████▌ | 859/1000[, loss=0.000216, val_loss=0.000834, val_acc=1]\u001B[A\n",
      "Training:  86%|████████▌ | 860/1000[, loss=0.000216, val_loss=0.000834, val_acc=1]\u001B[A\n",
      "Training:  86%|████████▌ | 860/1000[, loss=0.000215, val_loss=0.00083, val_acc=1] \u001B[A\n",
      "Training:  86%|████████▌ | 861/1000[, loss=0.000215, val_loss=0.000832, val_acc=1]\u001B[A\n",
      "Training:  86%|████████▌ | 862/1000[, loss=0.000215, val_loss=0.000832, val_acc=1]\u001B[A\n",
      "Training:  86%|████████▌ | 862/1000[, loss=0.000215, val_loss=0.000835, val_acc=1]\u001B[A\n",
      "Training:  86%|████████▋ | 863/1000[, loss=0.000214, val_loss=0.000829, val_acc=1]\u001B[A\n",
      "Training:  86%|████████▋ | 864/1000[, loss=0.000214, val_loss=0.000829, val_acc=1]\u001B[A\n",
      "Training:  86%|████████▋ | 864/1000[, loss=0.000214, val_loss=0.000833, val_acc=1]\u001B[A\n",
      "Training:  86%|████████▋ | 865/1000[, loss=0.000213, val_loss=0.000826, val_acc=1]\u001B[A\n",
      "Training:  87%|████████▋ | 866/1000[, loss=0.000213, val_loss=0.000826, val_acc=1]\u001B[A\n",
      "Training:  87%|████████▋ | 866/1000[, loss=0.000213, val_loss=0.000832, val_acc=1]\u001B[A\n",
      "Training:  87%|████████▋ | 867/1000[, loss=0.000213, val_loss=0.00083, val_acc=1] \u001B[A\n",
      "Training:  87%|████████▋ | 868/1000[, loss=0.000213, val_loss=0.00083, val_acc=1]\u001B[A\n",
      "Training:  87%|████████▋ | 868/1000[, loss=0.000212, val_loss=0.000829, val_acc=1]\u001B[A\n",
      "Training:  87%|████████▋ | 869/1000[, loss=0.000212, val_loss=0.000828, val_acc=1]\u001B[A\n",
      "Training:  87%|████████▋ | 870/1000[, loss=0.000212, val_loss=0.000828, val_acc=1]\u001B[A\n",
      "Training:  87%|████████▋ | 870/1000[, loss=0.000211, val_loss=0.000832, val_acc=1]\u001B[A\n",
      "Training:  87%|████████▋ | 871/1000[, loss=0.000211, val_loss=0.000827, val_acc=1]\u001B[A\n",
      "Training:  87%|████████▋ | 872/1000[, loss=0.000211, val_loss=0.000827, val_acc=1]\u001B[A\n",
      "Training:  87%|████████▋ | 872/1000[, loss=0.000211, val_loss=0.000829, val_acc=1]\u001B[A\n",
      "Training:  87%|████████▋ | 873/1000[, loss=0.00021, val_loss=0.000828, val_acc=1] \u001B[A\n",
      "Training:  87%|████████▋ | 874/1000[, loss=0.00021, val_loss=0.000828, val_acc=1]\u001B[A\n",
      "Training:  87%|████████▋ | 874/1000[, loss=0.00021, val_loss=0.000827, val_acc=1]\u001B[A\n",
      "Training:  88%|████████▊ | 875/1000[, loss=0.000209, val_loss=0.000831, val_acc=1]\u001B[A\n",
      "Training:  88%|████████▊ | 876/1000[, loss=0.000209, val_loss=0.000831, val_acc=1]\u001B[A\n",
      "Training:  88%|████████▊ | 876/1000[, loss=0.000209, val_loss=0.000824, val_acc=1]\u001B[A\n",
      "Training:  88%|████████▊ | 877/1000[, loss=0.000209, val_loss=0.00083, val_acc=1] \u001B[A\n",
      "Training:  88%|████████▊ | 878/1000[, loss=0.000209, val_loss=0.00083, val_acc=1]\u001B[A\n",
      "Training:  88%|████████▊ | 878/1000[, loss=0.000208, val_loss=0.000829, val_acc=1]\u001B[A\n",
      "Training:  88%|████████▊ | 879/1000[, loss=0.000208, val_loss=0.000824, val_acc=1]\u001B[A\n",
      "Training:  88%|████████▊ | 880/1000[, loss=0.000208, val_loss=0.000824, val_acc=1]\u001B[A\n",
      "Training:  88%|████████▊ | 880/1000[, loss=0.000208, val_loss=0.000827, val_acc=1]\u001B[A\n",
      "Training:  88%|████████▊ | 881/1000[, loss=0.000207, val_loss=0.000824, val_acc=1]\u001B[A\n",
      "Training:  88%|████████▊ | 882/1000[, loss=0.000207, val_loss=0.000824, val_acc=1]\u001B[A\n",
      "Training:  88%|████████▊ | 882/1000[, loss=0.000207, val_loss=0.000826, val_acc=1]\u001B[A\n",
      "Training:  88%|████████▊ | 883/1000[, loss=0.000206, val_loss=0.000823, val_acc=1]\u001B[A\n",
      "Training:  88%|████████▊ | 884/1000[, loss=0.000206, val_loss=0.000823, val_acc=1]\u001B[A\n",
      "Training:  88%|████████▊ | 884/1000[, loss=0.000206, val_loss=0.000829, val_acc=1]\u001B[A\n",
      "Training:  88%|████████▊ | 885/1000[, loss=0.000206, val_loss=0.000822, val_acc=1]\u001B[A\n",
      "Training:  89%|████████▊ | 886/1000[, loss=0.000206, val_loss=0.000822, val_acc=1]\u001B[A\n",
      "Training:  89%|████████▊ | 886/1000[, loss=0.000205, val_loss=0.000827, val_acc=1]\u001B[A\n",
      "Training:  89%|████████▊ | 887/1000[, loss=0.000205, val_loss=0.000821, val_acc=1]\u001B[A\n",
      "Training:  89%|████████▉ | 888/1000[, loss=0.000205, val_loss=0.000821, val_acc=1]\u001B[A\n",
      "Training:  89%|████████▉ | 888/1000[, loss=0.000204, val_loss=0.000827, val_acc=1]\u001B[A\n",
      "Training:  89%|████████▉ | 889/1000[, loss=0.000204, val_loss=0.000823, val_acc=1]\u001B[A\n",
      "Training:  89%|████████▉ | 890/1000[, loss=0.000204, val_loss=0.000823, val_acc=1]\u001B[A\n",
      "Training:  89%|████████▉ | 890/1000[, loss=0.000204, val_loss=0.000823, val_acc=1]\u001B[A\n",
      "Training:  89%|████████▉ | 891/1000[, loss=0.000203, val_loss=0.000824, val_acc=1]\u001B[A\n",
      "Training:  89%|████████▉ | 892/1000[, loss=0.000203, val_loss=0.000824, val_acc=1]\u001B[A\n",
      "Training:  89%|████████▉ | 892/1000[, loss=0.000203, val_loss=0.000822, val_acc=1]\u001B[A\n",
      "Training:  89%|████████▉ | 893/1000[, loss=0.000203, val_loss=0.000822, val_acc=1]\u001B[A\n",
      "Training:  89%|████████▉ | 894/1000[, loss=0.000203, val_loss=0.000822, val_acc=1]\u001B[A\n",
      "Training:  89%|████████▉ | 894/1000[, loss=0.000202, val_loss=0.00082, val_acc=1] \u001B[A\n",
      "Training:  90%|████████▉ | 895/1000[, loss=0.000202, val_loss=0.000823, val_acc=1]\u001B[A\n",
      "Training:  90%|████████▉ | 896/1000[, loss=0.000202, val_loss=0.000823, val_acc=1]\u001B[A\n",
      "Training:  90%|████████▉ | 896/1000[, loss=0.000201, val_loss=0.00082, val_acc=1] \u001B[A\n",
      "Training:  90%|████████▉ | 897/1000[, loss=0.000201, val_loss=0.000822, val_acc=1]\u001B[A\n",
      "Training:  90%|████████▉ | 898/1000[, loss=0.000201, val_loss=0.000822, val_acc=1]\u001B[A\n",
      "Training:  90%|████████▉ | 898/1000[, loss=0.000201, val_loss=0.00082, val_acc=1] \u001B[A\n",
      "Training:  90%|████████▉ | 899/1000[, loss=0.0002, val_loss=0.00082, val_acc=1]  \u001B[A\n",
      "Training:  90%|█████████ | 900/1000[, loss=0.0002, val_loss=0.00082, val_acc=1]\u001B[A\n",
      "Training:  90%|█████████ | 900/1000[, loss=0.0002, val_loss=0.000819, val_acc=1]\u001B[A\n",
      "Training:  90%|█████████ | 901/1000[, loss=0.0002, val_loss=0.000821, val_acc=1]\u001B[A\n",
      "Training:  90%|█████████ | 902/1000[, loss=0.0002, val_loss=0.000821, val_acc=1]\u001B[A\n",
      "Training:  90%|█████████ | 902/1000[, loss=0.000199, val_loss=0.000819, val_acc=1]\u001B[A\n",
      "Training:  90%|█████████ | 903/1000[, loss=0.000199, val_loss=0.000819, val_acc=1]\u001B[A\n",
      "Training:  90%|█████████ | 904/1000[, loss=0.000199, val_loss=0.000819, val_acc=1]\u001B[A\n",
      "Training:  90%|█████████ | 904/1000[, loss=0.000199, val_loss=0.000817, val_acc=1]\u001B[A\n",
      "Training:  90%|█████████ | 905/1000[, loss=0.000198, val_loss=0.000819, val_acc=1]\u001B[A\n",
      "Training:  91%|█████████ | 906/1000[, loss=0.000198, val_loss=0.000819, val_acc=1]\u001B[A\n",
      "Training:  91%|█████████ | 906/1000[, loss=0.000198, val_loss=0.000821, val_acc=1]\u001B[A\n",
      "Training:  91%|█████████ | 907/1000[, loss=0.000198, val_loss=0.000816, val_acc=1]\u001B[A\n",
      "Training:  91%|█████████ | 908/1000[, loss=0.000198, val_loss=0.000816, val_acc=1]\u001B[A\n",
      "Training:  91%|█████████ | 908/1000[, loss=0.000197, val_loss=0.000822, val_acc=1]\u001B[A\n",
      "Training:  91%|█████████ | 909/1000[, loss=0.000197, val_loss=0.000814, val_acc=1]\u001B[A\n",
      "Training:  91%|█████████ | 910/1000[, loss=0.000197, val_loss=0.000814, val_acc=1]\u001B[A\n",
      "Training:  91%|█████████ | 910/1000[, loss=0.000197, val_loss=0.00082, val_acc=1] \u001B[A\n",
      "Training:  91%|█████████ | 911/1000[, loss=0.000196, val_loss=0.000814, val_acc=1]\u001B[A\n",
      "Training:  91%|█████████ | 912/1000[, loss=0.000196, val_loss=0.000814, val_acc=1]\u001B[A\n",
      "Training:  91%|█████████ | 912/1000[, loss=0.000196, val_loss=0.00082, val_acc=1] \u001B[A\n",
      "Training:  91%|█████████▏| 913/1000[, loss=0.000195, val_loss=0.000816, val_acc=1]\u001B[A\n",
      "Training:  91%|█████████▏| 914/1000[, loss=0.000195, val_loss=0.000816, val_acc=1]\u001B[A\n",
      "Training:  91%|█████████▏| 914/1000[, loss=0.000195, val_loss=0.000818, val_acc=1]\u001B[A\n",
      "Training:  92%|█████████▏| 915/1000[, loss=0.000195, val_loss=0.00082, val_acc=1] \u001B[A\n",
      "Training:  92%|█████████▏| 916/1000[, loss=0.000195, val_loss=0.00082, val_acc=1]\u001B[A\n",
      "Training:  92%|█████████▏| 916/1000[, loss=0.000195, val_loss=0.000813, val_acc=1]\u001B[A\n",
      "Training:  92%|█████████▏| 917/1000[, loss=0.000194, val_loss=0.000818, val_acc=1]\u001B[A\n",
      "Training:  92%|█████████▏| 918/1000[, loss=0.000194, val_loss=0.000818, val_acc=1]\u001B[A\n",
      "Training:  92%|█████████▏| 918/1000[, loss=0.000194, val_loss=0.000813, val_acc=1]\u001B[A\n",
      "Training:  92%|█████████▏| 919/1000[, loss=0.000194, val_loss=0.000818, val_acc=1]\u001B[A\n",
      "Training:  92%|█████████▏| 920/1000[, loss=0.000194, val_loss=0.000818, val_acc=1]\u001B[A\n",
      "Training:  92%|█████████▏| 920/1000[, loss=0.000193, val_loss=0.000818, val_acc=1]\u001B[A\n",
      "Training:  92%|█████████▏| 921/1000[, loss=0.000193, val_loss=0.000813, val_acc=1]\u001B[A\n",
      "Training:  92%|█████████▏| 922/1000[, loss=0.000193, val_loss=0.000813, val_acc=1]\u001B[A\n",
      "Training:  92%|█████████▏| 922/1000[, loss=0.000193, val_loss=0.000816, val_acc=1]\u001B[A\n",
      "Training:  92%|█████████▏| 923/1000[, loss=0.000192, val_loss=0.000813, val_acc=1]\u001B[A\n",
      "Training:  92%|█████████▏| 924/1000[, loss=0.000192, val_loss=0.000813, val_acc=1]\u001B[A\n",
      "Training:  92%|█████████▏| 924/1000[, loss=0.000192, val_loss=0.000816, val_acc=1]\u001B[A\n",
      "Training:  92%|█████████▎| 925/1000[, loss=0.000192, val_loss=0.000812, val_acc=1]\u001B[A\n",
      "Training:  93%|█████████▎| 926/1000[, loss=0.000192, val_loss=0.000812, val_acc=1]\u001B[A\n",
      "Training:  93%|█████████▎| 926/1000[, loss=0.000191, val_loss=0.000817, val_acc=1]\u001B[A\n",
      "Training:  93%|█████████▎| 927/1000[, loss=0.000191, val_loss=0.000811, val_acc=1]\u001B[A\n",
      "Training:  93%|█████████▎| 928/1000[, loss=0.000191, val_loss=0.000811, val_acc=1]\u001B[A\n",
      "Training:  93%|█████████▎| 928/1000[, loss=0.000191, val_loss=0.000817, val_acc=1]\u001B[A\n",
      "Training:  93%|█████████▎| 929/1000[, loss=0.00019, val_loss=0.000813, val_acc=1] \u001B[A\n",
      "Training:  93%|█████████▎| 930/1000[, loss=0.00019, val_loss=0.000813, val_acc=1]\u001B[A\n",
      "Training:  93%|█████████▎| 930/1000[, loss=0.00019, val_loss=0.000813, val_acc=1]\u001B[A\n",
      "Training:  93%|█████████▎| 931/1000[, loss=0.00019, val_loss=0.000814, val_acc=1]\u001B[A\n",
      "Training:  93%|█████████▎| 932/1000[, loss=0.00019, val_loss=0.000814, val_acc=1]\u001B[A\n",
      "Training:  93%|█████████▎| 932/1000[, loss=0.000189, val_loss=0.00081, val_acc=1]\u001B[A\n",
      "Training:  93%|█████████▎| 933/1000[, loss=0.000189, val_loss=0.000814, val_acc=1]\u001B[A\n",
      "Training:  93%|█████████▎| 934/1000[, loss=0.000189, val_loss=0.000814, val_acc=1]\u001B[A\n",
      "Training:  93%|█████████▎| 934/1000[, loss=0.000189, val_loss=0.000812, val_acc=1]\u001B[A\n",
      "Training:  94%|█████████▎| 935/1000[, loss=0.000188, val_loss=0.000813, val_acc=1]\u001B[A\n",
      "Training:  94%|█████████▎| 936/1000[, loss=0.000188, val_loss=0.000813, val_acc=1]\u001B[A\n",
      "Training:  94%|█████████▎| 936/1000[, loss=0.000188, val_loss=0.000811, val_acc=1]\u001B[A\n",
      "Training:  94%|█████████▎| 937/1000[, loss=0.000188, val_loss=0.000812, val_acc=1]\u001B[A\n",
      "Training:  94%|█████████▍| 938/1000[, loss=0.000188, val_loss=0.000812, val_acc=1]\u001B[A\n",
      "Training:  94%|█████████▍| 938/1000[, loss=0.000187, val_loss=0.00081, val_acc=1] \u001B[A\n",
      "Training:  94%|█████████▍| 939/1000[, loss=0.000187, val_loss=0.000812, val_acc=1]\u001B[A\n",
      "Training:  94%|█████████▍| 940/1000[, loss=0.000187, val_loss=0.000812, val_acc=1]\u001B[A\n",
      "Training:  94%|█████████▍| 940/1000[, loss=0.000187, val_loss=0.000808, val_acc=1]\u001B[A\n",
      "Training:  94%|█████████▍| 941/1000[, loss=0.000186, val_loss=0.000811, val_acc=1]\u001B[A\n",
      "Training:  94%|█████████▍| 942/1000[, loss=0.000186, val_loss=0.000811, val_acc=1]\u001B[A\n",
      "Training:  94%|█████████▍| 942/1000[, loss=0.000186, val_loss=0.000809, val_acc=1]\u001B[A\n",
      "Training:  94%|█████████▍| 943/1000[, loss=0.000186, val_loss=0.00081, val_acc=1] \u001B[A\n",
      "Training:  94%|█████████▍| 944/1000[, loss=0.000186, val_loss=0.00081, val_acc=1]\u001B[A\n",
      "Training:  94%|█████████▍| 944/1000[, loss=0.000185, val_loss=0.000808, val_acc=1]\u001B[A\n",
      "Training:  94%|█████████▍| 945/1000[, loss=0.000185, val_loss=0.00081, val_acc=1] \u001B[A\n",
      "Training:  95%|█████████▍| 946/1000[, loss=0.000185, val_loss=0.00081, val_acc=1]\u001B[A\n",
      "Training:  95%|█████████▍| 946/1000[, loss=0.000185, val_loss=0.000808, val_acc=1]\u001B[A\n",
      "Training:  95%|█████████▍| 947/1000[, loss=0.000185, val_loss=0.000813, val_acc=1]\u001B[A\n",
      "Training:  95%|█████████▍| 948/1000[, loss=0.000185, val_loss=0.000813, val_acc=1]\u001B[A\n",
      "Training:  95%|█████████▍| 948/1000[, loss=0.000184, val_loss=0.000807, val_acc=1]\u001B[A\n",
      "Training:  95%|█████████▍| 949/1000[, loss=0.000184, val_loss=0.000812, val_acc=1]\u001B[A\n",
      "Training:  95%|█████████▌| 950/1000[, loss=0.000184, val_loss=0.000812, val_acc=1]\u001B[A\n",
      "Training:  95%|█████████▌| 950/1000[, loss=0.000184, val_loss=0.00081, val_acc=1] \u001B[A\n",
      "Training:  95%|█████████▌| 951/1000[, loss=0.000183, val_loss=0.000808, val_acc=1]\u001B[A\n",
      "Training:  95%|█████████▌| 952/1000[, loss=0.000183, val_loss=0.000808, val_acc=1]\u001B[A\n",
      "Training:  95%|█████████▌| 952/1000[, loss=0.000183, val_loss=0.000809, val_acc=1]\u001B[A\n",
      "Training:  95%|█████████▌| 953/1000[, loss=0.000183, val_loss=0.000806, val_acc=1]\u001B[A\n",
      "Training:  95%|█████████▌| 954/1000[, loss=0.000183, val_loss=0.000806, val_acc=1]\u001B[A\n",
      "Training:  95%|█████████▌| 954/1000[, loss=0.000182, val_loss=0.00081, val_acc=1] \u001B[A\n",
      "Training:  96%|█████████▌| 955/1000[, loss=0.000182, val_loss=0.000806, val_acc=1]\u001B[A\n",
      "Training:  96%|█████████▌| 956/1000[, loss=0.000182, val_loss=0.000806, val_acc=1]\u001B[A\n",
      "Training:  96%|█████████▌| 956/1000[, loss=0.000182, val_loss=0.000807, val_acc=1]\u001B[A\n",
      "Training:  96%|█████████▌| 957/1000[, loss=0.000182, val_loss=0.000806, val_acc=1]\u001B[A\n",
      "Training:  96%|█████████▌| 958/1000[, loss=0.000182, val_loss=0.000806, val_acc=1]\u001B[A\n",
      "Training:  96%|█████████▌| 958/1000[, loss=0.000181, val_loss=0.000808, val_acc=1]\u001B[A\n",
      "Training:  96%|█████████▌| 959/1000[, loss=0.000181, val_loss=0.000805, val_acc=1]\u001B[A\n",
      "Training:  96%|█████████▌| 960/1000[, loss=0.000181, val_loss=0.000805, val_acc=1]\u001B[A\n",
      "Training:  96%|█████████▌| 960/1000[, loss=0.000181, val_loss=0.000808, val_acc=1]\u001B[A\n",
      "Training:  96%|█████████▌| 961/1000[, loss=0.00018, val_loss=0.000805, val_acc=1] \u001B[A\n",
      "Training:  96%|█████████▌| 962/1000[, loss=0.00018, val_loss=0.000805, val_acc=1]\u001B[A\n",
      "Training:  96%|█████████▌| 962/1000[, loss=0.00018, val_loss=0.000807, val_acc=1]\u001B[A\n",
      "Training:  96%|█████████▋| 963/1000[, loss=0.00018, val_loss=0.000805, val_acc=1]\u001B[A\n",
      "Training:  96%|█████████▋| 964/1000[, loss=0.00018, val_loss=0.000805, val_acc=1]\u001B[A\n",
      "Training:  96%|█████████▋| 964/1000[, loss=0.000179, val_loss=0.000805, val_acc=1]\u001B[A\n",
      "Training:  96%|█████████▋| 965/1000[, loss=0.000179, val_loss=0.000804, val_acc=1]\u001B[A\n",
      "Training:  97%|█████████▋| 966/1000[, loss=0.000179, val_loss=0.000804, val_acc=1]\u001B[A\n",
      "Training:  97%|█████████▋| 966/1000[, loss=0.000179, val_loss=0.000807, val_acc=1]\u001B[A\n",
      "Training:  97%|█████████▋| 967/1000[, loss=0.000179, val_loss=0.000808, val_acc=1]\u001B[A\n",
      "Training:  97%|█████████▋| 968/1000[, loss=0.000179, val_loss=0.000808, val_acc=1]\u001B[A\n",
      "Training:  97%|█████████▋| 968/1000[, loss=0.000178, val_loss=0.000802, val_acc=1]\u001B[A\n",
      "Training:  97%|█████████▋| 969/1000[, loss=0.000178, val_loss=0.000808, val_acc=1]\u001B[A\n",
      "Training:  97%|█████████▋| 970/1000[, loss=0.000178, val_loss=0.000808, val_acc=1]\u001B[A\n",
      "Training:  97%|█████████▋| 970/1000[, loss=0.000178, val_loss=0.000802, val_acc=1]\u001B[A\n",
      "Training:  97%|█████████▋| 971/1000[, loss=0.000177, val_loss=0.000805, val_acc=1]\u001B[A\n",
      "Training:  97%|█████████▋| 972/1000[, loss=0.000177, val_loss=0.000805, val_acc=1]\u001B[A\n",
      "Training:  97%|█████████▋| 972/1000[, loss=0.000177, val_loss=0.000804, val_acc=1]\u001B[A\n",
      "Training:  97%|█████████▋| 973/1000[, loss=0.000177, val_loss=0.000804, val_acc=1]\u001B[A\n",
      "Training:  97%|█████████▋| 974/1000[, loss=0.000177, val_loss=0.000804, val_acc=1]\u001B[A\n",
      "Training:  97%|█████████▋| 974/1000[, loss=0.000177, val_loss=0.000807, val_acc=1]\u001B[A\n",
      "Training:  98%|█████████▊| 975/1000[, loss=0.000176, val_loss=0.000802, val_acc=1]\u001B[A\n",
      "Training:  98%|█████████▊| 976/1000[, loss=0.000176, val_loss=0.000802, val_acc=1]\u001B[A\n",
      "Training:  98%|█████████▊| 976/1000[, loss=0.000176, val_loss=0.000806, val_acc=1]\u001B[A\n",
      "Training:  98%|█████████▊| 977/1000[, loss=0.000176, val_loss=0.0008, val_acc=1]  \u001B[A\n",
      "Training:  98%|█████████▊| 978/1000[, loss=0.000176, val_loss=0.0008, val_acc=1]\u001B[A\n",
      "Training:  98%|█████████▊| 978/1000[, loss=0.000175, val_loss=0.000805, val_acc=1]\u001B[A\n",
      "Training:  98%|█████████▊| 979/1000[, loss=0.000175, val_loss=0.000805, val_acc=1]\u001B[A\n",
      "Training:  98%|█████████▊| 980/1000[, loss=0.000175, val_loss=0.000805, val_acc=1]\u001B[A\n",
      "Training:  98%|█████████▊| 980/1000[, loss=0.000175, val_loss=0.000801, val_acc=1]\u001B[A\n",
      "Training:  98%|█████████▊| 981/1000[, loss=0.000175, val_loss=0.000804, val_acc=1]\u001B[A\n",
      "Training:  98%|█████████▊| 982/1000[, loss=0.000175, val_loss=0.000804, val_acc=1]\u001B[A\n",
      "Training:  98%|█████████▊| 982/1000[, loss=0.000174, val_loss=0.0008, val_acc=1]  \u001B[A\n",
      "Training:  98%|█████████▊| 983/1000[, loss=0.000174, val_loss=0.000803, val_acc=1]\u001B[A\n",
      "Training:  98%|█████████▊| 984/1000[, loss=0.000174, val_loss=0.000803, val_acc=1]\u001B[A\n",
      "Training:  98%|█████████▊| 984/1000[, loss=0.000174, val_loss=0.0008, val_acc=1]  \u001B[A\n",
      "Training:  98%|█████████▊| 985/1000[, loss=0.000174, val_loss=0.000803, val_acc=1]\u001B[A\n",
      "Training:  99%|█████████▊| 986/1000[, loss=0.000174, val_loss=0.000803, val_acc=1]\u001B[A\n",
      "Training:  99%|█████████▊| 986/1000[, loss=0.000173, val_loss=0.0008, val_acc=1]  \u001B[A\n",
      "Training:  99%|█████████▊| 987/1000[, loss=0.000173, val_loss=0.000803, val_acc=1]\u001B[A\n",
      "Training:  99%|█████████▉| 988/1000[, loss=0.000173, val_loss=0.000803, val_acc=1]\u001B[A\n",
      "Training:  99%|█████████▉| 988/1000[, loss=0.000173, val_loss=0.000799, val_acc=1]\u001B[A\n",
      "Training:  99%|█████████▉| 989/1000[, loss=0.000172, val_loss=0.000802, val_acc=1]\u001B[A\n",
      "Training:  99%|█████████▉| 990/1000[, loss=0.000172, val_loss=0.000802, val_acc=1]\u001B[A\n",
      "Training:  99%|█████████▉| 990/1000[, loss=0.000172, val_loss=0.000799, val_acc=1]\u001B[A\n",
      "Training:  99%|█████████▉| 991/1000[, loss=0.000172, val_loss=0.000803, val_acc=1]\u001B[A\n",
      "Training:  99%|█████████▉| 992/1000[, loss=0.000172, val_loss=0.000803, val_acc=1]\u001B[A\n",
      "Training:  99%|█████████▉| 992/1000[, loss=0.000172, val_loss=0.000798, val_acc=1]\u001B[A\n",
      "Training:  99%|█████████▉| 993/1000[, loss=0.000171, val_loss=0.000803, val_acc=1]\u001B[A\n",
      "Training:  99%|█████████▉| 994/1000[, loss=0.000171, val_loss=0.000803, val_acc=1]\u001B[A\n",
      "Training:  99%|█████████▉| 994/1000[, loss=0.000171, val_loss=0.000802, val_acc=1]\u001B[A\n",
      "Training: 100%|█████████▉| 995/1000[, loss=0.000171, val_loss=0.0008, val_acc=1]  \u001B[A\n",
      "Training: 100%|█████████▉| 996/1000[, loss=0.000171, val_loss=0.0008, val_acc=1]\u001B[A\n",
      "Training: 100%|█████████▉| 996/1000[, loss=0.00017, val_loss=0.0008, val_acc=1] \u001B[A\n",
      "Training: 100%|█████████▉| 997/1000[, loss=0.00017, val_loss=0.000798, val_acc=1]\u001B[A\n",
      "Training: 100%|█████████▉| 998/1000[, loss=0.00017, val_loss=0.000798, val_acc=1]\u001B[A\n",
      "Training: 100%|█████████▉| 998/1000[, loss=0.00017, val_loss=0.0008, val_acc=1]  \u001B[A\n",
      "Training: 100%|█████████▉| 999/1000[, loss=0.00017, val_loss=0.000798, val_acc=1]\u001B[A\n",
      "Training: 100%|██████████| 1000/1000[, loss=0.00017, val_loss=0.000798, val_acc=1]\u001B[A\n"
     ]
    }
   ],
   "source": [
    "model = Network(17)\n",
    "model.add_layer(6, ReLU())\n",
    "model.add_layer(1, Tanh())\n",
    "stats = model.train((x_train, y_train), (x_test, y_test), metric=BinaryAccuracy(), loss=MeanSquaredError(), epochs=1000)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "<AxesSubplot: >"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 1500x500 with 2 Axes>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "sns.lineplot(stats['train_loss'], ax=axs[0])\n",
    "sns.lineplot(stats['val_loss'], ax=axs[0])\n",
    "sns.lineplot(stats['train_acc'], ax=axs[1])\n",
    "sns.lineplot(stats['val_acc'], ax=axs[1])\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = load_mnist()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = Network(64)\n",
    "model.add_layer(16, ReLU())\n",
    "model.add_layer(10, Sigmoid(0.001))\n",
    "stats = model.train(x_train, y_train, x_test, y_test, metric=MulticlassAccuracy(), loss=MeanSquaredError(), epochs=500)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "sns.lineplot(stats['train_loss'], ax=axs[0])\n",
    "sns.lineplot(stats['val_loss'], ax=axs[0])\n",
    "sns.lineplot(stats['train_acc'], ax=axs[1])\n",
    "sns.lineplot(stats['val_acc'], ax=axs[1])"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
