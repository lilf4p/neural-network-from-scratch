{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "sys.path.append('../src/')\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from src.utils import load_moons, load_monk1, load_mnist\n",
    "from src.network import Network\n",
    "from src.activations import ReLU, Tanh, Sigmoid\n",
    "from src.losses import MeanSquaredError\n",
    "from src.metrics import BinaryAccuracy, MulticlassAccuracy, MeanEuclideanError\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "x_train, x_val, x_test, y_train, y_val, y_test = load_moons(validation=True, noise=0.2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training:   9%|▊         | 43/500[, loss=0.0772, val_loss=0.0742, val_acc=0.9]\u001B[A\n",
      "Training:   9%|▉         | 44/500[, loss=0.0772, val_loss=0.0742, val_acc=0.9]\u001B[A\n",
      "Training:   9%|▉         | 44/500[, loss=0.0762, val_loss=0.0729, val_acc=0.92]\u001B[A\n",
      "Training:   9%|▉         | 45/500[, loss=0.0762, val_loss=0.0729, val_acc=0.92]\u001B[A\n",
      "Training:   9%|▉         | 45/500[, loss=0.0751, val_loss=0.072, val_acc=0.92] \u001B[A\n",
      "Training:   9%|▉         | 46/500[, loss=0.0751, val_loss=0.072, val_acc=0.92]\u001B[A\n",
      "Training:   9%|▉         | 46/500[, loss=0.074, val_loss=0.0709, val_acc=0.92]\u001B[A\n",
      "Training:   9%|▉         | 47/500[, loss=0.074, val_loss=0.0709, val_acc=0.92]\u001B[A\n",
      "Training:   9%|▉         | 47/500[, loss=0.0728, val_loss=0.0696, val_acc=0.92]\u001B[A\n",
      "Training:  10%|▉         | 48/500[, loss=0.0728, val_loss=0.0696, val_acc=0.92]\u001B[A\n",
      "Training:  10%|▉         | 48/500[, loss=0.0714, val_loss=0.0683, val_acc=0.92]\u001B[A\n",
      "Training:  10%|▉         | 49/500[, loss=0.0714, val_loss=0.0683, val_acc=0.92]\u001B[A\n",
      "Training:  10%|▉         | 49/500[, loss=0.0701, val_loss=0.067, val_acc=0.92] \u001B[A\n",
      "Training:  10%|█         | 50/500[, loss=0.0701, val_loss=0.067, val_acc=0.92]\u001B[A\n",
      "Training:  10%|█         | 50/500[, loss=0.0687, val_loss=0.0656, val_acc=0.92]\u001B[A\n",
      "Training:  10%|█         | 51/500[, loss=0.0687, val_loss=0.0656, val_acc=0.92]\u001B[A\n",
      "Training:  10%|█         | 51/500[, loss=0.067, val_loss=0.0638, val_acc=0.92] \u001B[A\n",
      "Training:  10%|█         | 52/500[, loss=0.067, val_loss=0.0638, val_acc=0.92]\u001B[A\n",
      "Training:  10%|█         | 52/500[, loss=0.065, val_loss=0.0622, val_acc=0.92]\u001B[A\n",
      "Training:  11%|█         | 53/500[, loss=0.065, val_loss=0.0622, val_acc=0.92]\u001B[A\n",
      "Training:  11%|█         | 53/500[, loss=0.0624, val_loss=0.0602, val_acc=0.9]\u001B[A\n",
      "Training:  11%|█         | 54/500[, loss=0.0624, val_loss=0.0602, val_acc=0.9]\u001B[A\n",
      "Training:  11%|█         | 54/500[, loss=0.0598, val_loss=0.0588, val_acc=0.9]\u001B[A\n",
      "Training:  11%|█         | 55/500[, loss=0.0598, val_loss=0.0588, val_acc=0.9]\u001B[A\n",
      "Training:  11%|█         | 55/500[, loss=0.0577, val_loss=0.057, val_acc=0.94]\u001B[A\n",
      "Training:  11%|█         | 56/500[, loss=0.0577, val_loss=0.057, val_acc=0.94]\u001B[A\n",
      "Training:  11%|█         | 56/500[, loss=0.056, val_loss=0.0555, val_acc=0.94]\u001B[A\n",
      "Training:  11%|█▏        | 57/500[, loss=0.056, val_loss=0.0555, val_acc=0.94]\u001B[A\n",
      "Training:  11%|█▏        | 57/500[, loss=0.0545, val_loss=0.0542, val_acc=0.94]\u001B[A\n",
      "Training:  12%|█▏        | 58/500[, loss=0.0545, val_loss=0.0542, val_acc=0.94]\u001B[A\n",
      "Training:  12%|█▏        | 58/500[, loss=0.0532, val_loss=0.053, val_acc=0.94] \u001B[A\n",
      "Training:  12%|█▏        | 59/500[, loss=0.0532, val_loss=0.053, val_acc=0.94]\u001B[A\n",
      "Training:  12%|█▏        | 59/500[, loss=0.0522, val_loss=0.0521, val_acc=0.94]\u001B[A\n",
      "Training:  12%|█▏        | 60/500[, loss=0.0522, val_loss=0.0521, val_acc=0.94]\u001B[A\n",
      "Training:  12%|█▏        | 60/500[, loss=0.0514, val_loss=0.051, val_acc=0.96] \u001B[A\n",
      "Training:  12%|█▏        | 61/500[, loss=0.0514, val_loss=0.051, val_acc=0.96]\u001B[A\n",
      "Training:  12%|█▏        | 61/500[, loss=0.0506, val_loss=0.0503, val_acc=0.96]\u001B[A\n",
      "Training:  12%|█▏        | 62/500[, loss=0.0506, val_loss=0.0503, val_acc=0.96]\u001B[A\n",
      "Training:  12%|█▏        | 62/500[, loss=0.05, val_loss=0.0498, val_acc=0.96]  \u001B[A\n",
      "Training:  13%|█▎        | 63/500[, loss=0.05, val_loss=0.0498, val_acc=0.96]\u001B[A\n",
      "Training:  13%|█▎        | 63/500[, loss=0.0493, val_loss=0.0493, val_acc=0.96]\u001B[A\n",
      "Training:  13%|█▎        | 64/500[, loss=0.0493, val_loss=0.0493, val_acc=0.96]\u001B[A\n",
      "Training:  13%|█▎        | 64/500[, loss=0.0488, val_loss=0.0487, val_acc=0.96]\u001B[A\n",
      "Training:  13%|█▎        | 65/500[, loss=0.0488, val_loss=0.0487, val_acc=0.96]\u001B[A\n",
      "Training:  13%|█▎        | 65/500[, loss=0.0483, val_loss=0.0485, val_acc=0.96]\u001B[A\n",
      "Training:  13%|█▎        | 66/500[, loss=0.0483, val_loss=0.0485, val_acc=0.96]\u001B[A\n",
      "Training:  13%|█▎        | 66/500[, loss=0.0479, val_loss=0.0481, val_acc=0.96]\u001B[A\n",
      "Training:  13%|█▎        | 67/500[, loss=0.0479, val_loss=0.0481, val_acc=0.96]\u001B[A\n",
      "Training:  13%|█▎        | 67/500[, loss=0.0475, val_loss=0.0478, val_acc=0.96]\u001B[A\n",
      "Training:  14%|█▎        | 68/500[, loss=0.0475, val_loss=0.0478, val_acc=0.96]\u001B[A\n",
      "Training:  14%|█▎        | 68/500[, loss=0.0471, val_loss=0.0475, val_acc=0.96]\u001B[A\n",
      "Training:  14%|█▍        | 69/500[, loss=0.0471, val_loss=0.0475, val_acc=0.96]\u001B[A\n",
      "Training:  14%|█▍        | 69/500[, loss=0.0468, val_loss=0.0476, val_acc=0.96]\u001B[A\n",
      "Training:  14%|█▍        | 70/500[, loss=0.0468, val_loss=0.0476, val_acc=0.96]\u001B[A\n",
      "Training:  14%|█▍        | 70/500[, loss=0.0465, val_loss=0.0476, val_acc=0.96]\u001B[A\n",
      "Training:  14%|█▍        | 71/500[, loss=0.0465, val_loss=0.0476, val_acc=0.96]\u001B[A\n",
      "Training:  14%|█▍        | 71/500[, loss=0.0461, val_loss=0.0475, val_acc=0.96]\u001B[A\n",
      "Training:  14%|█▍        | 72/500[, loss=0.0461, val_loss=0.0475, val_acc=0.96]\u001B[A\n",
      "Training:  14%|█▍        | 72/500[, loss=0.0459, val_loss=0.0474, val_acc=0.96]\u001B[A\n",
      "Training:  15%|█▍        | 73/500[, loss=0.0459, val_loss=0.0474, val_acc=0.96]\u001B[A\n",
      "Training:  15%|█▍        | 73/500[, loss=0.0456, val_loss=0.0472, val_acc=0.96]\u001B[A\n",
      "Training:  15%|█▍        | 74/500[, loss=0.0456, val_loss=0.0472, val_acc=0.96]\u001B[A\n",
      "Training:  15%|█▍        | 74/500[, loss=0.0453, val_loss=0.0471, val_acc=0.96]\u001B[A\n",
      "Training:  15%|█▌        | 75/500[, loss=0.0453, val_loss=0.0471, val_acc=0.96]\u001B[A\n",
      "Training:  15%|█▌        | 75/500[, loss=0.045, val_loss=0.0471, val_acc=0.96] \u001B[A\n",
      "Training:  15%|█▌        | 76/500[, loss=0.045, val_loss=0.0471, val_acc=0.96]\u001B[A\n",
      "Training:  15%|█▌        | 76/500[, loss=0.0448, val_loss=0.0465, val_acc=0.96]\u001B[A\n",
      "Training:  15%|█▌        | 77/500[, loss=0.0448, val_loss=0.0465, val_acc=0.96]\u001B[A\n",
      "Training:  15%|█▌        | 77/500[, loss=0.0445, val_loss=0.0463, val_acc=0.96]\u001B[A\n",
      "Training:  16%|█▌        | 78/500[, loss=0.0445, val_loss=0.0463, val_acc=0.96]\u001B[A\n",
      "Training:  16%|█▌        | 78/500[, loss=0.0443, val_loss=0.0462, val_acc=0.96]\u001B[A\n",
      "Training:  16%|█▌        | 79/500[, loss=0.0443, val_loss=0.0462, val_acc=0.96]\u001B[A\n",
      "Training:  16%|█▌        | 79/500[, loss=0.0441, val_loss=0.0461, val_acc=0.96]\u001B[A\n",
      "Training:  16%|█▌        | 80/500[, loss=0.0441, val_loss=0.0461, val_acc=0.96]\u001B[A\n",
      "Training:  16%|█▌        | 80/500[, loss=0.0439, val_loss=0.0458, val_acc=0.96]\u001B[A\n",
      "Training:  16%|█▌        | 81/500[, loss=0.0439, val_loss=0.0458, val_acc=0.96]\u001B[A\n",
      "Training:  16%|█▌        | 81/500[, loss=0.0437, val_loss=0.0458, val_acc=0.96]\u001B[A\n",
      "Training:  16%|█▋        | 82/500[, loss=0.0437, val_loss=0.0458, val_acc=0.96]\u001B[A\n",
      "Training:  16%|█▋        | 82/500[, loss=0.0435, val_loss=0.0456, val_acc=0.96]\u001B[A\n",
      "Training:  17%|█▋        | 83/500[, loss=0.0435, val_loss=0.0456, val_acc=0.96]\u001B[A\n",
      "Training:  17%|█▋        | 83/500[, loss=0.0433, val_loss=0.0455, val_acc=0.96]\u001B[A\n",
      "Training:  17%|█▋        | 84/500[, loss=0.0433, val_loss=0.0455, val_acc=0.96]\u001B[A\n",
      "Training:  17%|█▋        | 84/500[, loss=0.0431, val_loss=0.0453, val_acc=0.96]\u001B[A\n",
      "Training:  17%|█▋        | 85/500[, loss=0.0431, val_loss=0.0453, val_acc=0.96]\u001B[A\n",
      "Training:  17%|█▋        | 85/500[, loss=0.043, val_loss=0.0453, val_acc=0.96] \u001B[A\n",
      "Training:  17%|█▋        | 86/500[, loss=0.043, val_loss=0.0453, val_acc=0.96]\u001B[A\n",
      "Training:  17%|█▋        | 86/500[, loss=0.0428, val_loss=0.045, val_acc=0.96]\u001B[A\n",
      "Training:  17%|█▋        | 87/500[, loss=0.0428, val_loss=0.045, val_acc=0.96]\u001B[A\n",
      "Training:  17%|█▋        | 87/500[, loss=0.0427, val_loss=0.0448, val_acc=0.96]\u001B[A\n",
      "Training:  18%|█▊        | 88/500[, loss=0.0427, val_loss=0.0448, val_acc=0.96]\u001B[A\n",
      "Training:  18%|█▊        | 88/500[, loss=0.0425, val_loss=0.0447, val_acc=0.96]\u001B[A\n",
      "Training:  18%|█▊        | 89/500[, loss=0.0425, val_loss=0.0447, val_acc=0.96]\u001B[A\n",
      "Training:  18%|█▊        | 89/500[, loss=0.0424, val_loss=0.0446, val_acc=0.96]\u001B[A\n",
      "Training:  18%|█▊        | 90/500[, loss=0.0424, val_loss=0.0446, val_acc=0.96]\u001B[A\n",
      "Training:  18%|█▊        | 90/500[, loss=0.0422, val_loss=0.0444, val_acc=0.96]\u001B[A\n",
      "Training:  18%|█▊        | 91/500[, loss=0.0422, val_loss=0.0444, val_acc=0.96]\u001B[A\n",
      "Training:  18%|█▊        | 91/500[, loss=0.0421, val_loss=0.0443, val_acc=0.96]\u001B[A\n",
      "Training:  18%|█▊        | 92/500[, loss=0.0421, val_loss=0.0443, val_acc=0.96]\u001B[A\n",
      "Training:  18%|█▊        | 92/500[, loss=0.0419, val_loss=0.0441, val_acc=0.96]\u001B[A\n",
      "Training:  19%|█▊        | 93/500[, loss=0.0419, val_loss=0.0441, val_acc=0.96]\u001B[A\n",
      "Training:  19%|█▊        | 93/500[, loss=0.0418, val_loss=0.0439, val_acc=0.96]\u001B[A\n",
      "Training:  19%|█▉        | 94/500[, loss=0.0418, val_loss=0.0439, val_acc=0.96]\u001B[A\n",
      "Training:  19%|█▉        | 94/500[, loss=0.0417, val_loss=0.0438, val_acc=0.96]\u001B[A\n",
      "Training:  19%|█▉        | 95/500[, loss=0.0417, val_loss=0.0438, val_acc=0.96]\u001B[A\n",
      "Training:  19%|█▉        | 95/500[, loss=0.0415, val_loss=0.0436, val_acc=0.96]\u001B[A\n",
      "Training:  19%|█▉        | 96/500[, loss=0.0415, val_loss=0.0436, val_acc=0.96]\u001B[A\n",
      "Training:  19%|█▉        | 96/500[, loss=0.0414, val_loss=0.0434, val_acc=0.96]\u001B[A\n",
      "Training:  19%|█▉        | 97/500[, loss=0.0414, val_loss=0.0434, val_acc=0.96]\u001B[A\n",
      "Training:  19%|█▉        | 97/500[, loss=0.0413, val_loss=0.0433, val_acc=0.96]\u001B[A\n",
      "Training:  20%|█▉        | 98/500[, loss=0.0413, val_loss=0.0433, val_acc=0.96]\u001B[A\n",
      "Training:  20%|█▉        | 98/500[, loss=0.0412, val_loss=0.0432, val_acc=0.96]\u001B[A\n",
      "Training:  20%|█▉        | 99/500[, loss=0.0412, val_loss=0.0432, val_acc=0.96]\u001B[A\n",
      "Training:  20%|█▉        | 99/500[, loss=0.0411, val_loss=0.0432, val_acc=0.96]\u001B[A\n",
      "Training:  20%|██        | 100/500[, loss=0.0411, val_loss=0.0432, val_acc=0.96]\u001B[A\n",
      "Training:  20%|██        | 100/500[, loss=0.041, val_loss=0.043, val_acc=0.96]  \u001B[A\n",
      "Training:  20%|██        | 101/500[, loss=0.041, val_loss=0.043, val_acc=0.96]\u001B[A\n",
      "Training:  20%|██        | 101/500[, loss=0.0409, val_loss=0.043, val_acc=0.96]\u001B[A\n",
      "Training:  20%|██        | 102/500[, loss=0.0409, val_loss=0.043, val_acc=0.96]\u001B[A\n",
      "Training:  20%|██        | 102/500[, loss=0.0408, val_loss=0.0429, val_acc=0.96]\u001B[A\n",
      "Training:  21%|██        | 103/500[, loss=0.0408, val_loss=0.0429, val_acc=0.96]\u001B[A\n",
      "Training:  21%|██        | 103/500[, loss=0.0408, val_loss=0.0429, val_acc=0.96]\u001B[A\n",
      "Training:  21%|██        | 104/500[, loss=0.0408, val_loss=0.0429, val_acc=0.96]\u001B[A\n",
      "Training:  21%|██        | 104/500[, loss=0.0407, val_loss=0.0428, val_acc=0.96]\u001B[A\n",
      "Training:  21%|██        | 105/500[, loss=0.0407, val_loss=0.0428, val_acc=0.96]\u001B[A\n",
      "Training:  21%|██        | 105/500[, loss=0.0406, val_loss=0.0428, val_acc=0.96]\u001B[A\n",
      "Training:  21%|██        | 106/500[, loss=0.0406, val_loss=0.0428, val_acc=0.96]\u001B[A\n",
      "Training:  21%|██        | 106/500[, loss=0.0405, val_loss=0.0428, val_acc=0.96]\u001B[A\n",
      "Training:  21%|██▏       | 107/500[, loss=0.0405, val_loss=0.0428, val_acc=0.96]\u001B[A\n",
      "Training:  21%|██▏       | 107/500[, loss=0.0405, val_loss=0.0427, val_acc=0.96]\u001B[A\n",
      "Training:  22%|██▏       | 108/500[, loss=0.0405, val_loss=0.0427, val_acc=0.96]\u001B[A\n",
      "Training:  22%|██▏       | 108/500[, loss=0.0404, val_loss=0.0426, val_acc=0.96]\u001B[A\n",
      "Training:  22%|██▏       | 109/500[, loss=0.0404, val_loss=0.0426, val_acc=0.96]\u001B[A\n",
      "Training:  22%|██▏       | 109/500[, loss=0.0404, val_loss=0.0425, val_acc=0.96]\u001B[A\n",
      "Training:  22%|██▏       | 110/500[, loss=0.0404, val_loss=0.0425, val_acc=0.96]\u001B[A\n",
      "Training:  22%|██▏       | 110/500[, loss=0.0403, val_loss=0.0425, val_acc=0.96]\u001B[A\n",
      "Training:  22%|██▏       | 111/500[, loss=0.0403, val_loss=0.0425, val_acc=0.96]\u001B[A\n",
      "Training:  22%|██▏       | 111/500[, loss=0.0402, val_loss=0.0425, val_acc=0.96]\u001B[A\n",
      "Training:  22%|██▏       | 112/500[, loss=0.0402, val_loss=0.0425, val_acc=0.96]\u001B[A\n",
      "Training:  22%|██▏       | 112/500[, loss=0.0402, val_loss=0.0425, val_acc=0.96]\u001B[A\n",
      "Training:  23%|██▎       | 113/500[, loss=0.0402, val_loss=0.0425, val_acc=0.96]\u001B[A\n",
      "Training:  23%|██▎       | 113/500[, loss=0.0401, val_loss=0.0425, val_acc=0.96]\u001B[A\n",
      "Training:  23%|██▎       | 114/500[, loss=0.0401, val_loss=0.0425, val_acc=0.96]\u001B[A\n",
      "Training:  23%|██▎       | 114/500[, loss=0.0401, val_loss=0.0425, val_acc=0.96]\u001B[A\n",
      "Training:  23%|██▎       | 115/500[, loss=0.0401, val_loss=0.0425, val_acc=0.96]\u001B[A\n",
      "Training:  23%|██▎       | 115/500[, loss=0.04, val_loss=0.0425, val_acc=0.96]  \u001B[A\n",
      "Training:  23%|██▎       | 116/500[, loss=0.04, val_loss=0.0425, val_acc=0.96]\u001B[A\n",
      "Training:  23%|██▎       | 116/500[, loss=0.04, val_loss=0.0424, val_acc=0.96]\u001B[A\n",
      "Training:  23%|██▎       | 117/500[, loss=0.04, val_loss=0.0424, val_acc=0.96]\u001B[A\n",
      "Training:  23%|██▎       | 117/500[, loss=0.0399, val_loss=0.0423, val_acc=0.96]\u001B[A\n",
      "Training:  24%|██▎       | 118/500[, loss=0.0399, val_loss=0.0423, val_acc=0.96]\u001B[A\n",
      "Training:  24%|██▎       | 118/500[, loss=0.0399, val_loss=0.0423, val_acc=0.96]\u001B[A\n",
      "Training:  24%|██▍       | 119/500[, loss=0.0399, val_loss=0.0423, val_acc=0.96]\u001B[A\n",
      "Training:  24%|██▍       | 119/500[, loss=0.0398, val_loss=0.0422, val_acc=0.96]\u001B[A\n",
      "Training:  24%|██▍       | 120/500[, loss=0.0398, val_loss=0.0422, val_acc=0.96]\u001B[A\n",
      "Training:  24%|██▍       | 120/500[, loss=0.0398, val_loss=0.0422, val_acc=0.96]\u001B[A\n",
      "Training:  24%|██▍       | 121/500[, loss=0.0398, val_loss=0.0422, val_acc=0.96]\u001B[A\n",
      "Training:  24%|██▍       | 121/500[, loss=0.0397, val_loss=0.0422, val_acc=0.96]\u001B[A\n",
      "Training:  24%|██▍       | 122/500[, loss=0.0397, val_loss=0.0422, val_acc=0.96]\u001B[A\n",
      "Training:  24%|██▍       | 122/500[, loss=0.0397, val_loss=0.0422, val_acc=0.96]\u001B[A\n",
      "Training:  25%|██▍       | 123/500[, loss=0.0397, val_loss=0.0422, val_acc=0.96]\u001B[A\n",
      "Training:  25%|██▍       | 123/500[, loss=0.0397, val_loss=0.0422, val_acc=0.96]\u001B[A\n",
      "Training:  25%|██▍       | 124/500[, loss=0.0397, val_loss=0.0422, val_acc=0.96]\u001B[A\n",
      "Training:  25%|██▍       | 124/500[, loss=0.0396, val_loss=0.042, val_acc=0.96] \u001B[A\n",
      "Training:  25%|██▌       | 125/500[, loss=0.0396, val_loss=0.042, val_acc=0.96]\u001B[A\n",
      "Training:  25%|██▌       | 125/500[, loss=0.0396, val_loss=0.0421, val_acc=0.96]\u001B[A\n",
      "Training:  25%|██▌       | 126/500[, loss=0.0396, val_loss=0.0421, val_acc=0.96]\u001B[A\n",
      "Training:  25%|██▌       | 126/500[, loss=0.0396, val_loss=0.0421, val_acc=0.96]\u001B[A\n",
      "Training:  25%|██▌       | 127/500[, loss=0.0396, val_loss=0.0421, val_acc=0.96]\u001B[A\n",
      "Training:  25%|██▌       | 127/500[, loss=0.0395, val_loss=0.0421, val_acc=0.96]\u001B[A\n",
      "Training:  26%|██▌       | 128/500[, loss=0.0395, val_loss=0.0421, val_acc=0.96]\u001B[A\n",
      "Training:  26%|██▌       | 128/500[, loss=0.0395, val_loss=0.0418, val_acc=0.96]\u001B[A\n",
      "Training:  26%|██▌       | 129/500[, loss=0.0395, val_loss=0.0418, val_acc=0.96]\u001B[A\n",
      "Training:  26%|██▌       | 129/500[, loss=0.0394, val_loss=0.0418, val_acc=0.96]\u001B[A\n",
      "Training:  26%|██▌       | 130/500[, loss=0.0394, val_loss=0.0418, val_acc=0.96]\u001B[A\n",
      "Training:  26%|██▌       | 130/500[, loss=0.0394, val_loss=0.0417, val_acc=0.96]\u001B[A\n",
      "Training:  26%|██▌       | 131/500[, loss=0.0394, val_loss=0.0417, val_acc=0.96]\u001B[A\n",
      "Training:  26%|██▌       | 131/500[, loss=0.0394, val_loss=0.0417, val_acc=0.96]\u001B[A\n",
      "Training:  26%|██▋       | 132/500[, loss=0.0394, val_loss=0.0417, val_acc=0.96]\u001B[A\n",
      "Training:  26%|██▋       | 132/500[, loss=0.0394, val_loss=0.0417, val_acc=0.96]\u001B[A\n",
      "Training:  27%|██▋       | 133/500[, loss=0.0394, val_loss=0.0417, val_acc=0.96]\u001B[A\n",
      "Training:  27%|██▋       | 133/500[, loss=0.0393, val_loss=0.0418, val_acc=0.96]\u001B[A\n",
      "Training:  27%|██▋       | 134/500[, loss=0.0393, val_loss=0.0418, val_acc=0.96]\u001B[A\n",
      "Training:  27%|██▋       | 134/500[, loss=0.0393, val_loss=0.0417, val_acc=0.96]\u001B[A\n",
      "Training:  27%|██▋       | 135/500[, loss=0.0393, val_loss=0.0417, val_acc=0.96]\u001B[A\n",
      "Training:  27%|██▋       | 135/500[, loss=0.0393, val_loss=0.0418, val_acc=0.96]\u001B[A\n",
      "Training:  27%|██▋       | 136/500[, loss=0.0393, val_loss=0.0418, val_acc=0.96]\u001B[A\n",
      "Training:  27%|██▋       | 136/500[, loss=0.0393, val_loss=0.0418, val_acc=0.96]\u001B[A\n",
      "Training:  27%|██▋       | 137/500[, loss=0.0393, val_loss=0.0418, val_acc=0.96]\u001B[A\n",
      "Training:  27%|██▋       | 137/500[, loss=0.0393, val_loss=0.0417, val_acc=0.96]\u001B[A\n",
      "Training:  28%|██▊       | 138/500[, loss=0.0393, val_loss=0.0417, val_acc=0.96]\u001B[A\n",
      "Training:  28%|██▊       | 138/500[, loss=0.0392, val_loss=0.0417, val_acc=0.96]\u001B[A\n",
      "Training:  28%|██▊       | 139/500[, loss=0.0392, val_loss=0.0417, val_acc=0.96]\u001B[A\n",
      "Training:  28%|██▊       | 139/500[, loss=0.0392, val_loss=0.0418, val_acc=0.96]\u001B[A\n",
      "Training:  28%|██▊       | 140/500[, loss=0.0392, val_loss=0.0418, val_acc=0.96]\u001B[A\n",
      "Training:  28%|██▊       | 140/500[, loss=0.0392, val_loss=0.0417, val_acc=0.96]\u001B[A\n",
      "Training:  28%|██▊       | 141/500[, loss=0.0392, val_loss=0.0417, val_acc=0.96]\u001B[A\n",
      "Training:  28%|██▊       | 141/500[, loss=0.0392, val_loss=0.0417, val_acc=0.96]\u001B[A\n",
      "Training:  28%|██▊       | 142/500[, loss=0.0392, val_loss=0.0417, val_acc=0.96]\u001B[A\n",
      "Training:  28%|██▊       | 142/500[, loss=0.0392, val_loss=0.0417, val_acc=0.96]\u001B[A\n",
      "Training:  29%|██▊       | 143/500[, loss=0.0392, val_loss=0.0417, val_acc=0.96]\u001B[A\n",
      "Training:  29%|██▊       | 143/500[, loss=0.0391, val_loss=0.0417, val_acc=0.96]\u001B[A\n",
      "Training:  29%|██▉       | 144/500[, loss=0.0391, val_loss=0.0417, val_acc=0.96]\u001B[A\n",
      "Training:  29%|██▉       | 144/500[, loss=0.0391, val_loss=0.0416, val_acc=0.96]\u001B[A\n",
      "Training:  29%|██▉       | 145/500[, loss=0.0391, val_loss=0.0416, val_acc=0.96]\u001B[A\n",
      "Training:  29%|██▉       | 145/500[, loss=0.0391, val_loss=0.0416, val_acc=0.96]\u001B[A\n",
      "Training:  29%|██▉       | 146/500[, loss=0.0391, val_loss=0.0416, val_acc=0.96]\u001B[A\n",
      "Training:  29%|██▉       | 146/500[, loss=0.0391, val_loss=0.0416, val_acc=0.96]\u001B[A\n",
      "Training:  29%|██▉       | 147/500[, loss=0.0391, val_loss=0.0416, val_acc=0.96]\u001B[A\n",
      "Training:  29%|██▉       | 147/500[, loss=0.0391, val_loss=0.0415, val_acc=0.96]\u001B[A\n",
      "Training:  30%|██▉       | 148/500[, loss=0.0391, val_loss=0.0415, val_acc=0.96]\u001B[A\n",
      "Training:  30%|██▉       | 148/500[, loss=0.039, val_loss=0.0415, val_acc=0.96] \u001B[A\n",
      "Training:  30%|██▉       | 149/500[, loss=0.039, val_loss=0.0415, val_acc=0.96]\u001B[A\n",
      "Training:  30%|██▉       | 149/500[, loss=0.039, val_loss=0.0415, val_acc=0.96]\u001B[A\n",
      "Training:  30%|███       | 150/500[, loss=0.039, val_loss=0.0415, val_acc=0.96]\u001B[A\n",
      "Training:  30%|███       | 150/500[, loss=0.039, val_loss=0.0415, val_acc=0.96]\u001B[A\n",
      "Training:  30%|███       | 151/500[, loss=0.039, val_loss=0.0415, val_acc=0.96]\u001B[A\n",
      "Training:  30%|███       | 151/500[, loss=0.039, val_loss=0.0415, val_acc=0.96]\u001B[A\n",
      "Training:  30%|███       | 152/500[, loss=0.039, val_loss=0.0415, val_acc=0.96]\u001B[A\n",
      "Training:  30%|███       | 152/500[, loss=0.039, val_loss=0.0415, val_acc=0.96]\u001B[A\n",
      "Training:  31%|███       | 153/500[, loss=0.039, val_loss=0.0415, val_acc=0.96]\u001B[A\n",
      "Training:  31%|███       | 153/500[, loss=0.039, val_loss=0.0415, val_acc=0.96]\u001B[A\n",
      "Training:  31%|███       | 154/500[, loss=0.039, val_loss=0.0415, val_acc=0.96]\u001B[A\n",
      "Training:  31%|███       | 154/500[, loss=0.039, val_loss=0.0414, val_acc=0.96]\u001B[A\n",
      "Training:  31%|███       | 155/500[, loss=0.039, val_loss=0.0414, val_acc=0.96]\u001B[A\n",
      "Training:  31%|███       | 155/500[, loss=0.0389, val_loss=0.0414, val_acc=0.96]\u001B[A\n",
      "Training:  31%|███       | 156/500[, loss=0.0389, val_loss=0.0414, val_acc=0.96]\u001B[A\n",
      "Training:  31%|███       | 156/500[, loss=0.0389, val_loss=0.0414, val_acc=0.96]\u001B[A\n",
      "Training:  31%|███▏      | 157/500[, loss=0.0389, val_loss=0.0414, val_acc=0.96]\u001B[A\n",
      "Training:  31%|███▏      | 157/500[, loss=0.0389, val_loss=0.0414, val_acc=0.96]\u001B[A\n",
      "Training:  32%|███▏      | 158/500[, loss=0.0389, val_loss=0.0414, val_acc=0.96]\u001B[A\n",
      "Training:  32%|███▏      | 158/500[, loss=0.0389, val_loss=0.0414, val_acc=0.96]\u001B[A\n",
      "Training:  32%|███▏      | 159/500[, loss=0.0389, val_loss=0.0414, val_acc=0.96]\u001B[A\n",
      "Training:  32%|███▏      | 159/500[, loss=0.0389, val_loss=0.0414, val_acc=0.96]\u001B[A\n",
      "Training:  32%|███▏      | 160/500[, loss=0.0389, val_loss=0.0414, val_acc=0.96]\u001B[A\n",
      "Training:  32%|███▏      | 160/500[, loss=0.0389, val_loss=0.0413, val_acc=0.96]\u001B[A\n",
      "Training:  32%|███▏      | 161/500[, loss=0.0389, val_loss=0.0413, val_acc=0.96]\u001B[A\n",
      "Training:  32%|███▏      | 161/500[, loss=0.0389, val_loss=0.0412, val_acc=0.96]\u001B[A\n",
      "Training:  32%|███▏      | 162/500[, loss=0.0389, val_loss=0.0412, val_acc=0.96]\u001B[A\n",
      "Training:  32%|███▏      | 162/500[, loss=0.0389, val_loss=0.0412, val_acc=0.96]\u001B[A\n",
      "Training:  33%|███▎      | 163/500[, loss=0.0389, val_loss=0.0412, val_acc=0.96]\u001B[A\n",
      "Training:  33%|███▎      | 163/500[, loss=0.0389, val_loss=0.0411, val_acc=0.96]\u001B[A\n",
      "Training:  33%|███▎      | 164/500[, loss=0.0389, val_loss=0.0411, val_acc=0.96]\u001B[A\n",
      "Training:  33%|███▎      | 164/500[, loss=0.0388, val_loss=0.0411, val_acc=0.96]\u001B[A\n",
      "Training:  33%|███▎      | 165/500[, loss=0.0388, val_loss=0.0411, val_acc=0.96]\u001B[A\n",
      "Training:  33%|███▎      | 165/500[, loss=0.0388, val_loss=0.0411, val_acc=0.96]\u001B[A\n",
      "Training:  33%|███▎      | 166/500[, loss=0.0388, val_loss=0.0411, val_acc=0.96]\u001B[A\n",
      "Training:  33%|███▎      | 166/500[, loss=0.0388, val_loss=0.0411, val_acc=0.96]\u001B[A\n",
      "Training:  33%|███▎      | 167/500[, loss=0.0388, val_loss=0.0411, val_acc=0.96]\u001B[A\n",
      "Training:  33%|███▎      | 167/500[, loss=0.0388, val_loss=0.0411, val_acc=0.96]\u001B[A\n",
      "Training:  34%|███▎      | 168/500[, loss=0.0388, val_loss=0.0411, val_acc=0.96]\u001B[A\n",
      "Training:  34%|███▎      | 168/500[, loss=0.0388, val_loss=0.0411, val_acc=0.96]\u001B[A\n",
      "Training:  34%|███▍      | 169/500[, loss=0.0388, val_loss=0.0411, val_acc=0.96]\u001B[A\n",
      "Training:  34%|███▍      | 169/500[, loss=0.0388, val_loss=0.0411, val_acc=0.96]\u001B[A\n",
      "Training:  34%|███▍      | 170/500[, loss=0.0388, val_loss=0.0411, val_acc=0.96]\u001B[A\n",
      "Training:  34%|███▍      | 170/500[, loss=0.0388, val_loss=0.0411, val_acc=0.96]\u001B[A\n",
      "Training:  34%|███▍      | 171/500[, loss=0.0388, val_loss=0.0411, val_acc=0.96]\u001B[A\n",
      "Training:  34%|███▍      | 171/500[, loss=0.0388, val_loss=0.0412, val_acc=0.96]\u001B[A\n",
      "Training:  34%|███▍      | 172/500[, loss=0.0388, val_loss=0.0412, val_acc=0.96]\u001B[A\n",
      "Training:  34%|███▍      | 172/500[, loss=0.0388, val_loss=0.0411, val_acc=0.96]\u001B[A\n",
      "Training:  35%|███▍      | 173/500[, loss=0.0388, val_loss=0.0411, val_acc=0.96]\u001B[A\n",
      "Training:  35%|███▍      | 173/500[, loss=0.0388, val_loss=0.0412, val_acc=0.96]\u001B[A\n",
      "Training:  35%|███▍      | 174/500[, loss=0.0388, val_loss=0.0412, val_acc=0.96]\u001B[A\n",
      "Training:  35%|███▍      | 174/500[, loss=0.0388, val_loss=0.0411, val_acc=0.96]\u001B[A\n",
      "Training:  35%|███▌      | 175/500[, loss=0.0388, val_loss=0.0411, val_acc=0.96]\u001B[A\n",
      "Training:  35%|███▌      | 175/500[, loss=0.0388, val_loss=0.0412, val_acc=0.96]\u001B[A\n",
      "Training:  35%|███▌      | 176/500[, loss=0.0388, val_loss=0.0412, val_acc=0.96]\u001B[A\n",
      "Training:  35%|███▌      | 176/500[, loss=0.0387, val_loss=0.0411, val_acc=0.96]\u001B[A\n",
      "Training:  35%|███▌      | 177/500[, loss=0.0387, val_loss=0.0411, val_acc=0.96]\u001B[A\n",
      "Training:  35%|███▌      | 177/500[, loss=0.0387, val_loss=0.0412, val_acc=0.96]\u001B[A\n",
      "Training:  36%|███▌      | 178/500[, loss=0.0387, val_loss=0.0412, val_acc=0.96]\u001B[A\n",
      "Training:  36%|███▌      | 178/500[, loss=0.0387, val_loss=0.0412, val_acc=0.96]\u001B[A\n",
      "Training:  36%|███▌      | 179/500[, loss=0.0387, val_loss=0.0412, val_acc=0.96]\u001B[A\n",
      "Training:  36%|███▌      | 179/500[, loss=0.0387, val_loss=0.0412, val_acc=0.96]\u001B[A\n",
      "Training:  36%|███▌      | 180/500[, loss=0.0387, val_loss=0.0412, val_acc=0.96]\u001B[A\n",
      "Training:  36%|███▌      | 180/500[, loss=0.0387, val_loss=0.0411, val_acc=0.96]\u001B[A\n",
      "Training:  36%|███▌      | 181/500[, loss=0.0387, val_loss=0.0411, val_acc=0.96]\u001B[A\n",
      "Training:  36%|███▌      | 181/500[, loss=0.0387, val_loss=0.0411, val_acc=0.96]\u001B[A\n",
      "Training:  36%|███▋      | 182/500[, loss=0.0387, val_loss=0.0411, val_acc=0.96]\u001B[A\n",
      "Training:  36%|███▋      | 182/500[, loss=0.0387, val_loss=0.0411, val_acc=0.96]\u001B[A\n",
      "Training:  37%|███▋      | 183/500[, loss=0.0387, val_loss=0.0411, val_acc=0.96]\u001B[A\n",
      "Training:  37%|███▋      | 183/500[, loss=0.0387, val_loss=0.0411, val_acc=0.96]\u001B[A\n",
      "Training:  37%|███▋      | 184/500[, loss=0.0387, val_loss=0.0411, val_acc=0.96]\u001B[A\n",
      "Training:  37%|███▋      | 184/500[, loss=0.0387, val_loss=0.0411, val_acc=0.96]\u001B[A\n",
      "Training:  37%|███▋      | 185/500[, loss=0.0387, val_loss=0.0411, val_acc=0.96]\u001B[A\n",
      "Training:  37%|███▋      | 185/500[, loss=0.0387, val_loss=0.0412, val_acc=0.96]\u001B[A\n",
      "Training:  37%|███▋      | 186/500[, loss=0.0387, val_loss=0.0412, val_acc=0.96]\u001B[A\n",
      "Training:  37%|███▋      | 186/500[, loss=0.0387, val_loss=0.0411, val_acc=0.96]\u001B[A\n",
      "Training:  37%|███▋      | 187/500[, loss=0.0387, val_loss=0.0411, val_acc=0.96]\u001B[A\n",
      "Training:  37%|███▋      | 187/500[, loss=0.0387, val_loss=0.0412, val_acc=0.96]\u001B[A\n",
      "Training:  38%|███▊      | 188/500[, loss=0.0387, val_loss=0.0412, val_acc=0.96]\u001B[A\n",
      "Training:  38%|███▊      | 188/500[, loss=0.0386, val_loss=0.0411, val_acc=0.96]\u001B[A\n",
      "Training:  38%|███▊      | 189/500[, loss=0.0386, val_loss=0.0411, val_acc=0.96]\u001B[A\n",
      "Training:  38%|███▊      | 189/500[, loss=0.0386, val_loss=0.0411, val_acc=0.96]\u001B[A\n",
      "Training:  38%|███▊      | 190/500[, loss=0.0386, val_loss=0.0411, val_acc=0.96]\u001B[A\n",
      "Training:  38%|███▊      | 190/500[, loss=0.0386, val_loss=0.0412, val_acc=0.96]\u001B[A\n",
      "Training:  38%|███▊      | 191/500[, loss=0.0386, val_loss=0.0412, val_acc=0.96]\u001B[A\n",
      "Training:  38%|███▊      | 191/500[, loss=0.0386, val_loss=0.0412, val_acc=0.96]\u001B[A\n",
      "Training:  38%|███▊      | 192/500[, loss=0.0386, val_loss=0.0412, val_acc=0.96]\u001B[A\n",
      "Training:  38%|███▊      | 192/500[, loss=0.0386, val_loss=0.0412, val_acc=0.96]\u001B[A\n",
      "Training:  39%|███▊      | 193/500[, loss=0.0386, val_loss=0.0412, val_acc=0.96]\u001B[A\n",
      "Training:  39%|███▊      | 193/500[, loss=0.0386, val_loss=0.0412, val_acc=0.96]\u001B[A\n",
      "Training:  39%|███▉      | 194/500[, loss=0.0386, val_loss=0.0412, val_acc=0.96]\u001B[A\n",
      "Training:  39%|███▉      | 194/500[, loss=0.0386, val_loss=0.0412, val_acc=0.96]\u001B[A\n",
      "Training:  39%|███▉      | 195/500[, loss=0.0386, val_loss=0.0412, val_acc=0.96]\u001B[A\n",
      "Training:  39%|███▉      | 195/500[, loss=0.0386, val_loss=0.0412, val_acc=0.96]\u001B[A\n",
      "Training:  39%|███▉      | 196/500[, loss=0.0386, val_loss=0.0412, val_acc=0.96]\u001B[A\n",
      "Training:  39%|███▉      | 196/500[, loss=0.0386, val_loss=0.0412, val_acc=0.96]\u001B[A\n",
      "Training:  39%|███▉      | 197/500[, loss=0.0386, val_loss=0.0412, val_acc=0.96]\u001B[A\n",
      "Training:  39%|███▉      | 197/500[, loss=0.0386, val_loss=0.0412, val_acc=0.96]\u001B[A\n",
      "Training:  40%|███▉      | 198/500[, loss=0.0386, val_loss=0.0412, val_acc=0.96]\u001B[A\n",
      "Training:  40%|███▉      | 198/500[, loss=0.0386, val_loss=0.0412, val_acc=0.96]\u001B[A\n",
      "Training:  40%|███▉      | 199/500[, loss=0.0386, val_loss=0.0412, val_acc=0.96]\u001B[A\n",
      "Training:  40%|███▉      | 199/500[, loss=0.0386, val_loss=0.0412, val_acc=0.96]\u001B[A\n",
      "Training:  40%|████      | 200/500[, loss=0.0386, val_loss=0.0412, val_acc=0.96]\u001B[A\n",
      "Training:  40%|████      | 200/500[, loss=0.0386, val_loss=0.0412, val_acc=0.96]\u001B[A\n",
      "Training:  40%|████      | 201/500[, loss=0.0386, val_loss=0.0412, val_acc=0.96]\u001B[A\n",
      "Training:  40%|████      | 201/500[, loss=0.0386, val_loss=0.0412, val_acc=0.96]\u001B[A\n",
      "Training:  40%|████      | 202/500[, loss=0.0386, val_loss=0.0412, val_acc=0.96]\u001B[A\n",
      "Training:  40%|████      | 202/500[, loss=0.0386, val_loss=0.0412, val_acc=0.96]\u001B[A\n",
      "Training:  41%|████      | 203/500[, loss=0.0386, val_loss=0.0412, val_acc=0.96]\u001B[A\n",
      "Training:  41%|████      | 203/500[, loss=0.0386, val_loss=0.0411, val_acc=0.96]\u001B[A\n",
      "Training:  41%|████      | 204/500[, loss=0.0386, val_loss=0.0411, val_acc=0.96]\u001B[A\n",
      "Training:  41%|████      | 204/500[, loss=0.0386, val_loss=0.0411, val_acc=0.96]\u001B[A\n",
      "Training:  41%|████      | 205/500[, loss=0.0386, val_loss=0.0411, val_acc=0.96]\u001B[A\n",
      "Training:  41%|████      | 205/500[, loss=0.0386, val_loss=0.0411, val_acc=0.96]\u001B[A\n",
      "Training:  41%|████      | 206/500[, loss=0.0386, val_loss=0.0411, val_acc=0.96]\u001B[A\n",
      "Training:  41%|████      | 206/500[, loss=0.0385, val_loss=0.0411, val_acc=0.96]\u001B[A\n",
      "Training:  41%|████▏     | 207/500[, loss=0.0385, val_loss=0.0411, val_acc=0.96]\u001B[A\n",
      "Training:  41%|████▏     | 207/500[, loss=0.0385, val_loss=0.0411, val_acc=0.96]\u001B[A\n",
      "Training:  42%|████▏     | 208/500[, loss=0.0385, val_loss=0.0411, val_acc=0.96]\u001B[A\n",
      "Training:  42%|████▏     | 208/500[, loss=0.0385, val_loss=0.0411, val_acc=0.96]\u001B[A\n",
      "Training:  42%|████▏     | 209/500[, loss=0.0385, val_loss=0.0411, val_acc=0.96]\u001B[A\n",
      "Training:  42%|████▏     | 209/500[, loss=0.0385, val_loss=0.0411, val_acc=0.96]\u001B[A\n",
      "Training:  42%|████▏     | 210/500[, loss=0.0385, val_loss=0.0411, val_acc=0.96]\u001B[A\n",
      "Training:  42%|████▏     | 210/500[, loss=0.0385, val_loss=0.0411, val_acc=0.96]\u001B[A\n",
      "Training:  42%|████▏     | 211/500[, loss=0.0385, val_loss=0.0411, val_acc=0.96]\u001B[A\n",
      "Training:  42%|████▏     | 211/500[, loss=0.0385, val_loss=0.0411, val_acc=0.96]\u001B[A\n",
      "Training:  42%|████▏     | 212/500[, loss=0.0385, val_loss=0.0411, val_acc=0.96]\u001B[A\n",
      "Training:  42%|████▏     | 212/500[, loss=0.0385, val_loss=0.0411, val_acc=0.96]\u001B[A\n",
      "Training:  43%|████▎     | 213/500[, loss=0.0385, val_loss=0.0411, val_acc=0.96]\u001B[A\n",
      "Training:  43%|████▎     | 213/500[, loss=0.0385, val_loss=0.0411, val_acc=0.96]\u001B[A\n",
      "Training:  43%|████▎     | 214/500[, loss=0.0385, val_loss=0.0411, val_acc=0.96]\u001B[A\n",
      "Training:  43%|████▎     | 214/500[, loss=0.0385, val_loss=0.0411, val_acc=0.96]\u001B[A\n",
      "Training:  43%|████▎     | 215/500[, loss=0.0385, val_loss=0.0411, val_acc=0.96]\u001B[A\n",
      "Training:  43%|████▎     | 215/500[, loss=0.0385, val_loss=0.0411, val_acc=0.96]\u001B[A\n",
      "Training:  43%|████▎     | 216/500[, loss=0.0385, val_loss=0.0411, val_acc=0.96]\u001B[A\n",
      "Training:  43%|████▎     | 216/500[, loss=0.0385, val_loss=0.0411, val_acc=0.96]\u001B[A\n",
      "Training:  43%|████▎     | 217/500[, loss=0.0385, val_loss=0.0411, val_acc=0.96]\u001B[A\n",
      "Training:  43%|████▎     | 217/500[, loss=0.0385, val_loss=0.0411, val_acc=0.96]\u001B[A\n",
      "Training:  44%|████▎     | 218/500[, loss=0.0385, val_loss=0.0411, val_acc=0.96]\u001B[A\n",
      "Training:  44%|████▎     | 218/500[, loss=0.0385, val_loss=0.0411, val_acc=0.96]\u001B[A\n",
      "Training:  44%|████▍     | 219/500[, loss=0.0385, val_loss=0.0411, val_acc=0.96]\u001B[A\n",
      "Training:  44%|████▍     | 219/500[, loss=0.0385, val_loss=0.041, val_acc=0.96] \u001B[A\n",
      "Training:  44%|████▍     | 220/500[, loss=0.0385, val_loss=0.041, val_acc=0.96]\u001B[A\n",
      "Training:  44%|████▍     | 220/500[, loss=0.0385, val_loss=0.041, val_acc=0.96]\u001B[A\n",
      "Training:  44%|████▍     | 221/500[, loss=0.0385, val_loss=0.041, val_acc=0.96]\u001B[A\n",
      "Training:  44%|████▍     | 221/500[, loss=0.0385, val_loss=0.041, val_acc=0.96]\u001B[A\n",
      "Training:  44%|████▍     | 222/500[, loss=0.0385, val_loss=0.041, val_acc=0.96]\u001B[A\n",
      "Training:  44%|████▍     | 222/500[, loss=0.0385, val_loss=0.041, val_acc=0.96]\u001B[A\n",
      "Training:  45%|████▍     | 223/500[, loss=0.0385, val_loss=0.041, val_acc=0.96]\u001B[A\n",
      "Training:  45%|████▍     | 223/500[, loss=0.0385, val_loss=0.041, val_acc=0.96]\u001B[A\n",
      "Training:  45%|████▍     | 224/500[, loss=0.0385, val_loss=0.041, val_acc=0.96]\u001B[A\n",
      "Training:  45%|████▍     | 224/500[, loss=0.0385, val_loss=0.041, val_acc=0.96]\u001B[A\n",
      "Training:  45%|████▌     | 225/500[, loss=0.0385, val_loss=0.041, val_acc=0.96]\u001B[A\n",
      "Training:  45%|████▌     | 225/500[, loss=0.0385, val_loss=0.041, val_acc=0.96]\u001B[A\n",
      "Training:  45%|████▌     | 226/500[, loss=0.0385, val_loss=0.041, val_acc=0.96]\u001B[A\n",
      "Training:  45%|████▌     | 226/500[, loss=0.0385, val_loss=0.041, val_acc=0.96]\u001B[A\n",
      "Training:  45%|████▌     | 227/500[, loss=0.0385, val_loss=0.041, val_acc=0.96]\u001B[A\n",
      "Training:  45%|████▌     | 227/500[, loss=0.0385, val_loss=0.041, val_acc=0.96]\u001B[A\n",
      "Training:  46%|████▌     | 228/500[, loss=0.0385, val_loss=0.041, val_acc=0.96]\u001B[A\n",
      "Training:  46%|████▌     | 228/500[, loss=0.0385, val_loss=0.041, val_acc=0.96]\u001B[A\n",
      "Training:  46%|████▌     | 229/500[, loss=0.0385, val_loss=0.041, val_acc=0.96]\u001B[A\n",
      "Training:  46%|████▌     | 229/500[, loss=0.0385, val_loss=0.041, val_acc=0.96]\u001B[A\n",
      "Training:  46%|████▌     | 230/500[, loss=0.0385, val_loss=0.041, val_acc=0.96]\u001B[A\n",
      "Training:  46%|████▌     | 230/500[, loss=0.0385, val_loss=0.041, val_acc=0.96]\u001B[A\n",
      "Training:  46%|████▌     | 231/500[, loss=0.0385, val_loss=0.041, val_acc=0.96]\u001B[A\n",
      "Training:  46%|████▌     | 231/500[, loss=0.0385, val_loss=0.041, val_acc=0.96]\u001B[A\n",
      "Training:  46%|████▋     | 232/500[, loss=0.0385, val_loss=0.041, val_acc=0.96]\u001B[A\n",
      "Training:  46%|████▋     | 232/500[, loss=0.0385, val_loss=0.041, val_acc=0.96]\u001B[A\n",
      "Training:  47%|████▋     | 233/500[, loss=0.0385, val_loss=0.041, val_acc=0.96]\u001B[A\n",
      "Training:  47%|████▋     | 233/500[, loss=0.0385, val_loss=0.041, val_acc=0.96]\u001B[A\n",
      "Training:  47%|████▋     | 234/500[, loss=0.0385, val_loss=0.041, val_acc=0.96]\u001B[A\n",
      "Training:  47%|████▋     | 234/500[, loss=0.0384, val_loss=0.041, val_acc=0.96]\u001B[A\n",
      "Training:  47%|████▋     | 235/500[, loss=0.0384, val_loss=0.041, val_acc=0.96]\u001B[A\n",
      "Training:  47%|████▋     | 235/500[, loss=0.0384, val_loss=0.041, val_acc=0.96]\u001B[A\n",
      "Training:  47%|████▋     | 236/500[, loss=0.0384, val_loss=0.041, val_acc=0.96]\u001B[A\n",
      "Training:  47%|████▋     | 236/500[, loss=0.0384, val_loss=0.041, val_acc=0.96]\u001B[A\n",
      "Training:  47%|████▋     | 237/500[, loss=0.0384, val_loss=0.041, val_acc=0.96]\u001B[A\n",
      "Training:  47%|████▋     | 237/500[, loss=0.0384, val_loss=0.0409, val_acc=0.96]\u001B[A\n",
      "Training:  48%|████▊     | 238/500[, loss=0.0384, val_loss=0.0409, val_acc=0.96]\u001B[A\n",
      "Training:  48%|████▊     | 238/500[, loss=0.0384, val_loss=0.0409, val_acc=0.96]\u001B[A\n",
      "Training:  48%|████▊     | 239/500[, loss=0.0384, val_loss=0.0409, val_acc=0.96]\u001B[A\n",
      "Training:  48%|████▊     | 239/500[, loss=0.0384, val_loss=0.0409, val_acc=0.96]\u001B[A\n",
      "Training:  48%|████▊     | 240/500[, loss=0.0384, val_loss=0.0409, val_acc=0.96]\u001B[A\n",
      "Training:  48%|████▊     | 240/500[, loss=0.0384, val_loss=0.0409, val_acc=0.96]\u001B[A\n",
      "Training:  48%|████▊     | 241/500[, loss=0.0384, val_loss=0.0409, val_acc=0.96]\u001B[A\n",
      "Training:  48%|████▊     | 241/500[, loss=0.0384, val_loss=0.0409, val_acc=0.96]\u001B[A\n",
      "Training:  48%|████▊     | 242/500[, loss=0.0384, val_loss=0.0409, val_acc=0.96]\u001B[A\n",
      "Training:  48%|████▊     | 242/500[, loss=0.0384, val_loss=0.0409, val_acc=0.96]\u001B[A\n",
      "Training:  49%|████▊     | 243/500[, loss=0.0384, val_loss=0.0409, val_acc=0.96]\u001B[A\n",
      "Training:  49%|████▊     | 243/500[, loss=0.0384, val_loss=0.0409, val_acc=0.96]\u001B[A\n",
      "Training:  49%|████▉     | 244/500[, loss=0.0384, val_loss=0.0409, val_acc=0.96]\u001B[A\n",
      "Training:  49%|████▉     | 244/500[, loss=0.0384, val_loss=0.0409, val_acc=0.96]\u001B[A\n",
      "Training:  49%|████▉     | 245/500[, loss=0.0384, val_loss=0.0409, val_acc=0.96]\u001B[A\n",
      "Training:  49%|████▉     | 245/500[, loss=0.0384, val_loss=0.0409, val_acc=0.96]\u001B[A\n",
      "Training:  49%|████▉     | 246/500[, loss=0.0384, val_loss=0.0409, val_acc=0.96]\u001B[A\n",
      "Training:  49%|████▉     | 246/500[, loss=0.0384, val_loss=0.0409, val_acc=0.96]\u001B[A\n",
      "Training:  49%|████▉     | 247/500[, loss=0.0384, val_loss=0.0409, val_acc=0.96]\u001B[A\n",
      "Training:  49%|████▉     | 247/500[, loss=0.0384, val_loss=0.0409, val_acc=0.96]\u001B[A\n",
      "Training:  50%|████▉     | 248/500[, loss=0.0384, val_loss=0.0409, val_acc=0.96]\u001B[A\n",
      "Training:  50%|████▉     | 248/500[, loss=0.0384, val_loss=0.0409, val_acc=0.96]\u001B[A\n",
      "Training:  50%|████▉     | 249/500[, loss=0.0384, val_loss=0.0409, val_acc=0.96]\u001B[A\n",
      "Training:  50%|████▉     | 249/500[, loss=0.0384, val_loss=0.0409, val_acc=0.96]\u001B[A\n",
      "Training:  50%|█████     | 250/500[, loss=0.0384, val_loss=0.0409, val_acc=0.96]\u001B[A\n",
      "Training:  50%|█████     | 250/500[, loss=0.0384, val_loss=0.0409, val_acc=0.96]\u001B[A\n",
      "Training:  50%|█████     | 251/500[, loss=0.0384, val_loss=0.0409, val_acc=0.96]\u001B[A\n",
      "Training:  50%|█████     | 251/500[, loss=0.0384, val_loss=0.0409, val_acc=0.96]\u001B[A\n",
      "Training:  50%|█████     | 252/500[, loss=0.0384, val_loss=0.0409, val_acc=0.96]\u001B[A\n",
      "Training:  50%|█████     | 252/500[, loss=0.0384, val_loss=0.0409, val_acc=0.96]\u001B[A\n",
      "Training:  51%|█████     | 253/500[, loss=0.0384, val_loss=0.0409, val_acc=0.96]\u001B[A\n",
      "Training:  51%|█████     | 253/500[, loss=0.0384, val_loss=0.0408, val_acc=0.96]\u001B[A\n",
      "Training:  51%|█████     | 254/500[, loss=0.0384, val_loss=0.0408, val_acc=0.96]\u001B[A\n",
      "Training:  51%|█████     | 254/500[, loss=0.0384, val_loss=0.0408, val_acc=0.96]\u001B[A\n",
      "Training:  51%|█████     | 255/500[, loss=0.0384, val_loss=0.0408, val_acc=0.96]\u001B[A\n",
      "Training:  51%|█████     | 255/500[, loss=0.0384, val_loss=0.0408, val_acc=0.96]\u001B[A\n",
      "Training:  51%|█████     | 256/500[, loss=0.0384, val_loss=0.0408, val_acc=0.96]\u001B[A\n",
      "Training:  51%|█████     | 256/500[, loss=0.0384, val_loss=0.0408, val_acc=0.96]\u001B[A\n",
      "Training:  51%|█████▏    | 257/500[, loss=0.0384, val_loss=0.0408, val_acc=0.96]\u001B[A\n",
      "Training:  51%|█████▏    | 257/500[, loss=0.0384, val_loss=0.0408, val_acc=0.96]\u001B[A\n",
      "Training:  52%|█████▏    | 258/500[, loss=0.0384, val_loss=0.0408, val_acc=0.96]\u001B[A\n",
      "Training:  52%|█████▏    | 258/500[, loss=0.0384, val_loss=0.0408, val_acc=0.96]\u001B[A\n",
      "Training:  52%|█████▏    | 259/500[, loss=0.0384, val_loss=0.0408, val_acc=0.96]\u001B[A\n",
      "Training:  52%|█████▏    | 259/500[, loss=0.0384, val_loss=0.0408, val_acc=0.96]\u001B[A\n",
      "Training:  52%|█████▏    | 260/500[, loss=0.0384, val_loss=0.0408, val_acc=0.96]\u001B[A\n",
      "Training:  52%|█████▏    | 260/500[, loss=0.0384, val_loss=0.0408, val_acc=0.96]\u001B[A\n",
      "Training:  52%|█████▏    | 261/500[, loss=0.0384, val_loss=0.0408, val_acc=0.96]\u001B[A\n",
      "Training:  52%|█████▏    | 261/500[, loss=0.0384, val_loss=0.0408, val_acc=0.96]\u001B[A\n",
      "Training:  52%|█████▏    | 262/500[, loss=0.0384, val_loss=0.0408, val_acc=0.96]\u001B[A\n",
      "Training:  52%|█████▏    | 262/500[, loss=0.0384, val_loss=0.0408, val_acc=0.96]\u001B[A\n",
      "Training:  53%|█████▎    | 263/500[, loss=0.0384, val_loss=0.0408, val_acc=0.96]\u001B[A\n",
      "Training:  53%|█████▎    | 263/500[, loss=0.0384, val_loss=0.0408, val_acc=0.96]\u001B[A\n",
      "Training:  53%|█████▎    | 264/500[, loss=0.0384, val_loss=0.0408, val_acc=0.96]\u001B[A\n",
      "Training:  53%|█████▎    | 264/500[, loss=0.0384, val_loss=0.0408, val_acc=0.96]\u001B[A\n",
      "Training:  53%|█████▎    | 265/500[, loss=0.0384, val_loss=0.0408, val_acc=0.96]\u001B[A\n",
      "Training:  53%|█████▎    | 265/500[, loss=0.0384, val_loss=0.0408, val_acc=0.96]\u001B[A\n",
      "Training:  53%|█████▎    | 266/500[, loss=0.0384, val_loss=0.0408, val_acc=0.96]\u001B[A\n",
      "Training:  53%|█████▎    | 266/500[, loss=0.0384, val_loss=0.0407, val_acc=0.96]\u001B[A\n",
      "Training:  53%|█████▎    | 267/500[, loss=0.0384, val_loss=0.0407, val_acc=0.96]\u001B[A\n",
      "Training:  53%|█████▎    | 267/500[, loss=0.0384, val_loss=0.0408, val_acc=0.96]\u001B[A\n",
      "Training:  54%|█████▎    | 268/500[, loss=0.0384, val_loss=0.0408, val_acc=0.96]\u001B[A\n",
      "Training:  54%|█████▎    | 268/500[, loss=0.0384, val_loss=0.0407, val_acc=0.96]\u001B[A\n",
      "Training:  54%|█████▍    | 269/500[, loss=0.0384, val_loss=0.0407, val_acc=0.96]\u001B[A\n",
      "Training:  54%|█████▍    | 269/500[, loss=0.0384, val_loss=0.0407, val_acc=0.96]\u001B[A\n",
      "Training:  54%|█████▍    | 270/500[, loss=0.0384, val_loss=0.0407, val_acc=0.96]\u001B[A\n",
      "Training:  54%|█████▍    | 270/500[, loss=0.0384, val_loss=0.0407, val_acc=0.96]\u001B[A\n",
      "Training:  54%|█████▍    | 271/500[, loss=0.0384, val_loss=0.0407, val_acc=0.96]\u001B[A\n",
      "Training:  54%|█████▍    | 271/500[, loss=0.0383, val_loss=0.0407, val_acc=0.96]\u001B[A\n",
      "Training:  54%|█████▍    | 272/500[, loss=0.0383, val_loss=0.0407, val_acc=0.96]\u001B[A\n",
      "Training:  54%|█████▍    | 272/500[, loss=0.0383, val_loss=0.0407, val_acc=0.96]\u001B[A\n",
      "Training:  55%|█████▍    | 273/500[, loss=0.0383, val_loss=0.0407, val_acc=0.96]\u001B[A\n",
      "Training:  55%|█████▍    | 273/500[, loss=0.0383, val_loss=0.0407, val_acc=0.96]\u001B[A\n",
      "Training:  55%|█████▍    | 274/500[, loss=0.0383, val_loss=0.0407, val_acc=0.96]\u001B[A\n",
      "Training:  55%|█████▍    | 274/500[, loss=0.0383, val_loss=0.0407, val_acc=0.96]\u001B[A\n",
      "Training:  55%|█████▌    | 275/500[, loss=0.0383, val_loss=0.0407, val_acc=0.96]\u001B[A\n",
      "Training:  55%|█████▌    | 275/500[, loss=0.0383, val_loss=0.0407, val_acc=0.96]\u001B[A\n",
      "Training:  55%|█████▌    | 276/500[, loss=0.0383, val_loss=0.0407, val_acc=0.96]\u001B[A\n",
      "Training:  55%|█████▌    | 276/500[, loss=0.0383, val_loss=0.0407, val_acc=0.96]\u001B[A\n",
      "Training:  55%|█████▌    | 277/500[, loss=0.0383, val_loss=0.0407, val_acc=0.96]\u001B[A\n",
      "Training:  55%|█████▌    | 277/500[, loss=0.0383, val_loss=0.0407, val_acc=0.96]\u001B[A\n",
      "Training:  56%|█████▌    | 278/500[, loss=0.0383, val_loss=0.0407, val_acc=0.96]\u001B[A\n",
      "Training:  56%|█████▌    | 278/500[, loss=0.0383, val_loss=0.0407, val_acc=0.96]\u001B[A\n",
      "Training:  56%|█████▌    | 279/500[, loss=0.0383, val_loss=0.0407, val_acc=0.96]\u001B[A\n",
      "Training:  56%|█████▌    | 279/500[, loss=0.0383, val_loss=0.0407, val_acc=0.96]\u001B[A\n",
      "Training:  56%|█████▌    | 280/500[, loss=0.0383, val_loss=0.0407, val_acc=0.96]\u001B[A\n",
      "Training:  56%|█████▌    | 280/500[, loss=0.0383, val_loss=0.0407, val_acc=0.96]\u001B[A\n",
      "Training:  56%|█████▌    | 281/500[, loss=0.0383, val_loss=0.0407, val_acc=0.96]\u001B[A\n",
      "Training:  56%|█████▌    | 281/500[, loss=0.0383, val_loss=0.0407, val_acc=0.96]\u001B[A\n",
      "Training:  56%|█████▋    | 282/500[, loss=0.0383, val_loss=0.0407, val_acc=0.96]\u001B[A\n",
      "Training:  56%|█████▋    | 282/500[, loss=0.0383, val_loss=0.0407, val_acc=0.96]\u001B[A\n",
      "Training:  57%|█████▋    | 283/500[, loss=0.0383, val_loss=0.0407, val_acc=0.96]\u001B[A\n",
      "Training:  57%|█████▋    | 283/500[, loss=0.0383, val_loss=0.0406, val_acc=0.96]\u001B[A\n",
      "Training:  57%|█████▋    | 284/500[, loss=0.0383, val_loss=0.0406, val_acc=0.96]\u001B[A\n",
      "Training:  57%|█████▋    | 284/500[, loss=0.0383, val_loss=0.0406, val_acc=0.96]\u001B[A\n",
      "Training:  57%|█████▋    | 285/500[, loss=0.0383, val_loss=0.0406, val_acc=0.96]\u001B[A\n",
      "Training:  57%|█████▋    | 285/500[, loss=0.0383, val_loss=0.0406, val_acc=0.96]\u001B[A\n",
      "Training:  57%|█████▋    | 286/500[, loss=0.0383, val_loss=0.0406, val_acc=0.96]\u001B[A\n",
      "Training:  57%|█████▋    | 286/500[, loss=0.0383, val_loss=0.0406, val_acc=0.96]\u001B[A\n",
      "Training:  57%|█████▋    | 287/500[, loss=0.0383, val_loss=0.0406, val_acc=0.96]\u001B[A\n",
      "Training:  57%|█████▋    | 287/500[, loss=0.0383, val_loss=0.0406, val_acc=0.96]\u001B[A\n",
      "Training:  58%|█████▊    | 288/500[, loss=0.0383, val_loss=0.0406, val_acc=0.96]\u001B[A\n",
      "Training:  58%|█████▊    | 288/500[, loss=0.0383, val_loss=0.0406, val_acc=0.96]\u001B[A\n",
      "Training:  58%|█████▊    | 289/500[, loss=0.0383, val_loss=0.0406, val_acc=0.96]\u001B[A\n",
      "Training:  58%|█████▊    | 289/500[, loss=0.0383, val_loss=0.0406, val_acc=0.96]\u001B[A\n",
      "Training:  58%|█████▊    | 290/500[, loss=0.0383, val_loss=0.0406, val_acc=0.96]\u001B[A\n",
      "Training:  58%|█████▊    | 290/500[, loss=0.0383, val_loss=0.0406, val_acc=0.96]\u001B[A\n",
      "Training:  58%|█████▊    | 291/500[, loss=0.0383, val_loss=0.0406, val_acc=0.96]\u001B[A\n",
      "Training:  58%|█████▊    | 291/500[, loss=0.0383, val_loss=0.0406, val_acc=0.96]\u001B[A\n",
      "Training:  58%|█████▊    | 292/500[, loss=0.0383, val_loss=0.0406, val_acc=0.96]\u001B[A\n",
      "Training:  58%|█████▊    | 292/500[, loss=0.0383, val_loss=0.0406, val_acc=0.96]\u001B[A\n",
      "Training:  59%|█████▊    | 293/500[, loss=0.0383, val_loss=0.0406, val_acc=0.96]\u001B[A\n",
      "Training:  59%|█████▊    | 293/500[, loss=0.0383, val_loss=0.0406, val_acc=0.96]\u001B[A\n",
      "Training:  59%|█████▉    | 294/500[, loss=0.0383, val_loss=0.0406, val_acc=0.96]\u001B[A\n",
      "Training:  59%|█████▉    | 294/500[, loss=0.0383, val_loss=0.0406, val_acc=0.96]\u001B[A\n",
      "Training:  59%|█████▉    | 295/500[, loss=0.0383, val_loss=0.0406, val_acc=0.96]\u001B[A\n",
      "Training:  59%|█████▉    | 295/500[, loss=0.0383, val_loss=0.0406, val_acc=0.96]\u001B[A\n",
      "Training:  59%|█████▉    | 296/500[, loss=0.0383, val_loss=0.0406, val_acc=0.96]\u001B[A\n",
      "Training:  59%|█████▉    | 296/500[, loss=0.0383, val_loss=0.0406, val_acc=0.96]\u001B[A\n",
      "Training:  59%|█████▉    | 297/500[, loss=0.0383, val_loss=0.0406, val_acc=0.96]\u001B[A\n",
      "Training:  59%|█████▉    | 297/500[, loss=0.0383, val_loss=0.0406, val_acc=0.96]\u001B[A\n",
      "Training:  60%|█████▉    | 298/500[, loss=0.0383, val_loss=0.0406, val_acc=0.96]\u001B[A\n",
      "Training:  60%|█████▉    | 298/500[, loss=0.0383, val_loss=0.0406, val_acc=0.96]\u001B[A\n",
      "Training:  60%|█████▉    | 299/500[, loss=0.0383, val_loss=0.0406, val_acc=0.96]\u001B[A\n",
      "Training:  60%|█████▉    | 299/500[, loss=0.0383, val_loss=0.0406, val_acc=0.96]\u001B[A\n",
      "Training:  60%|██████    | 300/500[, loss=0.0383, val_loss=0.0406, val_acc=0.96]\u001B[A\n",
      "Training:  60%|██████    | 300/500[, loss=0.0383, val_loss=0.0406, val_acc=0.96]\u001B[A\n",
      "Training:  60%|██████    | 301/500[, loss=0.0383, val_loss=0.0406, val_acc=0.96]\u001B[A\n",
      "Training:  60%|██████    | 301/500[, loss=0.0383, val_loss=0.0406, val_acc=0.96]\u001B[A\n",
      "Training:  60%|██████    | 302/500[, loss=0.0383, val_loss=0.0406, val_acc=0.96]\u001B[A\n",
      "Training:  60%|██████    | 302/500[, loss=0.0383, val_loss=0.0406, val_acc=0.96]\u001B[A\n",
      "Training:  61%|██████    | 303/500[, loss=0.0383, val_loss=0.0406, val_acc=0.96]\u001B[A\n",
      "Training:  61%|██████    | 303/500[, loss=0.0383, val_loss=0.0405, val_acc=0.96]\u001B[A\n",
      "Training:  61%|██████    | 304/500[, loss=0.0383, val_loss=0.0405, val_acc=0.96]\u001B[A\n",
      "Training:  61%|██████    | 304/500[, loss=0.0383, val_loss=0.0405, val_acc=0.96]\u001B[A\n",
      "Training:  61%|██████    | 305/500[, loss=0.0383, val_loss=0.0405, val_acc=0.96]\u001B[A\n",
      "Training:  61%|██████    | 305/500[, loss=0.0383, val_loss=0.0405, val_acc=0.96]\u001B[A\n",
      "Training:  61%|██████    | 306/500[, loss=0.0383, val_loss=0.0405, val_acc=0.96]\u001B[A\n",
      "Training:  61%|██████    | 306/500[, loss=0.0383, val_loss=0.0405, val_acc=0.96]\u001B[A\n",
      "Training:  61%|██████▏   | 307/500[, loss=0.0383, val_loss=0.0405, val_acc=0.96]\u001B[A\n",
      "Training:  61%|██████▏   | 307/500[, loss=0.0383, val_loss=0.0405, val_acc=0.96]\u001B[A\n",
      "Training:  62%|██████▏   | 308/500[, loss=0.0383, val_loss=0.0405, val_acc=0.96]\u001B[A\n",
      "Training:  62%|██████▏   | 308/500[, loss=0.0383, val_loss=0.0405, val_acc=0.96]\u001B[A\n",
      "Training:  62%|██████▏   | 309/500[, loss=0.0383, val_loss=0.0405, val_acc=0.96]\u001B[A\n",
      "Training:  62%|██████▏   | 309/500[, loss=0.0383, val_loss=0.0405, val_acc=0.96]\u001B[A\n",
      "Training:  62%|██████▏   | 310/500[, loss=0.0383, val_loss=0.0405, val_acc=0.96]\u001B[A\n",
      "Training:  62%|██████▏   | 310/500[, loss=0.0383, val_loss=0.0405, val_acc=0.96]\u001B[A\n",
      "Training:  62%|██████▏   | 311/500[, loss=0.0383, val_loss=0.0405, val_acc=0.96]\u001B[A\n",
      "Training:  62%|██████▏   | 311/500[, loss=0.0383, val_loss=0.0405, val_acc=0.96]\u001B[A\n",
      "Training:  62%|██████▏   | 312/500[, loss=0.0383, val_loss=0.0405, val_acc=0.96]\u001B[A\n",
      "Training:  62%|██████▏   | 312/500[, loss=0.0383, val_loss=0.0405, val_acc=0.96]\u001B[A\n",
      "Training:  63%|██████▎   | 313/500[, loss=0.0383, val_loss=0.0405, val_acc=0.96]\u001B[A\n",
      "Training:  63%|██████▎   | 313/500[, loss=0.0383, val_loss=0.0405, val_acc=0.96]\u001B[A\n",
      "Training:  63%|██████▎   | 314/500[, loss=0.0383, val_loss=0.0405, val_acc=0.96]\u001B[A\n",
      "Training:  63%|██████▎   | 314/500[, loss=0.0383, val_loss=0.0405, val_acc=0.96]\u001B[A\n",
      "Training:  63%|██████▎   | 315/500[, loss=0.0383, val_loss=0.0405, val_acc=0.96]\u001B[A\n",
      "Training:  63%|██████▎   | 315/500[, loss=0.0383, val_loss=0.0405, val_acc=0.96]\u001B[A\n",
      "Training:  63%|██████▎   | 316/500[, loss=0.0383, val_loss=0.0405, val_acc=0.96]\u001B[A\n",
      "Training:  63%|██████▎   | 316/500[, loss=0.0383, val_loss=0.0405, val_acc=0.96]\u001B[A\n",
      "Training:  63%|██████▎   | 317/500[, loss=0.0383, val_loss=0.0405, val_acc=0.96]\u001B[A\n",
      "Training:  63%|██████▎   | 317/500[, loss=0.0382, val_loss=0.0405, val_acc=0.96]\u001B[A\n",
      "Training:  64%|██████▎   | 318/500[, loss=0.0382, val_loss=0.0405, val_acc=0.96]\u001B[A\n",
      "Training:  64%|██████▎   | 318/500[, loss=0.0382, val_loss=0.0405, val_acc=0.96]\u001B[A\n",
      "Training:  64%|██████▍   | 319/500[, loss=0.0382, val_loss=0.0405, val_acc=0.96]\u001B[A\n",
      "Training:  64%|██████▍   | 319/500[, loss=0.0382, val_loss=0.0405, val_acc=0.96]\u001B[A\n",
      "Training:  64%|██████▍   | 320/500[, loss=0.0382, val_loss=0.0405, val_acc=0.96]\u001B[A\n",
      "Training:  64%|██████▍   | 320/500[, loss=0.0382, val_loss=0.0405, val_acc=0.96]\u001B[A\n",
      "Training:  64%|██████▍   | 321/500[, loss=0.0382, val_loss=0.0405, val_acc=0.96]\u001B[A\n",
      "Training:  64%|██████▍   | 321/500[, loss=0.0382, val_loss=0.0405, val_acc=0.96]\u001B[A\n",
      "Training:  64%|██████▍   | 322/500[, loss=0.0382, val_loss=0.0405, val_acc=0.96]\u001B[A\n",
      "Training:  64%|██████▍   | 322/500[, loss=0.0382, val_loss=0.0404, val_acc=0.96]\u001B[A\n",
      "Training:  65%|██████▍   | 323/500[, loss=0.0382, val_loss=0.0404, val_acc=0.96]\u001B[A\n",
      "Training:  65%|██████▍   | 323/500[, loss=0.0382, val_loss=0.0404, val_acc=0.96]\u001B[A\n",
      "Training:  65%|██████▍   | 324/500[, loss=0.0382, val_loss=0.0404, val_acc=0.96]\u001B[A\n",
      "Training:  65%|██████▍   | 324/500[, loss=0.0382, val_loss=0.0404, val_acc=0.96]\u001B[A\n",
      "Training:  65%|██████▌   | 325/500[, loss=0.0382, val_loss=0.0404, val_acc=0.96]\u001B[A\n",
      "Training:  65%|██████▌   | 325/500[, loss=0.0382, val_loss=0.0404, val_acc=0.96]\u001B[A\n",
      "Training:  65%|██████▌   | 326/500[, loss=0.0382, val_loss=0.0404, val_acc=0.96]\u001B[A\n",
      "Training:  65%|██████▌   | 326/500[, loss=0.0382, val_loss=0.0404, val_acc=0.96]\u001B[A\n",
      "Training:  65%|██████▌   | 327/500[, loss=0.0382, val_loss=0.0404, val_acc=0.96]\u001B[A\n",
      "Training:  65%|██████▌   | 327/500[, loss=0.0382, val_loss=0.0404, val_acc=0.96]\u001B[A\n",
      "Training:  66%|██████▌   | 328/500[, loss=0.0382, val_loss=0.0404, val_acc=0.96]\u001B[A\n",
      "Training:  66%|██████▌   | 328/500[, loss=0.0382, val_loss=0.0404, val_acc=0.96]\u001B[A\n",
      "Training:  66%|██████▌   | 329/500[, loss=0.0382, val_loss=0.0404, val_acc=0.96]\u001B[A\n",
      "Training:  66%|██████▌   | 329/500[, loss=0.0382, val_loss=0.0404, val_acc=0.96]\u001B[A\n",
      "Training:  66%|██████▌   | 330/500[, loss=0.0382, val_loss=0.0404, val_acc=0.96]\u001B[A\n",
      "Training:  66%|██████▌   | 330/500[, loss=0.0382, val_loss=0.0404, val_acc=0.96]\u001B[A\n",
      "Training:  66%|██████▌   | 331/500[, loss=0.0382, val_loss=0.0404, val_acc=0.96]\u001B[A\n",
      "Training:  66%|██████▌   | 331/500[, loss=0.0382, val_loss=0.0404, val_acc=0.96]\u001B[A\n",
      "Training:  66%|██████▋   | 332/500[, loss=0.0382, val_loss=0.0404, val_acc=0.96]\u001B[A\n",
      "Training:  66%|██████▋   | 332/500[, loss=0.0382, val_loss=0.0404, val_acc=0.96]\u001B[A\n",
      "Training:  67%|██████▋   | 333/500[, loss=0.0382, val_loss=0.0404, val_acc=0.96]\u001B[A\n",
      "Training:  67%|██████▋   | 333/500[, loss=0.0382, val_loss=0.0404, val_acc=0.96]\u001B[A\n",
      "Training:  67%|██████▋   | 334/500[, loss=0.0382, val_loss=0.0404, val_acc=0.96]\u001B[A\n",
      "Training:  67%|██████▋   | 334/500[, loss=0.0382, val_loss=0.0404, val_acc=0.96]\u001B[A\n",
      "Training:  67%|██████▋   | 335/500[, loss=0.0382, val_loss=0.0404, val_acc=0.96]\u001B[A\n",
      "Training:  67%|██████▋   | 335/500[, loss=0.0382, val_loss=0.0404, val_acc=0.96]\u001B[A\n",
      "Training:  67%|██████▋   | 336/500[, loss=0.0382, val_loss=0.0404, val_acc=0.96]\u001B[A\n",
      "Training:  67%|██████▋   | 336/500[, loss=0.0382, val_loss=0.0404, val_acc=0.96]\u001B[A\n",
      "Training:  67%|██████▋   | 337/500[, loss=0.0382, val_loss=0.0404, val_acc=0.96]\u001B[A\n",
      "Training:  67%|██████▋   | 337/500[, loss=0.0382, val_loss=0.0404, val_acc=0.96]\u001B[A\n",
      "Training:  68%|██████▊   | 338/500[, loss=0.0382, val_loss=0.0404, val_acc=0.96]\u001B[A\n",
      "Training:  68%|██████▊   | 338/500[, loss=0.0382, val_loss=0.0404, val_acc=0.96]\u001B[A\n",
      "Training:  68%|██████▊   | 339/500[, loss=0.0382, val_loss=0.0404, val_acc=0.96]\u001B[A\n",
      "Training:  68%|██████▊   | 339/500[, loss=0.0382, val_loss=0.0404, val_acc=0.96]\u001B[A\n",
      "Training:  68%|██████▊   | 340/500[, loss=0.0382, val_loss=0.0404, val_acc=0.96]\u001B[A\n",
      "Training:  68%|██████▊   | 340/500[, loss=0.0382, val_loss=0.0403, val_acc=0.96]\u001B[A\n",
      "Training:  68%|██████▊   | 341/500[, loss=0.0382, val_loss=0.0403, val_acc=0.96]\u001B[A\n",
      "Training:  68%|██████▊   | 341/500[, loss=0.0382, val_loss=0.0403, val_acc=0.96]\u001B[A\n",
      "Training:  68%|██████▊   | 342/500[, loss=0.0382, val_loss=0.0403, val_acc=0.96]\u001B[A\n",
      "Training:  68%|██████▊   | 342/500[, loss=0.0382, val_loss=0.0403, val_acc=0.96]\u001B[A\n",
      "Training:  69%|██████▊   | 343/500[, loss=0.0382, val_loss=0.0403, val_acc=0.96]\u001B[A\n",
      "Training:  69%|██████▊   | 343/500[, loss=0.0382, val_loss=0.0403, val_acc=0.96]\u001B[A\n",
      "Training:  69%|██████▉   | 344/500[, loss=0.0382, val_loss=0.0403, val_acc=0.96]\u001B[A\n",
      "Training:  69%|██████▉   | 344/500[, loss=0.0382, val_loss=0.0403, val_acc=0.96]\u001B[A\n",
      "Training:  69%|██████▉   | 345/500[, loss=0.0382, val_loss=0.0403, val_acc=0.96]\u001B[A\n",
      "Training:  69%|██████▉   | 345/500[, loss=0.0382, val_loss=0.0403, val_acc=0.96]\u001B[A\n",
      "Training:  69%|██████▉   | 346/500[, loss=0.0382, val_loss=0.0403, val_acc=0.96]\u001B[A\n",
      "Training:  69%|██████▉   | 346/500[, loss=0.0382, val_loss=0.0403, val_acc=0.96]\u001B[A\n",
      "Training:  69%|██████▉   | 347/500[, loss=0.0382, val_loss=0.0403, val_acc=0.96]\u001B[A\n",
      "Training:  69%|██████▉   | 347/500[, loss=0.0382, val_loss=0.0403, val_acc=0.96]\u001B[A\n",
      "Training:  70%|██████▉   | 348/500[, loss=0.0382, val_loss=0.0403, val_acc=0.96]\u001B[A\n",
      "Training:  70%|██████▉   | 348/500[, loss=0.0382, val_loss=0.0403, val_acc=0.96]\u001B[A\n",
      "Training:  70%|██████▉   | 349/500[, loss=0.0382, val_loss=0.0403, val_acc=0.96]\u001B[A\n",
      "Training:  70%|██████▉   | 349/500[, loss=0.0382, val_loss=0.0403, val_acc=0.96]\u001B[A\n",
      "Training:  70%|███████   | 350/500[, loss=0.0382, val_loss=0.0403, val_acc=0.96]\u001B[A\n",
      "Training:  70%|███████   | 350/500[, loss=0.0382, val_loss=0.0403, val_acc=0.96]\u001B[A\n",
      "Training:  70%|███████   | 351/500[, loss=0.0382, val_loss=0.0403, val_acc=0.96]\u001B[A\n",
      "Training:  70%|███████   | 351/500[, loss=0.0382, val_loss=0.0403, val_acc=0.96]\u001B[A\n",
      "Training:  70%|███████   | 352/500[, loss=0.0382, val_loss=0.0403, val_acc=0.96]\u001B[A\n",
      "Training:  70%|███████   | 352/500[, loss=0.0382, val_loss=0.0403, val_acc=0.96]\u001B[A\n",
      "Training:  71%|███████   | 353/500[, loss=0.0382, val_loss=0.0403, val_acc=0.96]\u001B[A\n",
      "Training:  71%|███████   | 353/500[, loss=0.0382, val_loss=0.0403, val_acc=0.96]\u001B[A\n",
      "Training:  71%|███████   | 354/500[, loss=0.0382, val_loss=0.0403, val_acc=0.96]\u001B[A\n",
      "Training:  71%|███████   | 354/500[, loss=0.0382, val_loss=0.0403, val_acc=0.96]\u001B[A\n",
      "Training:  71%|███████   | 355/500[, loss=0.0382, val_loss=0.0403, val_acc=0.96]\u001B[A\n",
      "Training:  71%|███████   | 355/500[, loss=0.0382, val_loss=0.0403, val_acc=0.96]\u001B[A\n",
      "Training:  71%|███████   | 356/500[, loss=0.0382, val_loss=0.0403, val_acc=0.96]\u001B[A\n",
      "Training:  71%|███████   | 356/500[, loss=0.0382, val_loss=0.0403, val_acc=0.96]\u001B[A\n",
      "Training:  71%|███████▏  | 357/500[, loss=0.0382, val_loss=0.0403, val_acc=0.96]\u001B[A\n",
      "Training:  71%|███████▏  | 357/500[, loss=0.0382, val_loss=0.0403, val_acc=0.96]\u001B[A\n",
      "Training:  72%|███████▏  | 358/500[, loss=0.0382, val_loss=0.0403, val_acc=0.96]\u001B[A\n",
      "Training:  72%|███████▏  | 358/500[, loss=0.0382, val_loss=0.0402, val_acc=0.96]\u001B[A\n",
      "Training:  72%|███████▏  | 359/500[, loss=0.0382, val_loss=0.0402, val_acc=0.96]\u001B[A\n",
      "Training:  72%|███████▏  | 359/500[, loss=0.0382, val_loss=0.0402, val_acc=0.96]\u001B[A\n",
      "Training:  72%|███████▏  | 360/500[, loss=0.0382, val_loss=0.0402, val_acc=0.96]\u001B[A\n",
      "Training:  72%|███████▏  | 360/500[, loss=0.0382, val_loss=0.0402, val_acc=0.96]\u001B[A\n",
      "Training:  72%|███████▏  | 361/500[, loss=0.0382, val_loss=0.0402, val_acc=0.96]\u001B[A\n",
      "Training:  72%|███████▏  | 361/500[, loss=0.0382, val_loss=0.0402, val_acc=0.96]\u001B[A\n",
      "Training:  72%|███████▏  | 362/500[, loss=0.0382, val_loss=0.0402, val_acc=0.96]\u001B[A\n",
      "Training:  72%|███████▏  | 362/500[, loss=0.0382, val_loss=0.0402, val_acc=0.96]\u001B[A\n",
      "Training:  73%|███████▎  | 363/500[, loss=0.0382, val_loss=0.0402, val_acc=0.96]\u001B[A\n",
      "Training:  73%|███████▎  | 363/500[, loss=0.0382, val_loss=0.0402, val_acc=0.96]\u001B[A\n",
      "Training:  73%|███████▎  | 364/500[, loss=0.0382, val_loss=0.0402, val_acc=0.96]\u001B[A\n",
      "Training:  73%|███████▎  | 364/500[, loss=0.0382, val_loss=0.0402, val_acc=0.96]\u001B[A\n",
      "Training:  73%|███████▎  | 365/500[, loss=0.0382, val_loss=0.0402, val_acc=0.96]\u001B[A\n",
      "Training:  73%|███████▎  | 365/500[, loss=0.0382, val_loss=0.0402, val_acc=0.96]\u001B[A\n",
      "Training:  73%|███████▎  | 366/500[, loss=0.0382, val_loss=0.0402, val_acc=0.96]\u001B[A\n",
      "Training:  73%|███████▎  | 366/500[, loss=0.0382, val_loss=0.0402, val_acc=0.96]\u001B[A\n",
      "Training:  73%|███████▎  | 367/500[, loss=0.0382, val_loss=0.0402, val_acc=0.96]\u001B[A\n",
      "Training:  73%|███████▎  | 367/500[, loss=0.0381, val_loss=0.0402, val_acc=0.96]\u001B[A\n",
      "Training:  74%|███████▎  | 368/500[, loss=0.0381, val_loss=0.0402, val_acc=0.96]\u001B[A\n",
      "Training:  74%|███████▎  | 368/500[, loss=0.0381, val_loss=0.0402, val_acc=0.96]\u001B[A\n",
      "Training:  74%|███████▍  | 369/500[, loss=0.0381, val_loss=0.0402, val_acc=0.96]\u001B[A\n",
      "Training:  74%|███████▍  | 369/500[, loss=0.0381, val_loss=0.0402, val_acc=0.96]\u001B[A\n",
      "Training:  74%|███████▍  | 370/500[, loss=0.0381, val_loss=0.0402, val_acc=0.96]\u001B[A\n",
      "Training:  74%|███████▍  | 370/500[, loss=0.0381, val_loss=0.0402, val_acc=0.96]\u001B[A\n",
      "Training:  74%|███████▍  | 371/500[, loss=0.0381, val_loss=0.0402, val_acc=0.96]\u001B[A\n",
      "Training:  74%|███████▍  | 371/500[, loss=0.0381, val_loss=0.0402, val_acc=0.96]\u001B[A\n",
      "Training:  74%|███████▍  | 372/500[, loss=0.0381, val_loss=0.0402, val_acc=0.96]\u001B[A\n",
      "Training:  74%|███████▍  | 372/500[, loss=0.0381, val_loss=0.0402, val_acc=0.96]\u001B[A\n",
      "Training:  75%|███████▍  | 373/500[, loss=0.0381, val_loss=0.0402, val_acc=0.96]\u001B[A\n",
      "Training:  75%|███████▍  | 373/500[, loss=0.0381, val_loss=0.0402, val_acc=0.96]\u001B[A\n",
      "Training:  75%|███████▍  | 374/500[, loss=0.0381, val_loss=0.0402, val_acc=0.96]\u001B[A\n",
      "Training:  75%|███████▍  | 374/500[, loss=0.0381, val_loss=0.0402, val_acc=0.96]\u001B[A\n",
      "Training:  75%|███████▌  | 375/500[, loss=0.0381, val_loss=0.0402, val_acc=0.96]\u001B[A\n",
      "Training:  75%|███████▌  | 375/500[, loss=0.0381, val_loss=0.0402, val_acc=0.96]\u001B[A\n",
      "Training:  75%|███████▌  | 376/500[, loss=0.0381, val_loss=0.0402, val_acc=0.96]\u001B[A\n",
      "Training:  75%|███████▌  | 376/500[, loss=0.0381, val_loss=0.0402, val_acc=0.96]\u001B[A\n",
      "Training:  75%|███████▌  | 377/500[, loss=0.0381, val_loss=0.0402, val_acc=0.96]\u001B[A\n",
      "Training:  75%|███████▌  | 377/500[, loss=0.0381, val_loss=0.0402, val_acc=0.96]\u001B[A\n",
      "Training:  76%|███████▌  | 378/500[, loss=0.0381, val_loss=0.0402, val_acc=0.96]\u001B[A\n",
      "Training:  76%|███████▌  | 378/500[, loss=0.0381, val_loss=0.0402, val_acc=0.96]\u001B[A\n",
      "Training:  76%|███████▌  | 379/500[, loss=0.0381, val_loss=0.0402, val_acc=0.96]\u001B[A\n",
      "Training:  76%|███████▌  | 379/500[, loss=0.0381, val_loss=0.0402, val_acc=0.96]\u001B[A\n",
      "Training:  76%|███████▌  | 380/500[, loss=0.0381, val_loss=0.0402, val_acc=0.96]\u001B[A\n",
      "Training:  76%|███████▌  | 380/500[, loss=0.0381, val_loss=0.0402, val_acc=0.96]\u001B[A\n",
      "Training:  76%|███████▌  | 381/500[, loss=0.0381, val_loss=0.0402, val_acc=0.96]\u001B[A\n",
      "Training:  76%|███████▌  | 381/500[, loss=0.0381, val_loss=0.0402, val_acc=0.96]\u001B[A\n",
      "Training:  76%|███████▋  | 382/500[, loss=0.0381, val_loss=0.0402, val_acc=0.96]\u001B[A\n",
      "Training:  76%|███████▋  | 382/500[, loss=0.0381, val_loss=0.0402, val_acc=0.96]\u001B[A\n",
      "Training:  77%|███████▋  | 383/500[, loss=0.0381, val_loss=0.0402, val_acc=0.96]\u001B[A\n",
      "Training:  77%|███████▋  | 383/500[, loss=0.0381, val_loss=0.0402, val_acc=0.96]\u001B[A\n",
      "Training:  77%|███████▋  | 384/500[, loss=0.0381, val_loss=0.0402, val_acc=0.96]\u001B[A\n",
      "Training:  77%|███████▋  | 384/500[, loss=0.0381, val_loss=0.0402, val_acc=0.96]\u001B[A\n",
      "Training:  77%|███████▋  | 385/500[, loss=0.0381, val_loss=0.0402, val_acc=0.96]\u001B[A\n",
      "Training:  77%|███████▋  | 385/500[, loss=0.0381, val_loss=0.0402, val_acc=0.96]\u001B[A\n",
      "Training:  77%|███████▋  | 386/500[, loss=0.0381, val_loss=0.0402, val_acc=0.96]\u001B[A\n",
      "Training:  77%|███████▋  | 386/500[, loss=0.0381, val_loss=0.0402, val_acc=0.96]\u001B[A\n",
      "Training:  77%|███████▋  | 387/500[, loss=0.0381, val_loss=0.0402, val_acc=0.96]\u001B[A\n",
      "Training:  77%|███████▋  | 387/500[, loss=0.0381, val_loss=0.0401, val_acc=0.96]\u001B[A\n",
      "Training:  78%|███████▊  | 388/500[, loss=0.0381, val_loss=0.0401, val_acc=0.96]\u001B[A\n",
      "Training:  78%|███████▊  | 388/500[, loss=0.0381, val_loss=0.0401, val_acc=0.96]\u001B[A\n",
      "Training:  78%|███████▊  | 389/500[, loss=0.0381, val_loss=0.0401, val_acc=0.96]\u001B[A\n",
      "Training:  78%|███████▊  | 389/500[, loss=0.0381, val_loss=0.0401, val_acc=0.96]\u001B[A\n",
      "Training:  78%|███████▊  | 390/500[, loss=0.0381, val_loss=0.0401, val_acc=0.96]\u001B[A\n",
      "Training:  78%|███████▊  | 390/500[, loss=0.0381, val_loss=0.0401, val_acc=0.96]\u001B[A\n",
      "Training:  78%|███████▊  | 391/500[, loss=0.0381, val_loss=0.0401, val_acc=0.96]\u001B[A\n",
      "Training:  78%|███████▊  | 391/500[, loss=0.0381, val_loss=0.0401, val_acc=0.96]\u001B[A\n",
      "Training:  78%|███████▊  | 392/500[, loss=0.0381, val_loss=0.0401, val_acc=0.96]\u001B[A\n",
      "Training:  78%|███████▊  | 392/500[, loss=0.0381, val_loss=0.0401, val_acc=0.96]\u001B[A\n",
      "Training:  79%|███████▊  | 393/500[, loss=0.0381, val_loss=0.0401, val_acc=0.96]\u001B[A\n",
      "Training:  79%|███████▊  | 393/500[, loss=0.0381, val_loss=0.0401, val_acc=0.96]\u001B[A\n",
      "Training:  79%|███████▉  | 394/500[, loss=0.0381, val_loss=0.0401, val_acc=0.96]\u001B[A\n",
      "Training:  79%|███████▉  | 394/500[, loss=0.0381, val_loss=0.0401, val_acc=0.96]\u001B[A\n",
      "Training:  79%|███████▉  | 395/500[, loss=0.0381, val_loss=0.0401, val_acc=0.96]\u001B[A\n",
      "Training:  79%|███████▉  | 395/500[, loss=0.0381, val_loss=0.0401, val_acc=0.96]\u001B[A\n",
      "Training:  79%|███████▉  | 396/500[, loss=0.0381, val_loss=0.0401, val_acc=0.96]\u001B[A\n",
      "Training:  79%|███████▉  | 396/500[, loss=0.0381, val_loss=0.0401, val_acc=0.96]\u001B[A\n",
      "Training:  79%|███████▉  | 397/500[, loss=0.0381, val_loss=0.0401, val_acc=0.96]\u001B[A\n",
      "Training:  79%|███████▉  | 397/500[, loss=0.0381, val_loss=0.0401, val_acc=0.96]\u001B[A\n",
      "Training:  80%|███████▉  | 398/500[, loss=0.0381, val_loss=0.0401, val_acc=0.96]\u001B[A\n",
      "Training:  80%|███████▉  | 398/500[, loss=0.0381, val_loss=0.0401, val_acc=0.96]\u001B[A\n",
      "Training:  80%|███████▉  | 399/500[, loss=0.0381, val_loss=0.0401, val_acc=0.96]\u001B[A\n",
      "Training:  80%|███████▉  | 399/500[, loss=0.0381, val_loss=0.0401, val_acc=0.96]\u001B[A\n",
      "Training:  80%|████████  | 400/500[, loss=0.0381, val_loss=0.0401, val_acc=0.96]\u001B[A\n",
      "Training:  80%|████████  | 400/500[, loss=0.0381, val_loss=0.0401, val_acc=0.96]\u001B[A\n",
      "Training:  80%|████████  | 401/500[, loss=0.0381, val_loss=0.0401, val_acc=0.96]\u001B[A\n",
      "Training:  80%|████████  | 401/500[, loss=0.0381, val_loss=0.0401, val_acc=0.96]\u001B[A\n",
      "Training:  80%|████████  | 402/500[, loss=0.0381, val_loss=0.0401, val_acc=0.96]\u001B[A\n",
      "Training:  80%|████████  | 402/500[, loss=0.0381, val_loss=0.0401, val_acc=0.96]\u001B[A\n",
      "Training:  81%|████████  | 403/500[, loss=0.0381, val_loss=0.0401, val_acc=0.96]\u001B[A\n",
      "Training:  81%|████████  | 403/500[, loss=0.0381, val_loss=0.0401, val_acc=0.96]\u001B[A\n",
      "Training:  81%|████████  | 404/500[, loss=0.0381, val_loss=0.0401, val_acc=0.96]\u001B[A\n",
      "Training:  81%|████████  | 404/500[, loss=0.0381, val_loss=0.0401, val_acc=0.96]\u001B[A\n",
      "Training:  81%|████████  | 405/500[, loss=0.0381, val_loss=0.0401, val_acc=0.96]\u001B[A\n",
      "Training:  81%|████████  | 405/500[, loss=0.0381, val_loss=0.0401, val_acc=0.96]\u001B[A\n",
      "Training:  81%|████████  | 406/500[, loss=0.0381, val_loss=0.0401, val_acc=0.96]\u001B[A\n",
      "Training:  81%|████████  | 406/500[, loss=0.0381, val_loss=0.0401, val_acc=0.96]\u001B[A\n",
      "Training:  81%|████████▏ | 407/500[, loss=0.0381, val_loss=0.0401, val_acc=0.96]\u001B[A\n",
      "Training:  81%|████████▏ | 407/500[, loss=0.0381, val_loss=0.0401, val_acc=0.96]\u001B[A\n",
      "Training:  82%|████████▏ | 408/500[, loss=0.0381, val_loss=0.0401, val_acc=0.96]\u001B[A\n",
      "Training:  82%|████████▏ | 408/500[, loss=0.0381, val_loss=0.0401, val_acc=0.96]\u001B[A\n",
      "Training:  82%|████████▏ | 409/500[, loss=0.0381, val_loss=0.0401, val_acc=0.96]\u001B[A\n",
      "Training:  82%|████████▏ | 409/500[, loss=0.0381, val_loss=0.0401, val_acc=0.96]\u001B[A\n",
      "Training:  82%|████████▏ | 410/500[, loss=0.0381, val_loss=0.0401, val_acc=0.96]\u001B[A\n",
      "Training:  82%|████████▏ | 410/500[, loss=0.0381, val_loss=0.0401, val_acc=0.96]\u001B[A\n",
      "Training:  82%|████████▏ | 411/500[, loss=0.0381, val_loss=0.0401, val_acc=0.96]\u001B[A\n",
      "Training:  82%|████████▏ | 411/500[, loss=0.0381, val_loss=0.0401, val_acc=0.96]\u001B[A\n",
      "Training:  82%|████████▏ | 412/500[, loss=0.0381, val_loss=0.0401, val_acc=0.96]\u001B[A\n",
      "Training:  82%|████████▏ | 412/500[, loss=0.0381, val_loss=0.0401, val_acc=0.96]\u001B[A\n",
      "Training:  83%|████████▎ | 413/500[, loss=0.0381, val_loss=0.0401, val_acc=0.96]\u001B[A\n",
      "Training:  83%|████████▎ | 413/500[, loss=0.0381, val_loss=0.0401, val_acc=0.96]\u001B[A\n",
      "Training:  83%|████████▎ | 414/500[, loss=0.0381, val_loss=0.0401, val_acc=0.96]\u001B[A\n",
      "Training:  83%|████████▎ | 414/500[, loss=0.0381, val_loss=0.0401, val_acc=0.96]\u001B[A\n",
      "Training:  83%|████████▎ | 415/500[, loss=0.0381, val_loss=0.0401, val_acc=0.96]\u001B[A\n",
      "Training:  83%|████████▎ | 415/500[, loss=0.0381, val_loss=0.0401, val_acc=0.96]\u001B[A\n",
      "Training:  83%|████████▎ | 416/500[, loss=0.0381, val_loss=0.0401, val_acc=0.96]\u001B[A\n",
      "Training:  83%|████████▎ | 416/500[, loss=0.0381, val_loss=0.04, val_acc=0.96]  \u001B[A\n",
      "Training:  83%|████████▎ | 417/500[, loss=0.0381, val_loss=0.04, val_acc=0.96]\u001B[A\n",
      "Training:  83%|████████▎ | 417/500[, loss=0.0381, val_loss=0.04, val_acc=0.96]\u001B[A\n",
      "Training:  84%|████████▎ | 418/500[, loss=0.0381, val_loss=0.04, val_acc=0.96]\u001B[A\n",
      "Training:  84%|████████▎ | 418/500[, loss=0.0381, val_loss=0.04, val_acc=0.96]\u001B[A\n",
      "Training:  84%|████████▍ | 419/500[, loss=0.0381, val_loss=0.04, val_acc=0.96]\u001B[A\n",
      "Training:  84%|████████▍ | 419/500[, loss=0.0381, val_loss=0.04, val_acc=0.96]\u001B[A\n",
      "Training:  84%|████████▍ | 420/500[, loss=0.0381, val_loss=0.04, val_acc=0.96]\u001B[A\n",
      "Training:  84%|████████▍ | 420/500[, loss=0.0381, val_loss=0.04, val_acc=0.96]\u001B[A\n",
      "Training:  84%|████████▍ | 421/500[, loss=0.0381, val_loss=0.04, val_acc=0.96]\u001B[A\n",
      "Training:  84%|████████▍ | 421/500[, loss=0.0381, val_loss=0.04, val_acc=0.96]\u001B[A\n",
      "Training:  84%|████████▍ | 422/500[, loss=0.0381, val_loss=0.04, val_acc=0.96]\u001B[A\n",
      "Training:  84%|████████▍ | 422/500[, loss=0.0381, val_loss=0.04, val_acc=0.96]\u001B[A\n",
      "Training:  85%|████████▍ | 423/500[, loss=0.0381, val_loss=0.04, val_acc=0.96]\u001B[A\n",
      "Training:  85%|████████▍ | 423/500[, loss=0.0381, val_loss=0.04, val_acc=0.96]\u001B[A\n",
      "Training:  85%|████████▍ | 424/500[, loss=0.0381, val_loss=0.04, val_acc=0.96]\u001B[A\n",
      "Training:  85%|████████▍ | 424/500[, loss=0.0381, val_loss=0.04, val_acc=0.96]\u001B[A\n",
      "Training:  85%|████████▌ | 425/500[, loss=0.0381, val_loss=0.04, val_acc=0.96]\u001B[A\n",
      "Training:  85%|████████▌ | 425/500[, loss=0.0381, val_loss=0.04, val_acc=0.96]\u001B[A\n",
      "Training:  85%|████████▌ | 426/500[, loss=0.0381, val_loss=0.04, val_acc=0.96]\u001B[A\n",
      "Training:  85%|████████▌ | 426/500[, loss=0.038, val_loss=0.04, val_acc=0.96] \u001B[A\n",
      "Training:  85%|████████▌ | 427/500[, loss=0.038, val_loss=0.04, val_acc=0.96]\u001B[A\n",
      "Training:  85%|████████▌ | 427/500[, loss=0.038, val_loss=0.04, val_acc=0.96]\u001B[A\n",
      "Training:  86%|████████▌ | 428/500[, loss=0.038, val_loss=0.04, val_acc=0.96]\u001B[A\n",
      "Training:  86%|████████▌ | 428/500[, loss=0.038, val_loss=0.04, val_acc=0.96]\u001B[A\n",
      "Training:  86%|████████▌ | 429/500[, loss=0.038, val_loss=0.04, val_acc=0.96]\u001B[A\n",
      "Training:  86%|████████▌ | 429/500[, loss=0.038, val_loss=0.04, val_acc=0.96]\u001B[A\n",
      "Training:  86%|████████▌ | 430/500[, loss=0.038, val_loss=0.04, val_acc=0.96]\u001B[A\n",
      "Training:  86%|████████▌ | 430/500[, loss=0.038, val_loss=0.04, val_acc=0.96]\u001B[A\n",
      "Training:  86%|████████▌ | 431/500[, loss=0.038, val_loss=0.04, val_acc=0.96]\u001B[A\n",
      "Training:  86%|████████▌ | 431/500[, loss=0.038, val_loss=0.04, val_acc=0.96]\u001B[A\n",
      "Training:  86%|████████▋ | 432/500[, loss=0.038, val_loss=0.04, val_acc=0.96]\u001B[A\n",
      "Training:  86%|████████▋ | 432/500[, loss=0.038, val_loss=0.04, val_acc=0.96]\u001B[A\n",
      "Training:  87%|████████▋ | 433/500[, loss=0.038, val_loss=0.04, val_acc=0.96]\u001B[A\n",
      "Training:  87%|████████▋ | 433/500[, loss=0.038, val_loss=0.04, val_acc=0.96]\u001B[A\n",
      "Training:  87%|████████▋ | 434/500[, loss=0.038, val_loss=0.04, val_acc=0.96]\u001B[A\n",
      "Training:  87%|████████▋ | 434/500[, loss=0.038, val_loss=0.04, val_acc=0.96]\u001B[A\n",
      "Training:  87%|████████▋ | 435/500[, loss=0.038, val_loss=0.04, val_acc=0.96]\u001B[A\n",
      "Training:  87%|████████▋ | 435/500[, loss=0.038, val_loss=0.04, val_acc=0.96]\u001B[A\n",
      "Training:  87%|████████▋ | 436/500[, loss=0.038, val_loss=0.04, val_acc=0.96]\u001B[A\n",
      "Training:  87%|████████▋ | 436/500[, loss=0.038, val_loss=0.04, val_acc=0.96]\u001B[A\n",
      "Training:  87%|████████▋ | 437/500[, loss=0.038, val_loss=0.04, val_acc=0.96]\u001B[A\n",
      "Training:  87%|████████▋ | 437/500[, loss=0.038, val_loss=0.04, val_acc=0.96]\u001B[A\n",
      "Training:  88%|████████▊ | 438/500[, loss=0.038, val_loss=0.04, val_acc=0.96]\u001B[A\n",
      "Training:  88%|████████▊ | 438/500[, loss=0.038, val_loss=0.04, val_acc=0.96]\u001B[A\n",
      "Training:  88%|████████▊ | 439/500[, loss=0.038, val_loss=0.04, val_acc=0.96]\u001B[A\n",
      "Training:  88%|████████▊ | 439/500[, loss=0.038, val_loss=0.04, val_acc=0.96]\u001B[A\n",
      "Training:  88%|████████▊ | 440/500[, loss=0.038, val_loss=0.04, val_acc=0.96]\u001B[A\n",
      "Training:  88%|████████▊ | 440/500[, loss=0.038, val_loss=0.04, val_acc=0.96]\u001B[A\n",
      "Training:  88%|████████▊ | 441/500[, loss=0.038, val_loss=0.04, val_acc=0.96]\u001B[A\n",
      "Training:  88%|████████▊ | 441/500[, loss=0.038, val_loss=0.04, val_acc=0.96]\u001B[A\n",
      "Training:  88%|████████▊ | 442/500[, loss=0.038, val_loss=0.04, val_acc=0.96]\u001B[A\n",
      "Training:  88%|████████▊ | 442/500[, loss=0.038, val_loss=0.04, val_acc=0.96]\u001B[A\n",
      "Training:  89%|████████▊ | 443/500[, loss=0.038, val_loss=0.04, val_acc=0.96]\u001B[A\n",
      "Training:  89%|████████▊ | 443/500[, loss=0.038, val_loss=0.04, val_acc=0.96]\u001B[A\n",
      "Training:  89%|████████▉ | 444/500[, loss=0.038, val_loss=0.04, val_acc=0.96]\u001B[A\n",
      "Training:  89%|████████▉ | 444/500[, loss=0.038, val_loss=0.04, val_acc=0.96]\u001B[A\n",
      "Training:  89%|████████▉ | 445/500[, loss=0.038, val_loss=0.04, val_acc=0.96]\u001B[A\n",
      "Training:  89%|████████▉ | 445/500[, loss=0.038, val_loss=0.04, val_acc=0.96]\u001B[A\n",
      "Training:  89%|████████▉ | 446/500[, loss=0.038, val_loss=0.04, val_acc=0.96]\u001B[A\n",
      "Training:  89%|████████▉ | 446/500[, loss=0.038, val_loss=0.04, val_acc=0.96]\u001B[A\n",
      "Training:  89%|████████▉ | 447/500[, loss=0.038, val_loss=0.04, val_acc=0.96]\u001B[A\n",
      "Training:  89%|████████▉ | 447/500[, loss=0.038, val_loss=0.04, val_acc=0.96]\u001B[A\n",
      "Training:  90%|████████▉ | 448/500[, loss=0.038, val_loss=0.04, val_acc=0.96]\u001B[A\n",
      "Training:  90%|████████▉ | 448/500[, loss=0.038, val_loss=0.04, val_acc=0.96]\u001B[A\n",
      "Training:  90%|████████▉ | 449/500[, loss=0.038, val_loss=0.04, val_acc=0.96]\u001B[A\n",
      "Training:  90%|████████▉ | 449/500[, loss=0.038, val_loss=0.04, val_acc=0.96]\u001B[A\n",
      "Training:  90%|█████████ | 450/500[, loss=0.038, val_loss=0.04, val_acc=0.96]\u001B[A\n",
      "Training:  90%|█████████ | 450/500[, loss=0.038, val_loss=0.04, val_acc=0.96]\u001B[A\n",
      "Training:  90%|█████████ | 451/500[, loss=0.038, val_loss=0.04, val_acc=0.96]\u001B[A\n",
      "Training:  90%|█████████ | 451/500[, loss=0.038, val_loss=0.04, val_acc=0.96]\u001B[A\n",
      "Training:  90%|█████████ | 452/500[, loss=0.038, val_loss=0.04, val_acc=0.96]\u001B[A\n",
      "Training:  90%|█████████ | 452/500[, loss=0.038, val_loss=0.04, val_acc=0.96]\u001B[A\n",
      "Training:  91%|█████████ | 453/500[, loss=0.038, val_loss=0.04, val_acc=0.96]\u001B[A\n",
      "Training:  91%|█████████ | 453/500[, loss=0.038, val_loss=0.04, val_acc=0.96]\u001B[A\n",
      "Training:  91%|█████████ | 454/500[, loss=0.038, val_loss=0.04, val_acc=0.96]\u001B[A\n",
      "Training:  91%|█████████ | 454/500[, loss=0.038, val_loss=0.04, val_acc=0.96]\u001B[A\n",
      "Training:  91%|█████████ | 455/500[, loss=0.038, val_loss=0.04, val_acc=0.96]\u001B[A\n",
      "Training:  91%|█████████ | 455/500[, loss=0.038, val_loss=0.04, val_acc=0.96]\u001B[A\n",
      "Training:  91%|█████████ | 456/500[, loss=0.038, val_loss=0.04, val_acc=0.96]\u001B[A\n",
      "Training:  91%|█████████ | 456/500[, loss=0.038, val_loss=0.04, val_acc=0.96]\u001B[A\n",
      "Training:  91%|█████████▏| 457/500[, loss=0.038, val_loss=0.04, val_acc=0.96]\u001B[A\n",
      "Training:  91%|█████████▏| 457/500[, loss=0.038, val_loss=0.04, val_acc=0.96]\u001B[A\n",
      "Training:  92%|█████████▏| 458/500[, loss=0.038, val_loss=0.04, val_acc=0.96]\u001B[A\n",
      "Training:  92%|█████████▏| 458/500[, loss=0.038, val_loss=0.0399, val_acc=0.96]\u001B[A\n",
      "Training:  92%|█████████▏| 459/500[, loss=0.038, val_loss=0.0399, val_acc=0.96]\u001B[A\n",
      "Training:  92%|█████████▏| 459/500[, loss=0.038, val_loss=0.0399, val_acc=0.96]\u001B[A\n",
      "Training:  92%|█████████▏| 460/500[, loss=0.038, val_loss=0.0399, val_acc=0.96]\u001B[A\n",
      "Training:  92%|█████████▏| 460/500[, loss=0.038, val_loss=0.0399, val_acc=0.96]\u001B[A\n",
      "Training:  92%|█████████▏| 461/500[, loss=0.038, val_loss=0.0399, val_acc=0.96]\u001B[A\n",
      "Training:  92%|█████████▏| 461/500[, loss=0.038, val_loss=0.0399, val_acc=0.96]\u001B[A\n",
      "Training:  92%|█████████▏| 462/500[, loss=0.038, val_loss=0.0399, val_acc=0.96]\u001B[A\n",
      "Training:  92%|█████████▏| 462/500[, loss=0.038, val_loss=0.0399, val_acc=0.96]\u001B[A\n",
      "Training:  93%|█████████▎| 463/500[, loss=0.038, val_loss=0.0399, val_acc=0.96]\u001B[A\n",
      "Training:  93%|█████████▎| 463/500[, loss=0.038, val_loss=0.0399, val_acc=0.96]\u001B[A\n",
      "Training:  93%|█████████▎| 464/500[, loss=0.038, val_loss=0.0399, val_acc=0.96]\u001B[A\n",
      "Training:  93%|█████████▎| 464/500[, loss=0.038, val_loss=0.0399, val_acc=0.96]\u001B[A\n",
      "Training:  93%|█████████▎| 465/500[, loss=0.038, val_loss=0.0399, val_acc=0.96]\u001B[A\n",
      "Training:  93%|█████████▎| 465/500[, loss=0.038, val_loss=0.0399, val_acc=0.96]\u001B[A\n",
      "Training:  93%|█████████▎| 466/500[, loss=0.038, val_loss=0.0399, val_acc=0.96]\u001B[A\n",
      "Training:  93%|█████████▎| 466/500[, loss=0.038, val_loss=0.0399, val_acc=0.96]\u001B[A\n",
      "Training:  93%|█████████▎| 467/500[, loss=0.038, val_loss=0.0399, val_acc=0.96]\u001B[A\n",
      "Training:  93%|█████████▎| 467/500[, loss=0.038, val_loss=0.0399, val_acc=0.96]\u001B[A\n",
      "Training:  94%|█████████▎| 468/500[, loss=0.038, val_loss=0.0399, val_acc=0.96]\u001B[A\n",
      "Training:  94%|█████████▎| 468/500[, loss=0.038, val_loss=0.0399, val_acc=0.96]\u001B[A\n",
      "Training:  94%|█████████▍| 469/500[, loss=0.038, val_loss=0.0399, val_acc=0.96]\u001B[A\n",
      "Training:  94%|█████████▍| 469/500[, loss=0.038, val_loss=0.0399, val_acc=0.96]\u001B[A\n",
      "Training:  94%|█████████▍| 470/500[, loss=0.038, val_loss=0.0399, val_acc=0.96]\u001B[A\n",
      "Training:  94%|█████████▍| 470/500[, loss=0.038, val_loss=0.0399, val_acc=0.96]\u001B[A\n",
      "Training:  94%|█████████▍| 471/500[, loss=0.038, val_loss=0.0399, val_acc=0.96]\u001B[A\n",
      "Training:  94%|█████████▍| 471/500[, loss=0.038, val_loss=0.0399, val_acc=0.96]\u001B[A\n",
      "Training:  94%|█████████▍| 472/500[, loss=0.038, val_loss=0.0399, val_acc=0.96]\u001B[A\n",
      "Training:  94%|█████████▍| 472/500[, loss=0.038, val_loss=0.0399, val_acc=0.96]\u001B[A\n",
      "Training:  95%|█████████▍| 473/500[, loss=0.038, val_loss=0.0399, val_acc=0.96]\u001B[A\n",
      "Training:  95%|█████████▍| 473/500[, loss=0.038, val_loss=0.0399, val_acc=0.96]\u001B[A\n",
      "Training:  95%|█████████▍| 474/500[, loss=0.038, val_loss=0.0399, val_acc=0.96]\u001B[A\n",
      "Training:  95%|█████████▍| 474/500[, loss=0.038, val_loss=0.0399, val_acc=0.96]\u001B[A\n",
      "Training:  95%|█████████▌| 475/500[, loss=0.038, val_loss=0.0399, val_acc=0.96]\u001B[A\n",
      "Training:  95%|█████████▌| 475/500[, loss=0.038, val_loss=0.0399, val_acc=0.96]\u001B[A\n",
      "Training:  95%|█████████▌| 476/500[, loss=0.038, val_loss=0.0399, val_acc=0.96]\u001B[A\n",
      "Training:  95%|█████████▌| 476/500[, loss=0.038, val_loss=0.0399, val_acc=0.96]\u001B[A\n",
      "Training:  95%|█████████▌| 477/500[, loss=0.038, val_loss=0.0399, val_acc=0.96]\u001B[A\n",
      "Training:  95%|█████████▌| 477/500[, loss=0.038, val_loss=0.0399, val_acc=0.96]\u001B[A\n",
      "Training:  96%|█████████▌| 478/500[, loss=0.038, val_loss=0.0399, val_acc=0.96]\u001B[A\n",
      "Training:  96%|█████████▌| 478/500[, loss=0.038, val_loss=0.0399, val_acc=0.96]\u001B[A\n",
      "Training:  96%|█████████▌| 479/500[, loss=0.038, val_loss=0.0399, val_acc=0.96]\u001B[A\n",
      "Training:  96%|█████████▌| 479/500[, loss=0.038, val_loss=0.0399, val_acc=0.96]\u001B[A\n",
      "Training:  96%|█████████▌| 480/500[, loss=0.038, val_loss=0.0399, val_acc=0.96]\u001B[A\n",
      "Training:  96%|█████████▌| 480/500[, loss=0.038, val_loss=0.0399, val_acc=0.96]\u001B[A\n",
      "Training:  96%|█████████▌| 481/500[, loss=0.038, val_loss=0.0399, val_acc=0.96]\u001B[A\n",
      "Training:  96%|█████████▌| 481/500[, loss=0.038, val_loss=0.0399, val_acc=0.96]\u001B[A\n",
      "Training:  96%|█████████▋| 482/500[, loss=0.038, val_loss=0.0399, val_acc=0.96]\u001B[A\n",
      "Training:  96%|█████████▋| 482/500[, loss=0.038, val_loss=0.0399, val_acc=0.96]\u001B[A\n",
      "Training:  97%|█████████▋| 483/500[, loss=0.038, val_loss=0.0399, val_acc=0.96]\u001B[A\n",
      "Training:  97%|█████████▋| 483/500[, loss=0.038, val_loss=0.0399, val_acc=0.96]\u001B[A\n",
      "Training:  97%|█████████▋| 484/500[, loss=0.038, val_loss=0.0399, val_acc=0.96]\u001B[A\n",
      "Training:  97%|█████████▋| 484/500[, loss=0.038, val_loss=0.0399, val_acc=0.96]\u001B[A\n",
      "Training:  97%|█████████▋| 485/500[, loss=0.038, val_loss=0.0399, val_acc=0.96]\u001B[A\n",
      "Training:  97%|█████████▋| 485/500[, loss=0.038, val_loss=0.0399, val_acc=0.96]\u001B[A\n",
      "Training:  97%|█████████▋| 486/500[, loss=0.038, val_loss=0.0399, val_acc=0.96]\u001B[A\n",
      "Training:  97%|█████████▋| 486/500[, loss=0.0379, val_loss=0.0399, val_acc=0.96]\u001B[A\n",
      "Training:  97%|█████████▋| 487/500[, loss=0.0379, val_loss=0.0399, val_acc=0.96]\u001B[A\n",
      "Training:  97%|█████████▋| 487/500[, loss=0.0379, val_loss=0.0399, val_acc=0.96]\u001B[A\n",
      "Training:  98%|█████████▊| 488/500[, loss=0.0379, val_loss=0.0399, val_acc=0.96]\u001B[A\n",
      "Training:  98%|█████████▊| 488/500[, loss=0.0379, val_loss=0.0399, val_acc=0.96]\u001B[A\n",
      "Training:  98%|█████████▊| 489/500[, loss=0.0379, val_loss=0.0399, val_acc=0.96]\u001B[A\n",
      "Training:  98%|█████████▊| 489/500[, loss=0.0379, val_loss=0.0399, val_acc=0.96]\u001B[A\n",
      "Training:  98%|█████████▊| 490/500[, loss=0.0379, val_loss=0.0399, val_acc=0.96]\u001B[A\n",
      "Training:  98%|█████████▊| 490/500[, loss=0.0379, val_loss=0.0399, val_acc=0.96]\u001B[A\n",
      "Training:  98%|█████████▊| 491/500[, loss=0.0379, val_loss=0.0399, val_acc=0.96]\u001B[A\n",
      "Training:  98%|█████████▊| 491/500[, loss=0.0379, val_loss=0.0399, val_acc=0.96]\u001B[A\n",
      "Training:  98%|█████████▊| 492/500[, loss=0.0379, val_loss=0.0399, val_acc=0.96]\u001B[A\n",
      "Training:  98%|█████████▊| 492/500[, loss=0.0379, val_loss=0.0399, val_acc=0.96]\u001B[A\n",
      "Training:  99%|█████████▊| 493/500[, loss=0.0379, val_loss=0.0399, val_acc=0.96]\u001B[A\n",
      "Training:  99%|█████████▊| 493/500[, loss=0.0379, val_loss=0.0399, val_acc=0.96]\u001B[A\n",
      "Training:  99%|█████████▉| 494/500[, loss=0.0379, val_loss=0.0399, val_acc=0.96]\u001B[A\n",
      "Training:  99%|█████████▉| 494/500[, loss=0.0379, val_loss=0.0399, val_acc=0.96]\u001B[A\n",
      "Training:  99%|█████████▉| 495/500[, loss=0.0379, val_loss=0.0399, val_acc=0.96]\u001B[A\n",
      "Training:  99%|█████████▉| 495/500[, loss=0.0379, val_loss=0.0399, val_acc=0.96]\u001B[A\n",
      "Training:  99%|█████████▉| 496/500[, loss=0.0379, val_loss=0.0399, val_acc=0.96]\u001B[A\n",
      "Training:  99%|█████████▉| 496/500[, loss=0.0379, val_loss=0.0399, val_acc=0.96]\u001B[A\n",
      "Training:  99%|█████████▉| 497/500[, loss=0.0379, val_loss=0.0399, val_acc=0.96]\u001B[A\n",
      "Training:  99%|█████████▉| 497/500[, loss=0.0379, val_loss=0.0399, val_acc=0.96]\u001B[A\n",
      "Training: 100%|█████████▉| 498/500[, loss=0.0379, val_loss=0.0399, val_acc=0.96]\u001B[A\n",
      "Training: 100%|█████████▉| 498/500[, loss=0.0379, val_loss=0.0399, val_acc=0.96]\u001B[A\n",
      "Training: 100%|█████████▉| 499/500[, loss=0.0379, val_loss=0.0399, val_acc=0.96]\u001B[A\n",
      "Training: 100%|█████████▉| 499/500[, loss=0.0379, val_loss=0.0399, val_acc=0.96]\u001B[A\n",
      "Training: 100%|██████████| 500/500[, loss=0.0379, val_loss=0.0399, val_acc=0.96]\u001B[A\n",
      "\n",
      "Training:   0%|          | 0/500[]\u001B[A\n",
      "Training:   0%|          | 0/500[, loss=0.223, val_loss=0.225, val_acc=0.7]\u001B[A\n",
      "Training:   0%|          | 1/500[, loss=0.223, val_loss=0.225, val_acc=0.7]\u001B[A\n",
      "Training:   0%|          | 1/500[, loss=0.136, val_loss=0.139, val_acc=0.88]\u001B[A\n",
      "Training:   0%|          | 2/500[, loss=0.136, val_loss=0.139, val_acc=0.88]\u001B[A\n",
      "Training:   0%|          | 2/500[, loss=0.105, val_loss=0.109, val_acc=0.88]\u001B[A\n",
      "Training:   1%|          | 3/500[, loss=0.105, val_loss=0.109, val_acc=0.88]\u001B[A\n",
      "Training:   1%|          | 3/500[, loss=0.0974, val_loss=0.104, val_acc=0.88]\u001B[A\n",
      "Training:   1%|          | 4/500[, loss=0.0974, val_loss=0.104, val_acc=0.88]\u001B[A\n",
      "Training:   1%|          | 4/500[, loss=0.0954, val_loss=0.103, val_acc=0.86]\u001B[A\n",
      "Training:   1%|          | 5/500[, loss=0.0954, val_loss=0.103, val_acc=0.86]\u001B[A\n",
      "Training:   1%|          | 5/500[, loss=0.0947, val_loss=0.103, val_acc=0.84]\u001B[A\n",
      "Training:   1%|          | 6/500[, loss=0.0947, val_loss=0.103, val_acc=0.84]\u001B[A\n",
      "Training:   1%|          | 6/500[, loss=0.0944, val_loss=0.103, val_acc=0.84]\u001B[A\n",
      "Training:   1%|▏         | 7/500[, loss=0.0944, val_loss=0.103, val_acc=0.84]\u001B[A\n",
      "Training:   1%|▏         | 7/500[, loss=0.0942, val_loss=0.104, val_acc=0.84]\u001B[A\n",
      "Training:   2%|▏         | 8/500[, loss=0.0942, val_loss=0.104, val_acc=0.84]\u001B[A\n",
      "Training:   2%|▏         | 8/500[, loss=0.0941, val_loss=0.104, val_acc=0.84]\u001B[A\n",
      "Training:   2%|▏         | 9/500[, loss=0.0941, val_loss=0.104, val_acc=0.84]\u001B[A\n",
      "Training:   2%|▏         | 9/500[, loss=0.0941, val_loss=0.104, val_acc=0.84]\u001B[A\n",
      "Training:   2%|▏         | 10/500[, loss=0.0941, val_loss=0.104, val_acc=0.84]\u001B[A\n",
      "Training:   2%|▏         | 10/500[, loss=0.094, val_loss=0.104, val_acc=0.84] \u001B[A\n",
      "Training:   2%|▏         | 11/500[, loss=0.094, val_loss=0.104, val_acc=0.84]\u001B[A\n",
      "Training:   2%|▏         | 11/500[, loss=0.094, val_loss=0.104, val_acc=0.84]\u001B[A\n",
      "Training:   2%|▏         | 12/500[, loss=0.094, val_loss=0.104, val_acc=0.84]\u001B[A\n",
      "Training:   2%|▏         | 12/500[, loss=0.0939, val_loss=0.104, val_acc=0.84]\u001B[A\n",
      "Training:   3%|▎         | 13/500[, loss=0.0939, val_loss=0.104, val_acc=0.84]\u001B[A\n",
      "Training:   3%|▎         | 13/500[, loss=0.0939, val_loss=0.104, val_acc=0.84]\u001B[A\n",
      "Training:   3%|▎         | 14/500[, loss=0.0939, val_loss=0.104, val_acc=0.84]\u001B[A\n",
      "Training:   3%|▎         | 14/500[, loss=0.0938, val_loss=0.104, val_acc=0.84]\u001B[A\n",
      "Training:   3%|▎         | 15/500[, loss=0.0938, val_loss=0.104, val_acc=0.84]\u001B[A\n",
      "Training:   3%|▎         | 15/500[, loss=0.0938, val_loss=0.104, val_acc=0.84]\u001B[A\n",
      "Training:   3%|▎         | 16/500[, loss=0.0938, val_loss=0.104, val_acc=0.84]\u001B[A\n",
      "Training:   3%|▎         | 16/500[, loss=0.0937, val_loss=0.104, val_acc=0.84]\u001B[A\n",
      "Training:   3%|▎         | 17/500[, loss=0.0937, val_loss=0.104, val_acc=0.84]\u001B[A\n",
      "Training:   3%|▎         | 17/500[, loss=0.0937, val_loss=0.104, val_acc=0.84]\u001B[A\n",
      "Training:   4%|▎         | 18/500[, loss=0.0937, val_loss=0.104, val_acc=0.84]\u001B[A\n",
      "Training:   4%|▎         | 18/500[, loss=0.0936, val_loss=0.104, val_acc=0.84]\u001B[A\n",
      "Training:   4%|▍         | 19/500[, loss=0.0936, val_loss=0.104, val_acc=0.84]\u001B[A\n",
      "Training:   4%|▍         | 19/500[, loss=0.0935, val_loss=0.103, val_acc=0.84]\u001B[A\n",
      "Training:   4%|▍         | 20/500[, loss=0.0935, val_loss=0.103, val_acc=0.84]\u001B[A\n",
      "Training:   4%|▍         | 20/500[, loss=0.0935, val_loss=0.103, val_acc=0.84]\u001B[A\n",
      "Training:   4%|▍         | 21/500[, loss=0.0935, val_loss=0.103, val_acc=0.84]\u001B[A\n",
      "Training:   4%|▍         | 21/500[, loss=0.0934, val_loss=0.103, val_acc=0.84]\u001B[A\n",
      "Training:   4%|▍         | 22/500[, loss=0.0934, val_loss=0.103, val_acc=0.84]\u001B[A\n",
      "Training:   4%|▍         | 22/500[, loss=0.0932, val_loss=0.103, val_acc=0.84]\u001B[A\n",
      "Training:   5%|▍         | 23/500[, loss=0.0932, val_loss=0.103, val_acc=0.84]\u001B[A\n",
      "Training:   5%|▍         | 23/500[, loss=0.0931, val_loss=0.103, val_acc=0.84]\u001B[A\n",
      "Training:   5%|▍         | 24/500[, loss=0.0931, val_loss=0.103, val_acc=0.84]\u001B[A\n",
      "Training:   5%|▍         | 24/500[, loss=0.0929, val_loss=0.102, val_acc=0.84]\u001B[A\n",
      "Training:   5%|▌         | 25/500[, loss=0.0929, val_loss=0.102, val_acc=0.84]\u001B[A\n",
      "Training:   5%|▌         | 25/500[, loss=0.0927, val_loss=0.102, val_acc=0.84]\u001B[A\n",
      "Training:   5%|▌         | 26/500[, loss=0.0927, val_loss=0.102, val_acc=0.84]\u001B[A\n",
      "Training:   5%|▌         | 26/500[, loss=0.0925, val_loss=0.102, val_acc=0.84]\u001B[A\n",
      "Training:   5%|▌         | 27/500[, loss=0.0925, val_loss=0.102, val_acc=0.84]\u001B[A\n",
      "Training:   5%|▌         | 27/500[, loss=0.0922, val_loss=0.101, val_acc=0.84]\u001B[A\n",
      "Training:   6%|▌         | 28/500[, loss=0.0922, val_loss=0.101, val_acc=0.84]\u001B[A\n",
      "Training:   6%|▌         | 28/500[, loss=0.092, val_loss=0.101, val_acc=0.84] \u001B[A\n",
      "Training:   6%|▌         | 29/500[, loss=0.092, val_loss=0.101, val_acc=0.84]\u001B[A\n",
      "Training:   6%|▌         | 29/500[, loss=0.0917, val_loss=0.1, val_acc=0.84] \u001B[A\n",
      "Training:   6%|▌         | 30/500[, loss=0.0917, val_loss=0.1, val_acc=0.84]\u001B[A\n",
      "Training:   6%|▌         | 30/500[, loss=0.0913, val_loss=0.0998, val_acc=0.84]\u001B[A\n",
      "Training:   6%|▌         | 31/500[, loss=0.0913, val_loss=0.0998, val_acc=0.84]\u001B[A\n",
      "Training:   6%|▌         | 31/500[, loss=0.091, val_loss=0.0992, val_acc=0.84] \u001B[A\n",
      "Training:   6%|▋         | 32/500[, loss=0.091, val_loss=0.0992, val_acc=0.84]\u001B[A\n",
      "Training:   6%|▋         | 32/500[, loss=0.0906, val_loss=0.0986, val_acc=0.86]\u001B[A\n",
      "Training:   7%|▋         | 33/500[, loss=0.0906, val_loss=0.0986, val_acc=0.86]\u001B[A\n",
      "Training:   7%|▋         | 33/500[, loss=0.0902, val_loss=0.098, val_acc=0.86] \u001B[A\n",
      "Training:   7%|▋         | 34/500[, loss=0.0902, val_loss=0.098, val_acc=0.86]\u001B[A\n",
      "Training:   7%|▋         | 34/500[, loss=0.0898, val_loss=0.0973, val_acc=0.86]\u001B[A\n",
      "Training:   7%|▋         | 35/500[, loss=0.0898, val_loss=0.0973, val_acc=0.86]\u001B[A\n",
      "Training:   7%|▋         | 35/500[, loss=0.0894, val_loss=0.0967, val_acc=0.86]\u001B[A\n",
      "Training:   7%|▋         | 36/500[, loss=0.0894, val_loss=0.0967, val_acc=0.86]\u001B[A\n",
      "Training:   7%|▋         | 36/500[, loss=0.089, val_loss=0.096, val_acc=0.86]  \u001B[A\n",
      "Training:   7%|▋         | 37/500[, loss=0.089, val_loss=0.096, val_acc=0.86]\u001B[A\n",
      "Training:   7%|▋         | 37/500[, loss=0.0887, val_loss=0.0954, val_acc=0.86]\u001B[A\n",
      "Training:   8%|▊         | 38/500[, loss=0.0887, val_loss=0.0954, val_acc=0.86]\u001B[A\n",
      "Training:   8%|▊         | 38/500[, loss=0.0883, val_loss=0.0948, val_acc=0.86]\u001B[A\n",
      "Training:   8%|▊         | 39/500[, loss=0.0883, val_loss=0.0948, val_acc=0.86]\u001B[A\n",
      "Training:   8%|▊         | 39/500[, loss=0.088, val_loss=0.0942, val_acc=0.86] \u001B[A\n",
      "Training:   8%|▊         | 40/500[, loss=0.088, val_loss=0.0942, val_acc=0.86]\u001B[A\n",
      "Training:   8%|▊         | 40/500[, loss=0.0876, val_loss=0.0936, val_acc=0.86]\u001B[A\n",
      "Training:   8%|▊         | 41/500[, loss=0.0876, val_loss=0.0936, val_acc=0.86]\u001B[A\n",
      "Training:   8%|▊         | 41/500[, loss=0.0873, val_loss=0.093, val_acc=0.86] \u001B[A\n",
      "Training:   8%|▊         | 42/500[, loss=0.0873, val_loss=0.093, val_acc=0.86]\u001B[A\n",
      "Training:   8%|▊         | 42/500[, loss=0.0869, val_loss=0.0924, val_acc=0.86]\u001B[A\n",
      "Training:   9%|▊         | 43/500[, loss=0.0869, val_loss=0.0924, val_acc=0.86]\u001B[A\n",
      "Training:   9%|▊         | 43/500[, loss=0.0866, val_loss=0.0916, val_acc=0.86]\u001B[A\n",
      "Training:   9%|▉         | 44/500[, loss=0.0866, val_loss=0.0916, val_acc=0.86]\u001B[A\n",
      "Training:   9%|▉         | 44/500[, loss=0.0862, val_loss=0.0908, val_acc=0.88]\u001B[A\n",
      "Training:   9%|▉         | 45/500[, loss=0.0862, val_loss=0.0908, val_acc=0.88]\u001B[A\n",
      "Training:   9%|▉         | 45/500[, loss=0.0857, val_loss=0.0898, val_acc=0.88]\u001B[A\n",
      "Training:   9%|▉         | 46/500[, loss=0.0857, val_loss=0.0898, val_acc=0.88]\u001B[A\n",
      "Training:   9%|▉         | 46/500[, loss=0.0852, val_loss=0.0887, val_acc=0.88]\u001B[A\n",
      "Training:   9%|▉         | 47/500[, loss=0.0852, val_loss=0.0887, val_acc=0.88]\u001B[A"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[13], line 6\u001B[0m\n\u001B[0;32m      3\u001B[0m model\u001B[38;5;241m.\u001B[39madd_layer(\u001B[38;5;241m1\u001B[39m, Tanh())\n\u001B[0;32m      5\u001B[0m \u001B[38;5;66;03m# create stats\u001B[39;00m\n\u001B[1;32m----> 6\u001B[0m stats \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mx_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_train\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalidation\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mx_val\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_val\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmetric\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mBinaryAccuracy\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mloss\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mMeanSquaredError\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m500\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43meta\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0.01\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnesterov\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\neural-network-from-scratch\\notebooks\\..\\src\\network.py:123\u001B[0m, in \u001B[0;36mNetwork.train\u001B[1;34m(self, train, validation, metric, loss, epochs, eta, batch_size, verbose, callbacks, nesterov)\u001B[0m\n\u001B[0;32m    120\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m x, target \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m(train_data, train_labels):\n\u001B[0;32m    122\u001B[0m     pred \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m__forward_prop__(x)\n\u001B[1;32m--> 123\u001B[0m     deltas \u001B[38;5;241m=\u001B[39m \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpred\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtarget\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    124\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m__backward_prop__(deltas\u001B[38;5;241m=\u001B[39mdeltas, eta\u001B[38;5;241m=\u001B[39meta, nesterov\u001B[38;5;241m=\u001B[39mnesterov)\n\u001B[0;32m    127\u001B[0m \u001B[38;5;66;03m# compute training error and accuracy for current epoch and append stats\u001B[39;00m\n",
      "File \u001B[1;32m~\\PycharmProjects\\neural-network-from-scratch\\notebooks\\..\\src\\losses.py:18\u001B[0m, in \u001B[0;36mMeanSquaredError.backward\u001B[1;34m(self, pred, labels)\u001B[0m\n\u001B[0;32m     17\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mbackward\u001B[39m(\u001B[38;5;28mself\u001B[39m, pred, labels):\n\u001B[1;32m---> 18\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmean\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpred\u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[43mlabels\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkeepdims\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maxis\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m<__array_function__ internals>:177\u001B[0m, in \u001B[0;36mmean\u001B[1;34m(*args, **kwargs)\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "model = Network(2)\n",
    "model.add_layer(8, ReLU())\n",
    "model.add_layer(1, Tanh())\n",
    "\n",
    "# create stats\n",
    "stats = model.train(train=(x_train, y_train), validation=(x_val, y_val), metric=BinaryAccuracy(), loss=MeanSquaredError(), epochs=500, eta=0.01, nesterov=0)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "<AxesSubplot: >"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 1500x500 with 2 Axes>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "sns.lineplot(stats['train_loss'], ax=axs[0])\n",
    "sns.lineplot(stats['val_loss'], ax=axs[0])\n",
    "sns.lineplot(stats['train_acc'], ax=axs[1])\n",
    "sns.lineplot(stats['val_acc'], ax=axs[1])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "predictions = model.multiple_outputs(x_test)\n",
    "\n",
    "accuracy_score(y_test.flatten(), np.round(predictions.flatten()))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "x_train, x_val, x_test, y_train, y_val, y_test = load_monk1()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = Network(17)\n",
    "model.add_layer(6, ReLU())\n",
    "model.add_layer(1, Tanh())\n",
    "stats = model.train((x_train, y_train), (x_test, y_test), metric=BinaryAccuracy(), loss=MeanSquaredError(), epochs=1000)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "sns.lineplot(stats['train_loss'], ax=axs[0])\n",
    "sns.lineplot(stats['val_loss'], ax=axs[0])\n",
    "sns.lineplot(stats['train_acc'], ax=axs[1])\n",
    "sns.lineplot(stats['val_acc'], ax=axs[1])\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = load_mnist()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = Network(64)\n",
    "model.add_layer(16, ReLU())\n",
    "model.add_layer(10, Sigmoid(0.001))\n",
    "stats = model.train(x_train, y_train, x_test, y_test, metric=MulticlassAccuracy(), loss=MeanSquaredError(), epochs=500)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "sns.lineplot(stats['train_loss'], ax=axs[0])\n",
    "sns.lineplot(stats['val_loss'], ax=axs[0])\n",
    "sns.lineplot(stats['train_acc'], ax=axs[1])\n",
    "sns.lineplot(stats['val_acc'], ax=axs[1])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def load_cup(test_size=0.2):\n",
    "    df = pd.read_csv(\"../data/cup/cup.train\", comment=\"#\", index_col='id', skipinitialspace=True)\n",
    "    scaler = MinMaxScaler()\n",
    "    scaled = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)\n",
    "    x_train, x_val, y_train, y_val = train_test_split(scaled.drop([\"ty\", 'tx'], axis=1).values, scaled[['tx','ty']].values, test_size=0.25, random_state=42)\n",
    "    x_train = np.expand_dims(x_train, 2)\n",
    "    x_val = np.expand_dims(x_val, 2)\n",
    "    y_train = np.expand_dims(y_train, 2)\n",
    "    y_val = np.expand_dims(y_val, 2)\n",
    "\n",
    "    return x_train, x_val, y_train, y_val"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "x_train, x_val, y_train, y_val = load_cup()\n",
    "x_train.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = Network(9)\n",
    "model.add_layer(16, ReLU())\n",
    "model.add_layer(8, ReLU())\n",
    "model.add_layer(2, Sigmoid())\n",
    "\n",
    "stats = model.train((x_train, y_train), (x_val, y_val), metric=MeanEuclideanError(), loss=MeanSquaredError(), epochs=1000,\n",
    "                    callbacks=[])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "sns.lineplot(stats['train_loss'], ax=axs[0])\n",
    "sns.lineplot(stats['val_loss'], ax=axs[0])\n",
    "sns.lineplot(stats['train_acc'], ax=axs[1])\n",
    "sns.lineplot(stats['val_acc'], ax=axs[1])\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
